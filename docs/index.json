[
  {
    "content": " grafana-agent 的 metrics采集，完全兼容 prometheus exporter 生态，一些常见的 exporter，会在 grafana-agent 中内嵌实现（列表如下）; 对于未嵌入到 grafana-agent中的 exporter，则可以在 grafana-agent 中配置 scrape_configs 来完成抓取和收集，请参考抓取第三方exporter;  grafana-agent 内置实现的 exporter 列表  node-exporter mysqld-exporter process-exporter cadvisor windows-exporter postgres-exporter mongodb-exporter redis-exporter memcached-exporter kafka-exporter elasticsearch-exporter consul-exporter dnsmasq-exporter  内置exporter的配置项说明 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125  # grafana-agent 本身的配置 server: log_level: info http_listen_port: 12345 # grafana-agent 抓取 metrics 的相关配置（类似于prometheus的scrape_configs） metrics: global: scrape_interval: 15s scrape_timeout: 10s remote_write: - url: https://n9e-server:19000/prometheus/v1/write basic_auth: username: \u003cstring\u003e password: \u003cstring\u003e # grafana-agent integration 相关的配置 integrations: ## grafana-agent self-integration ## grafana-agent 本身的metrics 采集，这也是一个内嵌的 integration，可以选择启用或者关闭。 agent: ### 是否开启针对grafana-agent 自身的integration，允许grafana-agent自动采集和发送其自身的metrics [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the agent hostname # and HTTP listen port, delimited by a colon. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the agent integration will be run but not scraped and thus not # remote_written. Metrics for the integration will be exposed at # /integrations/agent/metrics and can be scraped by an external process. ### 这个配置项如果设置为false，那么 /integrations/agent/metrics 的数据并不会被自动抓取和发送 ### 但是，该接口 /integrations/agent/metrics 的数据仍然支持被外部的抓取进程所抓取  [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # Client TLS Configuration # Client Cert/Key Values need to be defined if the server is requesting a certificate # (Client Auth Type = RequireAndVerifyClientCert || RequireAnyClientCert). http_tls_config: \u003ctls_config\u003e ## 控制内嵌的 node_exporter 工作逻辑  node_exporter: \u003cnode_exporter_config\u003e ## 控制内嵌的 process_exporter 工作逻辑 process_exporter: \u003cprocess_exporter_config\u003e ## 控制内嵌的 mysqld_exporter 工作逻辑 mysqld_exporter: \u003cmysqld_exporter_config\u003e ## 控制内嵌的 redis_exporter 工作逻辑 redis_exporter: \u003credis_exporter_config\u003e ## 控制内嵌的 dnsmasq_exporter 工作逻辑 dnsmasq_exporter: \u003cdnsmasq_exporter_config\u003e ## 控制内嵌的 elasticsearch_exporter 工作逻辑 elasticsearch_expoter: \u003celasticsearch_expoter_config\u003e # Controls the memcached_exporter integration memcached_exporter: \u003cmemcached_exporter_config\u003e ## 控制内嵌的 postgres_exporter 工作逻辑 postgres_exporter: \u003cpostgres_exporter_config\u003e ## 控制内嵌的 statsd_exporter 工作逻辑 statsd_exporter: \u003cstatsd_exporter_config\u003e ## 控制内嵌的 consul_exporter 工作逻辑 consul_exporter: \u003cconsul_exporter_config\u003e ## 控制内嵌的 windows_exporter 工作逻辑 windows_exporter: \u003cwindows_exporter_config\u003e ## 控制内嵌的 kafka_exporter 工作逻辑 kafka_exporter: \u003ckafka_exporter_config\u003e ## 控制内嵌的 mongodb_exporter 工作逻辑  mongodb_exporter: \u003cmongodb_exporter_config\u003e ## 控制内嵌的 github_exporter 工作逻辑 github_exporter: \u003cgithub_exporter_config\u003e # Automatically collect metrics from enabled integrations. If disabled, # integrations will be run but not scraped and thus not remote_written. Metrics # for integrations will be exposed at /integrations/\u003cintegration_key\u003e/metrics # and can be scraped by an external process. ## 如果设置为false，相关的exporter metrics接口仍会被暴露出来，但是grafana-agent不会去主动抓取和发送 [scrape_integrations: \u003cboolean\u003e | default = true] # Extra labels to add to all samples coming from integrations. labels: { \u003cstring\u003e: \u003cstring\u003e } # The period to wait before restarting an integration that exits with an # error. [integration_restart_backoff: \u003cduration\u003e | default = \"5s\"] # A list of remote_write targets. Defaults to global_config.remote_write. # If provided, overrides the global defaults. prometheus_remote_write: - [\u003cremote_write\u003e]   通过grafana-agent抓取第三方exporter并收集 如文章开头所述，对于未嵌入到grafana-agent中的exporter，则可以在grafana-agent中配置scrape_configs来完成抓取和收集，其配置形式完全等同于 prometheus scrape_configs。\ngrafana-agent中关于自定义配置scrape_configs的详细说明如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  # scrape_configs like prometheus style configs: scrape_timeout: 10s # 比如，我们可以配置抓取 grafana-agent 本身的 metrics ： http://127.0.0.1:12345/metrics - name: grafana-agent host_filter: false scrape_configs: - job_name: grafana-agent static_configs: - targets: ['127.0.0.1:12345'] remote_write: - url: http://localhost:9090/api/v1/write # 再比如,我们也可以配置抓取您的应用程序暴露的metrics接口： http://helloworld.app:8088/metrics - name: outside-exporters host_filter: false scrape_configs: - job_name: prometheus static_configs: - targets: ['127.0.0.1:9090'] labels: cluster: 'fc-monitoring' remote_write: - url: https://n9e-server:19000/prometheus/v1/write basic_auth: username: \u003cstring\u003e password: \u003cstring\u003e   ",
    "description": "",
    "tags": null,
    "title": "grafana-agent采集常用exporter",
    "uri": "/grafana-agent/integrations/"
  },
  {
    "content": " 采用Docker Compose做编排，用于单机快速启动环境做测试，包含了MySQL、Redis、Prometheus、Ibex、Nightingale、Telegraf\n 从Github下载夜莺的源码，进入docker目录，执行docker-compose up -d即可，docker-compose会自动拉取镜像并启动，查看各个容器启动状态，使用命令docker-compose ps，都是Up状态则表示启动成功。\n$ git clone https://github.com/didi/nightingale.git $ cd nightingale/docker $ docker-compose up -d Creating network \"docker_nightingale\" with driver \"bridge\" Creating mysql ... done Creating redis ... done Creating prometheus ... done Creating ibex ... done Creating agentd ... done Creating nwebapi ... done Creating nserver ... done Creating telegraf ... done $ docker-compose ps Name Command State Ports ---------------------------------------------------------------------------------------------------------------------------- agentd /app/ibex agentd Up 10090/tcp, 20090/tcp ibex /app/ibex server Up 0.0.0.0:10090-\u003e10090/tcp, 0.0.0.0:20090-\u003e20090/tcp mysql docker-entrypoint.sh mysqld Up 0.0.0.0:3306-\u003e3306/tcp, 33060/tcp nserver /app/n9e server Up 18000/tcp, 0.0.0.0:19000-\u003e19000/tcp nwebapi /app/n9e webapi Up 0.0.0.0:18000-\u003e18000/tcp, 19000/tcp prometheus /bin/prometheus --config.f ... Up 0.0.0.0:9090-\u003e9090/tcp redis docker-entrypoint.sh redis ... Up 0.0.0.0:6379-\u003e6379/tcp telegraf /entrypoint.sh telegraf Up 0.0.0.0:8092-\u003e8092/udp, 0.0.0.0:8094-\u003e8094/tcp, 0.0.0.0:8125-\u003e8125/udp 更多docker-compose相关知识请参考官网\nWarning启动成功之后，建议把initsql目录下的内容挪走，这样下次重启的时候，DB就不会重新初始化了。否则下次启动mysql还是会自动执行initsql下面的sql文件导致DB重新初始化，页面上创建的规则、用户等都会丢失。\ndocker-compose 这种部署方式，只是用于简单测试，不推荐在生产环境使用，当然了，如果您是 docker-compose 专家，另当别论。\n 服务启动之后，浏览器访问nwebapi的端口，即18000，默认用户是root，密码是root.2020\n",
    "description": "",
    "tags": null,
    "title": "使用Docker快速启动测试",
    "uri": "/quickstart/compose/"
  },
  {
    "content": "对于未嵌入到 grafana-agent中的 exporter，则可以在 grafana-agent 中配置 scrape_configs 来完成抓取和收集，请参考抓取第三方exporter。\n",
    "description": "",
    "tags": null,
    "title": "grafana-agent收集三方exporter",
    "uri": "/grafana-agent/scrape_exporters/"
  },
  {
    "content": " 本节讲述如何部署单机版，单机版对于很多中小公司足够用了，简单高效、快速直接，建议使用云主机，性能不够了直接升配，可以应对每秒上报的数据点小于100万的情形，如果只是监控机器（每台机器每个周期大概采集200个数据点）采集周期频率设置10秒的话，支撑上限是5万台\n 如果仅仅是为了快速测试，Docker 部署方式是最快的，不过很多朋友未必有 Docker 环境，另外为了减少引入更多技术栈，增强生产环境稳定性，有些朋友可能也不愿意用 Docker，那本篇就来讲解如何快速部署单机版，单机版的配套时序库是使用 Prometheus。如果要监控的机器有几千台，服务有几百个，单机版的容量无法满足，可以上集群版，集群版的时序库建议使用 VictoriaMetrics，也可以使用 M3DB，不过 M3DB 的架构更复杂，很多朋友无法搞定，选择简单的 VictoriaMetrics，对大部分公司来讲，足够用了。我们先来看一下服务端架构：\n按照单机版本的这个架构图可以看出，服务端需要安装的组件有：MySQL、Redis、Prometheus、n9e-server、n9e-webapi，Agent 有多种选型，可以是 Telegraf、Datadog-Agent、Grafana-Agent 等，Agent 应该部署在所有的目标机器上，包括服务端的这台机器，Exporters 是指 Prometheus 生态的各类 Exporter 采集器，比如 mysqld_exporter、redis_exporter、blackbox_exporter 等，这些 Exporter 是非必须的，看各自公司的情况。\n环境准备 依赖的组件有：mysql、redis、prometheus，这三个组件都是开源软件，请大家自行安装，其中prometheus在启动的时候要注意开启 --enable-feature=remote-write-receiver 这里也提供一个小脚本来安装这3个组件，大家可以参考：\n# install prometheus mkdir -p /opt/prometheus wget https://s3-gz01.didistatic.com/n9e-pub/prome/prometheus-2.28.0.linux-amd64.tar.gz -O prometheus-2.28.0.linux-amd64.tar.gz tar xf prometheus-2.28.0.linux-amd64.tar.gz cp -far prometheus-2.28.0.linux-amd64/* /opt/prometheus/ # service  cat \u003c\u003cEOF \u003e/etc/systemd/system/prometheus.service [Unit] Description=\"prometheus\" Documentation=https://prometheus.io/ After=network.target [Service] Type=simple ExecStart=/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml --storage.tsdb.path=/opt/prometheus/data --web.enable-lifecycle --enable-feature=remote-write-receiver --query.lookback-delta=2m Restart=on-failure SuccessExitStatus=0 LimitNOFILE=65536 StandardOutput=syslog StandardError=syslog SyslogIdentifier=prometheus [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable prometheus systemctl restart prometheus systemctl status prometheus # install mysql yum -y install mariadb* systemctl enable mariadb systemctl restart mariadb mysql -e \"SET PASSWORD FOR 'root'@'localhost' = PASSWORD('1234');\" # install redis yum install -y redis systemctl enable redis systemctl restart redis 上例中mysql的root密码设置为了1234，建议维持这个不变，后续就省去了修改配置文件的麻烦。\n安装夜莺组件 mkdir -p /opt/n9e \u0026\u0026 cd /opt/n9e # 去 https://github.com/didi/nightingale/releases 找最新版本的包，文档里的包地址可能已经不是最新的了 tarball=n9e-5.7.0.tar.gz urlpath=https://github.com/didi/nightingale/releases/download/v5.7.0/${tarball} wget $urlpath || exit 1 tar zxvf ${tarball} mysql -uroot -p1234 \u003c docker/initsql/a-n9e.sql nohup ./n9e server \u0026\u003e server.log \u0026 nohup ./n9e webapi \u0026\u003e webapi.log \u0026 # check logs # check port 如果启动成功，server 默认会监听在 19000 端口，webapi 会监听在 18000 端口，且日志没有报错。上面使用 nohup 简单演示，生产环境建议用 systemd 托管，相关 service 文件可以在 etc/service 目录下，供参考。\n配置文件etc/server.conf和etc/webapi.conf中都含有 mysql 的连接地址配置，检查一下用户名和密码，prometheus 如果使用上面的脚本安装，默认会监听本机 9090 端口，server.conf 和 webapi.conf 中的 prometheus 相关地址都不用修改就是对的。\n好了，浏览器访问 webapi 的端口（默认是18000）就可以体验相关功能了，默认用户是root，密码是root.2020。如果安装过程出现问题，可以参考公众号（云原生监控）的视频教程。\n夜莺服务端部署好了，接下来要考虑监控数据采集的问题，如果是 Prometheus 重度用户，可以继续使用各类 Exporter 来采集，只要数据进了时序库了，夜莺就能够消费（判断告警、展示图表等）；如果想快速看到效果，可以使用 Telegraf 来采集监控数据，请参考后续文档章节。\n",
    "description": "",
    "tags": null,
    "title": "使用二进制部署单机版服务端",
    "uri": "/quickstart/standalone/"
  },
  {
    "content": "如果您对Docker的使用非常熟悉，建议利用Docker compose的方式快速启动测试，请参考使用Docker Compose快速部署，如果对Docker不熟悉，那就用二进制方式部署，也非常简单，最小的可运行环境是Prometheus+MySQL+Redis+Nightingale，请参考快速在生产环境部署启动单机版。这个最小的环境只有Prometheus采集到的自身的一些监控指标，略显单薄，此时，我们可以引入Telegraf，采集机器、网络设备、各类中间件的指标，请参考使用Telegraf采集监控数据。\n如果您运行和维护着K8s集群，并且希望把Nightingale部署到K8s中，我们推荐您使用n9e-helm-chart来部署、升级、管理。\n如果公司体量很大，建议把单机版本的Prometheus替换为VictoriaMetrics，请参考使用VictoriaMetrics作为时序库。或者直接部署多个Prometheus，按照业务线或者按照地域来划分集群，此时你可能需要接入多个Prom/VM/M3DB集群，在引入多个TSDB的过程中，就要同步使用夜莺的多Server部署模型了，请参考生产环境部署高可用集群版。\n",
    "description": "",
    "tags": null,
    "title": "安装部署",
    "uri": "/quickstart/"
  },
  {
    "content": " Telegraf 是 InfluxData 开源的一款采集器，可以采集操作系统、各种中间件的监控指标，采集目标列表，看起来是非常丰富，Telegraf是一个大一统的设计，即一个二进制可以采集CPU、内存、mysql、mongodb、redis、snmp等，不像Prometheus的exporter，每个监控对象一个exporter，管理起来略麻烦。一个二进制分发起来确实比较方便。\n 这里提供快速安装的教程，Telegraf的更多知识，请参考Telegraf官网，笔者之前也写了一个Telegraf调研笔记，讲解了Telegraf的基本用法，一定要看！！！，大家亦可参考。\nTelegraf下载地址在这里，根据自己的平台选择对应的二进制下载即可。笔者的环境是CentOS，下面是安装脚本，/opt/telegraf/telegraf.conf 是一个经过删减的干净的配置文件，指定了opentsdb output plugin，这个plugin的写入地址配置的是n9e-server，所以，Telegraf采集的数据会被推送给n9e-server，二者贯通：\n#!/bin/sh  version=1.20.4 tarball=telegraf-${version}_linux_amd64.tar.gz wget https://dl.influxdata.com/telegraf/releases/$tarball tar xzvf $tarball mkdir -p /opt/telegraf cp -far telegraf-${version}/usr/bin/telegraf /opt/telegraf cat \u003c\u003cEOF \u003e /opt/telegraf/telegraf.conf [global_tags] [agent] interval = \"10s\" round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \"0s\" flush_interval = \"10s\" flush_jitter = \"0s\" precision = \"\" hostname = \"\" omit_hostname = false [[outputs.opentsdb]] host = \"http://127.0.0.1\" port = 19000 http_batch_size = 50 http_path = \"/opentsdb/put\" debug = false separator = \"_\" [[inputs.cpu]] percpu = true totalcpu = true collect_cpu_time = false report_active = true [[inputs.disk]] ignore_fs = [\"tmpfs\", \"devtmpfs\", \"devfs\", \"iso9660\", \"overlay\", \"aufs\", \"squashfs\"] [[inputs.diskio]] [[inputs.kernel]] [[inputs.mem]] [[inputs.processes]] [[inputs.system]] fielddrop = [\"uptime_format\"] [[inputs.net]] ignore_protocol_stats = true EOF cat \u003c\u003cEOF \u003e /etc/systemd/system/telegraf.service [Unit] Description=\"telegraf\" After=network.target [Service] Type=simple ExecStart=/opt/telegraf/telegraf --config telegraf.conf WorkingDirectory=/opt/telegraf SuccessExitStatus=0 LimitNOFILE=65536 StandardOutput=syslog StandardError=syslog SyslogIdentifier=telegraf KillMode=process KillSignal=SIGQUIT TimeoutStopSec=5 Restart=always [Install] WantedBy=multi-user.target EOF systemctl daemon-reload systemctl enable telegraf systemctl restart telegraf systemctl status telegraf  Warning/opt/telegraf/telegraf.conf的内容是个删减版，只是为了让大家快速跑起来，如果要采集更多监控对象，比如mysql、redis、tomcat等，还是要仔细去阅读从tarball里解压出来的那个配置文件，那里有很详细的注释，也可以参考官方提供的各个采集插件下的README\n 💡 Telegraf告警策略 | Telegraf监控大盘\n",
    "description": "",
    "tags": null,
    "title": "使用Telegraf采集监控数据",
    "uri": "/quickstart/telegraf/"
  },
  {
    "content": "在本文档中，介绍如何以Deployment或者Daemonset的方式部署grafana-agent到您的k8s集群中，抓取宿主机上kubelet和cAdvisor的metrics指标，并把抓取到的数据，以remote_write的方式推送到Nightingale.\n通过本文档，我们预期达成以下目标：\n 部署grafana-agent到您的K8s集群中； 配置grafana-agent抓取kubelet和cAdvisor的metrics；  K8s是开源的容器编排系统，自动化管理容器的部署、扩缩容等工作。K8s默认会暴露Node和控制面的若干metrics接口，这些接口兼容Prometheus的metrics规范。我们可以部署grafana-agent来收集Node的cAdvisor和kubelet metrics，并以remote_write的方式发送到Nightingale.\n前置依赖  一个开启RBAC（role-based access control）的Kubernetes集群； 安装并配置好了kubectl命令行工具；  步骤一：创建 ServiceAcount、ClusterRole、ClusterRoleBinding export NAMESPACE=default MANIFEST_URL=https://raw.githubusercontent.com/flashcatcloud/fc-agent/fc-release/etc/k8s/agent-bare.yaml curl -fsSL $MANIFEST_URL | envsubst | kubectl apply -f - 步骤二：创建ConfigMap，配置grafana-agent export NAMESPACE=default export CLUSTER_NAME=kubernetes export FC_REMOTE_WRITE_URL=http://10.206.0.16:8480/insert/0/prometheus/api/v1/write #export FC_REMOTE_WRITE_URL=https://n9e-server:19000/prometheus/v1/write #export FC_REMOTE_WRITE_USERNAME=fc_laiwei #export FC_REMOTE_WRITE_PASSWORD=fc_laiweisecret cat \u003c\u003cEOF | kind: ConfigMap metadata: name: grafana-agent apiVersion: v1 data: agent.yaml: | server: http_listen_port: 12345 metrics: wal_directory: /tmp/grafana-agent-wal global: scrape_interval: 15s scrape_timeout: 10s external_labels: cluster: ${CLUSTER_NAME} configs: - name: integrations remote_write: - url: ${FC_REMOTE_WRITE_URL} basic_auth: username: ${FC_REMOTE_WRITE_USERNAME} password: ${FC_REMOTE_WRITE_PASSWORD} scrape_configs: - job_name: integrations/kubernetes/cadvisor bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node metric_relabel_configs: - action: drop regex: container_([a-z_]+); source_labels: - __name__ - image - action: drop regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s) source_labels: - __name__ relabel_configs: - replacement: kubernetes.default.svc:443 target_label: __address__ - regex: (.+) replacement: /api/v1/nodes/\\$1/proxy/metrics/cadvisor source_labels: - __meta_kubernetes_node_name target_label: __metrics_path__ scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false server_name: kubernetes - job_name: integrations/kubernetes/kubelet bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - replacement: kubernetes.default.svc:443 target_label: __address__ - regex: (.+) replacement: /api/v1/nodes/\\$1/proxy/metrics source_labels: - __meta_kubernetes_node_name target_label: __metrics_path__ scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: false server_name: kubernetes EOF envsubst | kubectl apply -n $NAMESPACE -f - 步骤三：在K8s中创建grafana-agent实例  Daemonset\n 对于采集 node_exporter/ kubelet/ cAdvisor等指标，每个节点上只运行一个grafana-agent实例的情况，推荐以daemonset运行 export NAMESPACE=default MANIFEST_URL=https://raw.githubusercontent.com/flashcatcloud/fc-agent/fc-release/etc/k8s/agent-daemonset.yaml curl -fsSL $MANIFEST_URL | envsubst | kubectl apply -f -  Deployment\n 对于采集MySQLd_Exporter等需要运行多个grafana-agent实例的情况，推荐以deployment运行。 export NAMESPACE=default MANIFEST_URL=https://raw.githubusercontent.com/flashcatcloud/fc-agent/fc-release/etc/k8s/agent-deployment.yaml curl -fsSL $MANIFEST_URL | envsubst | kubectl apply -f - 如何重建grafana-agent  Daemonset\n kubectl rollout restart daemonset/grafana-agent  Deployment\n kubectl rollout restart deployment/grafana-agent 至此，我们已经完成了在K8s中部署grafana-agent并收集metrics，进一步，我们还可以配置grafana-agent来建立起完整的kubernetes指标监控体系。\n",
    "description": "",
    "tags": null,
    "title": "在K8s中运行grafana-agent收集metrics",
    "uri": "/grafana-agent/k8s_metrics/"
  },
  {
    "content": " VictoriaMetrics 架构简单，可靠性高，在性能，成本，可扩展性方面表现出色，社区活跃，且和 Prometheus 生态绑定紧密。夜莺推荐您在生产环境中使用 VictoriaMetrics 作为时序数据库。\n VictoriaMetrics 提供单机版和集群版。如果您的每秒写入数据点数小于100万，VictoriaMetrics 官方默认推荐您使用单机版，单机版可以通过增加服务器的CPU核心数，增加内存，增加IOPS来获得线性的性能提升。且单机版易于配置和运维。\n接下来的文章，介绍在夜莺中，以 VictoriaMetrics 集群版本作为时序数据库为例，完整的安装和配置过程。\nvmstorage、vminsert、vmselect 三者组合构成 VictoriaMetrics 的集群功能，三者都可以通过启动多个实例来分担承载流量。\n vmstorage 是数据存储模块：\n  其数据保存在-storageDataPath指定的目录中，默认为./vmstorage-data/，vmstorage 是有状态模块，删除 storage node 会丢失约 1/N的历史数据（N 为集群中 vmstorage node 的节点数量）。增加 storage node，则需要同步修改 vminsert 和 vmselect 的启动参数，将新加入的storage node节点地址通过命令行参数 -storageNode传入给vminsert和vmselect。 vmstorage 启动后，会监听三个端口，分别是 -httpListenAddr :8482、-vminsertAddr :8400、-vmselectAddr :8401。端口8400负责接收来自 vminsert 的写入请求，端口8401负责接收来自 vmselect 的数据查询请求，端口8482则是 vmstorage 自身提供的 http api 接口。   vminsert 接收来自客户端的数据写入请求，并负责转发到选定的vmstorage：\n  vminsert 接收到数据写入请求后，按照 jump consistent hash 算法，将数据转发到选定的某个vmstorage node 上。vminsert 本身是无状态模块，可以增加或者删除一个或多个实例，而不会造成数据的损失。vminsert 模块通过启动时的参数 -storageNode xxx,yyy,zzz 来感知到整个 vmstorage 集群的完整 node 地址列表。 vminsert 启动后，会监听一个端口-httpListenAddr :8480。该端口实现了 prometheus remote_write协议，因此可以接收和解析通过 remote_write 协议写入的数据。不过要注意，VictoriaMetrics 集群版本具有多租户功能，因此租户ID会以如下形式出现在 API URL 中: http://vminsert:8480/insert/\u003caccount_id\u003e/prometheus/api/v1/write。 更多 URL Format 可以参考 VictoriaMetrics官网。   vmselect 接收来自客户端的数据查询请求，并负责转发到所有的 vmstorage 查询结果并合并：\n  vmselect 启动后，会监听一个端口-httpListenAddr :8481。该端口实现了 prometheus remote_query等协议，因此可以接收和解析 remote_query 协议的查询。不过要注意，VictoriaMetrics 集群版本具有多租户功能，因此租户ID会以如下形式出现在 API URL 中: http://vminsert:8481/select/\u003caccount_id\u003e/prometheus/api/v1/query。 更多 URL Format 可以参考 VictoriaMetrics官网。  下载和安装 VictoriaMetrics 集群版  去 vm release 下载编译好的二进制版本，比如我们选择下载 v1.69.0 amd64。 解压缩后得到：  $ls -l vm*-prod -rwxr-xr-x 1 work work 10946416 Nov 8 22:03 vminsert-prod* -rwxr-xr-x 1 work work 13000624 Nov 8 22:03 vmselect-prod* -rwxr-xr-x 1 work work 11476736 Nov 8 22:03 vmstorage-prod*  启动三个 vmstorage 实例(可以用下面的脚本快速生成不同实例的启动命令)：  #!/bin/bash  for i in `seq 0 2`; do if [ $i -eq 0 ]; then i=\"\" fi pp=$i httpListenAddr=${pp}8482 vminsertAddr=${pp}8400 vmselectAddr=${pp}8401 storageDataPath=./${pp}vmstorage-data prog=\"nohup ./vmstorage-prod -loggerTimezone Asia/Shanghai \\ -storageDataPath $storageDataPath\\ -httpListenAddr :$httpListenAddr\\ -vminsertAddr :$vminsertAddr\\ -vmselectAddr :$vmselectAddr\\ \u0026\u003e ${pp}vmstor.log \u0026\" echo $prog (exec \"$prog\") done 也可以输入以下命令行启动三个实例：\nnohup ./vmstorage-prod -loggerTimezone Asia/Shanghai -storageDataPath ./vmstorage-data -httpListenAddr :8482 -vminsertAddr :8400 -vmselectAddr :8401 \u0026\u003e vmstor.log \u0026 nohup ./vmstorage-prod -loggerTimezone Asia/Shanghai -storageDataPath ./1vmstorage-data -httpListenAddr :18482 -vminsertAddr :18400 -vmselectAddr :18401 \u0026\u003e 1vmstor.log \u0026 nohup ./vmstorage-prod -loggerTimezone Asia/Shanghai -storageDataPath ./2vmstorage-data -httpListenAddr :28482 -vminsertAddr :28400 -vmselectAddr :28401 \u0026\u003e 2vmstor.log \u0026  启动一个 vminsert 实例：  nohup ./vminsert-prod -httpListenAddr :8480 -storageNode=127.0.0.1:8400,127.0.0.1:18400,127.0.0.1:28400 \u0026\u003evminsert.log \u0026  启动一个 vmselect 实例：  nohup ./vmselect-prod -httpListenAddr :8481 -storageNode=127.0.0.1:8401,127.0.0.1:18401,127.0.0.1:28401 \u0026\u003evmselect.log \u0026  查看 vmstorage，vminsert，vmselect 的 /metrics 接口:  curl http://127.0.0.1:8482/metrics curl http://127.0.0.1:18482/metrics curl http://127.0.0.1:28482/metrics curl http://127.0.0.1:8481/metrics curl http://127.0.0.1:8480/metrics  n9e-server通过remote write接口写入时序库，vm作为时序库的一个选择，其remote write接口地址为：http://127.0.0.1:8480/insert/0/prometheus/api/v1/write 把这个地址配置到server.conf当中即可，配置完了重启n9e-server  # Reader部分修改Url [Reader] Url = \"http://172.21.0.8:8481/select/0/prometheus\" # Writers部分修改Url [[Writers]] Url = \"http://172.21.0.8:8480/insert/0/prometheus/api/v1/write\"  修改您的 n9e-webapi 的配置文件 ./etc/webapi.conf 如下：  [[Clusters]] # Prometheus cluster name Name = \"Default\" # Prometheus APIs base url Prom = \"http://127.0.0.1:8481/select/0/prometheus\" 然后，重新启动n9e-webapi，这样夜莺就可以通过 remote query 查询到 victoriametrics 集群的数据了。\nInfon9e-webapi 的安装、配置和启动，请参考 这里。\n FAQ  VictoriaMetrics 单机版本如何保障数据的可靠性？  vm 针对磁盘IO有针对性的优化，单机版可以考虑将数据的可靠性保障交给 EBS 等云盘来保证。\n  VictoriaMetrics 如何评估容量？  参考vm的官方文档。\n  VictoriaMetrics 集群版本增加或者删除vmstorage Node的时候，数据如何再平衡？  vm 不支持扩缩容节点时，对数据进行自动的再平衡。\n  VictoriaMetrics 的数据大小如何查看？  可以通过 vmstorage 实例暴露的 /metrics 接口来获取到相应的统计数据，譬如：\n $ curl http://127.0.0.1:8482/metrics |grep -i data_size vm_data_size_bytes{type=\"indexdb\"} 609291 vm_data_size_bytes{type=\"storage/big\"} 0 vm_data_size_bytes{type=\"storage/small\"} 8749893  vminsert 在将数据写入多个 vmstorage Node的时候，是按照什么规则将数据写入到不同的 node 上的？  采用jump consistent hash 对数据进行分片，写入到相应的storage node上。\n  vmselect 在接到查询请求的时候，如何定位到请求的数据是在哪个 storage node上的？  vmselect 并不知道每个metrics对应的数据分布的storage node，vmselect会对所有的storage node发起查询请求，最后进行数据合并，并返回。\n  VictoriaMetrics 和 M3db 的对比和选择？  m3db架构设计上更高级，实现难度高，m3db在时序数据功能之后，重点解决了自动扩缩容，数据自动平衡等运维难题。但是因此也更复杂，可靠性目前也更难保证。VictoriaMetrics架构设计上的tradeoff 更倾向于简单可靠，重点优化了单机版的性能，强调垂直扩展，同时和prometheus 生态做到兼容，甚至于在很多的点上做到了加强。但是 VictoriaMetrics 对于时序数据downsample，节点的自动扩缩容，数据自动再平衡等高级功能和分布式能力，是有缺失的。\n   Info如果您使用的是VictoriaMetrics单机版，nightingale 的配置文件需要做如下调整：\n # Reader部分修改为： [Reader] Url = \"http://127.0.0.1:8428\" # Writers部分修改为： [[Writers]] Url = \"http://127.0.0.1:8428/api/v1/write\" # Clusters部分修改为： [[Clusters]] # Prometheus cluster name Name = \"Default\" # Prometheus APIs base url Prom = \"http://127.0.0.1:8428\" 相关资料  使用 Docker Compose 快速部署 VictoriaMetrics。 使用 Helm Chart 快速在 Kubernetes中部署 VictoriaMetrics。 使用 VictoriaMetrics Operator 在 Kubernetes中部署 VictoriaMetrics。 VictoriaMetrics 集群版架构：   ",
    "description": "",
    "tags": null,
    "title": "使用VictoriaMetrics作为时序库",
    "uri": "/quickstart/victoriametrics/"
  },
  {
    "content": "在本文档中，介绍如何以Daemonset的形式部署grafana-agent到您的k8s集群中，收集您的K8s集群中应用的日志，并将其推送到Nightingale.\n",
    "description": "",
    "tags": null,
    "title": "在K8s中运行grafana-agent收集log",
    "uri": "/grafana-agent/k8s_logs/"
  },
  {
    "content": "在本文档中，介绍如何以 Deployment 的形式部署grafana-agent到您的K8s集群中，收集您的K8s集群中应用的trace数据，并将其推送到Nightingale.\n",
    "description": "",
    "tags": null,
    "title": "在K8s中运行grafana-agent收集trace",
    "uri": "/grafana-agent/k8s_traces/"
  },
  {
    "content": "由于Prometheus没有集群版本，受限于容量问题，很多公司会搭建多套Prometheus，比如按照业务拆分，不同的业务使用不同的Prometheus集群，或者按照地域拆分，不同的地域使用不同的Prometheus集群。这里是以Prometheus来举例，VictoriaMetrics、M3DB都有集群版本，不过有时为了不相互干扰和地域网络问题，也会拆成多个集群。对于多集群的协同，需要在夜莺里做一些配置，回顾一下夜莺的架构图：\n图上分了 3 个 region，每个 region 一套时序库，每个 region 一套 n9e-server，n9e-server 依赖 redis，所以每个 region 一个 redis，n9e-webapi 和 mysql 放到中心，n9e-webapi 也依赖一个 redis，所以中心端放置的是 n9e-webapi、redis、mysql，如果想图省事，redis 也是可以复用的，各个 region 的 n9e-server 都连接中心的 redis 也是可以的。\n为了高可用，各个 region 的 n9e-server 可以多部署几个实例组成一个集群，集群中的所有 n9e-server 的配置文件 server.conf 中的 ClusterName 要设置成一样的字符串。\n假设，我们有两个时序库，在北京搭建了一个 Prometheus，在广州搭建了一个 VictoriaMetrics，n9e-webapi 会把这两个时序库作为 DataSource，所以在 n9e-webapi 的配置文件中，要配置上这俩存储的地址，举例：\n[[Clusters]] # cluster name Name = \"Prom-Beijing\" # Prometheus APIs base url Prom = \"http://10.2.3.4:9090\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 100 [[Clusters]] # cluster name Name = \"VM-Guangzhou\" # Prometheus APIs base url Prom = \"http://172.21.0.8:8481/select/0/prometheus\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 100 另外图上也可以看出，一个 n9e-server 对应一个时序库，所以在 n9e-server 的配置文件中，也需要配置对应的时序库的地址，比如北京的 server，配置如下，Writers 下面的 Url 配置的是 remote write 的地址，而 Reader 下面配置的 Url 是实现Prometheus 原生查询接口的 BaseUrl。\n[Reader] # prometheus base url Url = \"http://127.0.0.1:9090\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 10 [[Writers]] Url = \"http://127.0.0.1:9090/api/v1/write\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 100 假设上海区域用的是 VictoriaMetrics，所以 Url 略有不同，配置如下：\n[Reader] # prometheus base url Url = \"http://127.0.0.1:8481/select/0/prometheus\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 10 [[Writers]] Url = \"http://127.0.0.1:8480/insert/0/prometheus/api/v1/write\" # Basic auth username BasicAuthUser = \"\" # Basic auth password BasicAuthPass = \"\" # timeout settings, unit: ms Timeout = 30000 DialTimeout = 10000 TLSHandshakeTimeout = 30000 ExpectContinueTimeout = 1000 IdleConnTimeout = 90000 # time duration, unit: ms KeepAlive = 30000 MaxConnsPerHost = 0 MaxIdleConns = 100 MaxIdleConnsPerHost = 100 n9e-webapi 是要响应前端 ajax 请求的，前端会从 n9e-webapi 查询监控数据，n9e-webapi 自身不存储监控数据，而是仅仅做了一个代理，把请求代理给后端的时序库，前端读取数据时会调用 Prometheus 的那些原生接口，即：/api/v1/query /api/v1/query_range /api/v1/labels 这种接口，所以注意啦，n9e-webapi 中配置的 Clusters 下面的Url，都是要支持Prometheus 原生接口的 BaseUrl。\n对于 n9e-server，有两个重要作用，一个是接收监控数据，然后转发给后端多个 Writer，所以，Writer 可以配置多个，配置文件是 toml 格式，[[Writers]]双中括号这种就表示数组，数据写给后端存储，走的协议是 Prometheus 的 Remote Write，所以，所有支持 Remote Write 的存储，都可以使用。n9e-server 的另一个重要作用，是做告警判断，会周期性从 mysql 同步告警规则，然后根据用户配置的 PromQL 调用时序库的 query 接口，所以 n9e-server 的 Reader 下面的 Url，也是要配置支持 Prometheus 原生接口的 BaseUrl。另外注意，Writer 可以配置多个，但是 Reader 只能配置一个。比如监控数据可以写一份到Prometheus 存储近期数据用于告警判断，再写一份到 OpenTSDB 存储长期数据，Writer 就可以配置为 Prometheus 和 OpenTSDB 这两个，而 Reader 只配置 Prometheus 即可。\n",
    "description": "",
    "tags": null,
    "title": "接入多个Prom/VM/M3DB集群",
    "uri": "/quickstart/multitsdb/"
  },
  {
    "content": "对于规模相对较小的公司，比如几百台机器这个体量，个人认为单机版足够用了，使用云主机部署，性能不足可以直接升配，存储使用云存储保证，硬件故障云平台也会自动把虚拟机热迁移走，非常省心。那如果咱们体量确实比较大，或者没有云主机这种基础设施，这里会讲解集群版的部署方式。\n时序库 时序库的集群部署，就看时序库自身的机制了，这里不展开，如果是使用 Prometheus，Prometheus 没有集群版，VictoriaMetrics、M3DB、Thanos 等都是有集群版的，请参考他们的文档\nMySQL MySQL 一般部署成主从，这个请你们的 DBA 来搞吧，或者直接使用云上 RDS，这里就不展开了\nRedis 夜莺当前 n9e-webapi 和 n9e-server 都依赖 redis，redis 一般有三种模式，单机模式、集群模式、哨兵模式，夜莺当前只支持单机模式。具体使用几个 redis，要看大家的环境情况，如果图省事，全局就使用一个 redis 也是可以的。也可以把 n9e-webapi 用的 redis 和 n9e-server 用的 redis 分开，可以参考文档：接入多个 Prom/VM/M3DB 集群\nn9e-webapi 这个模块是放中心的，可以部署多个实例，前面统一放置 nginx 或者 lvs，某个 n9e-webapi 实例如果挂了，nginx、lvs 都可以自动摘除，保证了高可用。\nn9e-server n9e-server 是随着时序库走的，每个时序库对应一套 n9e-server，一套 n9e-server 可以只有一个 n9e-server 实例，也可以部署多个保证高可用和性能。同一套 n9e-server 内部的多个实例，其配置文件 server.conf 中的 ClusterName 字段，要配置成一样的字符串。\n",
    "description": "",
    "tags": null,
    "title": "生产环境部署高可用集群版",
    "uri": "/quickstart/clusters/"
  },
  {
    "content": "首页有个架构图，大家可以看到，夜莺把接收到的监控数据都直接写入了后端时序数据库，所以，读取监控数据，无需经由夜莺的接口，直接读取后端的时序库的接口就可以了。即：如果使用了 Prometheus，就通过 Prometheus 的接口读取监控数据，如果用了 VictoriaMetrics，就通过 VictoriaMetrics 的接口读取监控数据。\n比如 Prometheus，就是那些/api/v1/query /api/v1/query_range之类的接口。相关接口文档请参考：Prometheus官网\n",
    "description": "",
    "tags": null,
    "title": "读取监控数据",
    "uri": "/api/read/"
  },
  {
    "content": "概述 所谓的告警自愈，典型手段是在告警触发时自动回调某个webhook地址，在这个webhook里写告警自愈的逻辑，夜莺默认支持这种方式。另外，夜莺还可以更进一步，配合ibex这个模块，在告警触发的时候，自动去告警的机器执行某个脚本，这种机制可以大幅简化构建运维自愈链路的工作量，毕竟，不是所有的运维人员都擅长写http server，但所有的运维人员，都擅长写脚本。这种方式是典型的物理机时代的产物，希望各位朋友用不到这个工具（说明贵司的IT技术已经走得非常靠前了）。\n架构 ibex模块，类似之前夜莺v3版本中的job模块，可以批量执行脚本，其架构非常简单，包括server和agentd两个模块，agentd周期性调用server的rpc接口，询问有哪些任务要执行，如果有分配给自己的任务，就从server拿到任务脚本信息，在本地fork一个进程运行，然后将结果上报给服务端。为了简化部署，server和agentd融合成了一个二进制，就是ibex，通过传入不同的参数来启动不同的角色。ibex架构图如下：\n项目地址  Git仓库：https://gitee.com/cnperl/ibex 编译方法看这里 Linux 下编译好的包 在这里  安装启动 下载安装包之后，解压缩，在etc下可以找到服务端和客户端的配置文件，在sql目录下可以找到初始化sql脚本。\n初始化sql mysql \u003c sql/ibex.sql 启动server server的配置文件是etc/server.conf，注意修改里边的mysql连接地址，配置正确的mysql用户名和密码。然后就可以直接启动了：\nnohup ./ibex server \u0026\u003e server.log \u0026 ibex没有web页面，只提供api接口，鉴权方式是http basic auth，basic auth的用户名和密码默认都是ibex，在etc/server.conf中可以找到，如果ibex部署在互联网，一定要修改默认用户名和密码，当然，因为n9e要调用ibex，所以n9e的server.conf和webapi.conf中也配置了ibex的basic auth账号信息，要改就要一起改啦。\n启动agentd 客户端的配置非常非常简单，agentd.conf内容如下：\n# debug, release RunMode = \"release\" # task meta storage dir MetaDir = \"./meta\" [HTTP] Enable = true # http listening address Host = \"0.0.0.0\" # http listening port Port = 2090 # https cert file path CertFile = \"\" # https key file path KeyFile = \"\" # whether print access log PrintAccessLog = true # whether enable pprof PProf = false # http graceful shutdown timeout, unit: s ShutdownTimeout = 30 # max content length: 64M MaxContentLength = 67108864 # http server read timeout, unit: s ReadTimeout = 20 # http server write timeout, unit: s WriteTimeout = 40 # http server idle timeout, unit: s IdleTimeout = 120 [Heartbeat] # unit: ms Interval = 1000 # rpc servers Servers = [\"10.2.3.4:20090\"] # $ip or $hostname or specified string Host = \"telegraf01\" 客户端的HTTP接口用处不大，可以把Enable设置为false，关闭监听，重点关注Heartbeat这个部分，Interval是心跳频率，默认是1000毫秒，如果机器量比较小，比如小于1000台，维持1000没问题，如果机器量比较大，可以适当调大这个频率，比如2000或者3000，可以减轻服务端的压力。Servers是个数组，配置的是ibex-server的地址，ibex-server可以启动多个，多个地址都配置到这里即可，Host这个字段，是本机的唯一标识，有三种配置方式，如果配置为$ip，系统会自动探测本机的IP，如果是$hostname，系统会自动探测本机的hostname，如果是其他字符串，那就直接把该字符串作为本机的唯一标识。每个机器上都要部署ibex-agentd，不同的机器要保证Host字段获取的内容不能重复。\n另外，Telegraf的配置文件中，有下面这么一段：\n[agent] interval = \"10s\" round_interval = true metric_batch_size = 1000 metric_buffer_limit = 10000 collection_jitter = \"0s\" flush_interval = \"10s\" flush_jitter = \"0s\" precision = \"\" hostname = \"\" omit_hostname = false 其中hostname默认留空，表示自动探测本机的hostname，如果写了具体的某个字符串，那就把写的那个字符串作为监控数据的host字段的内容，这个hostname字段要和ibex的agentd.conf中的Host字段保持一致，典型的做法有：\n Telegraf中把hostname配置为空，Telegraf自动获取本机hostname，ibex的Host配置为$hostname，ibex也会自动获取本机hostname，这样Telegraf和ibex可以获取到相同的标识内容 Telegraf中手工把hostname配置为本机的ip，ibex则把Host配置为$ip，这样二者也可以获取到相同的标识内容 Telegraf和ibex都使用某个特定写死的字符串来作为标识信息，这样也OK，但是要保证不同的机器，这个字符串不能重复  下面是启动ibex-agentd的命令：\nnohup ./ibex agentd \u0026\u003e agentd.log \u0026 另外，细心的读者应该会发现ibex的压缩包里的etc目录下有个service目录，里边准备好了两个service样例文件，便于大家使用systemd来管理ibex进程，生产环境，建议使用systemd来管理。\n",
    "description": "",
    "tags": null,
    "title": "告警自愈依赖的脚本下发执行模块",
    "uri": "/quickstart/ibex/"
  },
  {
    "content": "调用 n9e-server 的 /opentsdb/put 接口，POST 方法，该接口实现了 OpenTSDB 的数据协议，监控数据做成 JSON 放到 HTTP Request Body 中，举例：\n[ { \"metric\": \"cpu_usage_idle\", \"timestamp\": 1637732157, \"tags\": { \"cpu\": \"cpu-total\", \"ident\": \"c3-ceph01.bj\" }, \"value\": 30.5 }, { \"metric\": \"cpu_usage_util\", \"timestamp\": 1637732157, \"tags\": { \"cpu\": \"cpu-total\", \"ident\": \"c3-ceph01.bj\" }, \"value\": 69.5 } ] 显然，JSON 最外层是个数组，如果只上报一条监控数据，也可以不要外面的中括号，直接把对象结构上报：\n{ \"metric\": \"cpu_usage_idle\", \"timestamp\": 1637732157, \"tags\": { \"cpu\": \"cpu-total\", \"ident\": \"c3-ceph01.bj\" }, \"value\": 30.5 } 服务端会看第一个字符是否是[，来判断上报的是数组，还是单个对象，自动做相应的 Decode。如果觉得上报的内容太过占用带宽，也可以做 gzip 压缩，此时上报的数据，要带有Content-Encoding: gzip的 Header。\nInfo注意 ident 这个标签，ident 是 identity 的缩写，表示设备的唯一标识，如果标签中有 ident 标签，n9e-server 就认为这个监控数据是来自某个机器的，会自动获取 ident 的 value，注册到监控对象的列表里\n ",
    "description": "",
    "tags": null,
    "title": "推送监控数据（OpenTSDB协议）",
    "uri": "/api/opentsdb/"
  },
  {
    "content": " 本节讲述 Nightingale 的源码编译方式，分前后端两部分。另外，如果用到告警自愈模块，会用到 ibex 这个模块，本节也会一并讲解 ibex 模块的编译方法。对于 ARM 的处理器，我们没有提供编译好的二进制，大家就要用下面的方法自行编译了。\n 前端 git clone https://github.com/n9e/fe-v5.git cd fe-v5 npm install npm run build 后端 Nightingale 后端采用 Go 语言编写，编译的前置条件就是配置 Go 的开发环境。\n配置Go环境 到Go官网选择对应的版本下载，我的环境是Linux，选择的go1.17.3.linux-amd64.tar.gz，直接下载到/root目录下了，然后解压缩，即Go的内容都放到了/root/go目录下了。同时准备gopath目录，如下：\ncd /root \u0026\u0026 mkdir -p gopath/src echo \"GOROOT=/root/go\" \u003e\u003e .bash_profile echo \"GOPATH=/root/gopath\" \u003e\u003e .bash_profile echo 'export PATH=$GOROOT/bin:$GOPATH/bin:$PATH' \u003e\u003e .bash_profile source .bash_profile 编译n9e git clone https://github.com/didi/nightingale.git # 国内配置一下代理，可以加速编译 export GOPROXY=https://goproxy.cn # 执行编译 cd nightingale \u0026\u0026 make 编译完成之后如果生成二进制：n9e，就表示编译成功！想要快速入门Go语言？可以参考GOCN的资料！\n编译ibex 如果需要告警自愈能力，夜莺依赖ibex做命令下发执行，ibex的编译和n9e几乎一模一样，如下：\ngit clone https://gitee.com/cnperl/ibex.git # 国内配置一下代理，可以加速编译 export GOPROXY=https://goproxy.cn # 执行编译 cd ibex \u0026\u0026 make 编译完成之后如果生成二进制：ibex，就表示编译成功！\n",
    "description": "",
    "tags": null,
    "title": "源码编译夜莺前后端及告警自愈模块",
    "uri": "/quickstart/compile/"
  },
  {
    "content": "简介 n9e-webapi 模块提供了两类接口，一个是 /api/n9e 打头的，给前端调用，另一类是 /v1/n9e 打头的，给第三方系统调用。如果想以个人身份模仿WEB操作，也是调用 /api/n9e 相关接口。\n以个人身份模仿WEB操作 这种方式，页面上 JavaScript 可以调用的所有接口，你都可以用程序调用，打开 chrome 的开发者工具，扒拉这些接口，还是非常容易的。当然，要先登录，登录调用 webapi 模块的 /api/n9e/auth/login 接口，系统使用 jwt 认证，如果登录成功，会返回 access_token 和 refresh_token，每次调用的时候都要把 access_token 放到 Header 里，access_token 差不多15分钟过期，之后可以重新调用登录接口换 token，也可以调用 /api/n9e/auth/refresh 接口用 refresh_token 换一个新的 access_token，当然，也会顺道返回一个新的 refresh_token，举例：\n# 调用登录接口拿到access_token和refresh_token记录下来，后面调用其他接口的时候会用到 [root@10-255-0-34 ~]# curl -X POST 'http://localhost:18000/api/n9e/auth/login' -d '{\"username\": \"root\", \"password\": \"root.2020\"}' {\"dat\":{\"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NfdXVpZCI6ImIxNTcyMjgwLWZlNzAtNDhjZi1hNDQ3LWVlMjVhZmYwMjRhZCIsImF1dGhvcml6ZWQiOnRydWUsImV4cCI6MTYzNzgyMzA1OSwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.nJ56Pc7qS5Ik_UaVmlNWu_QlABaBc4pZ_WkU45u2wWk\",\"refresh_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzgzMzc4NTksInJlZnJlc2hfdXVpZCI6ImIxNTcyMjgwLWZlNzAtNDhjZi1hNDQ3LWVlMjVhZmYwMjRhZCsrMS1yb290IiwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.JKsbfTYBCOOfR_oPsf496N9ml9yXbP7BHb4E8Yfnzbo\",\"user\":{\"id\":1,\"username\":\"root\",\"nickname\":\"超管\",\"phone\":\"\",\"email\":\"\",\"portrait\":\"\",\"roles\":[\"Admin\"],\"contacts\":{},\"create_at\":1637545881,\"create_by\":\"system\",\"update_at\":1637546351,\"update_by\":\"root\",\"admin\":true}},\"err\":\"\"} # access_token放到Authorization这个Header里，Bearer的验证方式 [root@10-255-0-34 ~]# curl -H \"Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NfdXVpZCI6ImIxNTcyMjgwLWZlNzAtNDhjZi1hNDQ3LWVlMjVhZmYwMjRhZCIsImF1dGhvcml6ZWQiOnRydWUsImV4cCI6MTYzNzgyMzA1OSwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.nJ56Pc7qS5Ik_UaVmlNWu_QlABaBc4pZ_WkU45u2wWk\" 'http://localhost:18000/api/n9e/self/profile' {\"dat\":{\"id\":1,\"username\":\"root\",\"nickname\":\"超管\",\"phone\":\"\",\"email\":\"\",\"portrait\":\"\",\"roles\":[\"Admin\"],\"contacts\":{},\"create_at\":1637545881,\"create_by\":\"system\",\"update_at\":1637546351,\"update_by\":\"root\",\"admin\":true},\"err\":\"\"} # 如果token过期了，后端会返回异常HTTP状态码，此时要调用refresh接口换取新的token [root@10-255-0-34 ~]# curl -X POST 'http://localhost:18000/api/n9e/auth/refresh' -d '{\"refresh_token\": \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzgzMzc4NTksInJlZnJlc2hfdXVpZCI6ImIxNTcyMjgwLWZlNzAtNDhjZi1hNDQ3LWVlMjVhZmYwMjRhZCsrMS1yb290IiwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.JKsbfTYBCOOfR_oPsf496N9ml9yXbP7BHb4E8Yfnzbo\"}' {\"dat\":{\"access_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NfdXVpZCI6IjAxMzkzYzkxLTk5MWItNGE0Yi04ODk2LTJhZGRjMDUwYjcxMCIsImF1dGhvcml6ZWQiOnRydWUsImV4cCI6MTYzNzgyMzMxOCwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.2BeWyYfcnRi3qw69zecaaeFnPFUNAGsiPIZBBnd5lug\",\"refresh_token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2MzgzMzgxMTgsInJlZnJlc2hfdXVpZCI6IjAxMzkzYzkxLTk5MWItNGE0Yi04ODk2LTJhZGRjMDUwYjcxMCsrMS1yb290IiwidXNlcl9pZGVudGl0eSI6IjEtcm9vdCJ9.zFZaRYcJI6G5maSgDVF-jZzxQ3Tb5dybIqufJhBy034\"},\"err\":\"\"} 第三方系统调用夜莺 比如第三方系统想获取夜莺中的所有未恢复告警，或者获取夜莺中的全量用户列表，这些需求，建议走 /v1/n9e 打头的接口，这些接口走 BasicAuth 认证，BasicAuth 的用户名和密码在 webapi.conf 中可以找到，就是 BasicAuth 那个 section 的配置。当前这个阶段，/v1/n9e 前缀的接口还比较少，不过代码框架已经搭起来了，代码在 src/webapi/router/router.go 文件中，如果贵司要封装夜莺的接口，可能要在这个路由分组下加一些路由配置了。作为开源软件，说清楚原理就好了，如果贵司仍然搞不明白可以联系我们，我们提供商业技术支持服务 :-)\n",
    "description": "",
    "tags": null,
    "title": "调用webapi的接口",
    "uri": "/api/webapi/"
  },
  {
    "content": "夜莺无需对接 Grafana，夜莺会把监控数据转存到后端时序库，比如 Prometheus、VictoriaMetrics、M3DB 等，把这些时序库配置为 Grafana 的数据源即可。\n",
    "description": "",
    "tags": null,
    "title": "对接Grafana",
    "uri": "/usage/grafana/"
  },
  {
    "content": "监控 Linux 常用的有两种方式，一个是通过部署 Telegraf，一个是通过 node_exporter，关注三个层面的问题：怎么部署？配置哪些告警规则？监控大盘如何配置？\nTelegraf  部署方式：使用Telegraf采集监控数据 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/linux_by_telegraf.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/linux_by_telegraf.json  node_exporter  部署方式：https://github.com/prometheus/node_exporter 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/linux_by_exporter.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/linux_by_exporter.json  ",
    "description": "",
    "tags": null,
    "title": "监控Linux",
    "uri": "/usage/linux/"
  },
  {
    "content": "推荐使用 windows_exporter 监控 windows\n 部署方式：https://github.com/prometheus-community/windows_exporter 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/windows_by_exporter.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/windows_by_exporter.json  ",
    "description": "",
    "tags": null,
    "title": "监控Windows",
    "uri": "/usage/windows/"
  },
  {
    "content": "Telegraf内置支持snmp的采集，本节给一个入门例子，让大家快速上手，更多具体知识可以参考这里。在telegraf.conf中搜索inputs.snmp，即可找到对应的配置，例子如下：\n[[inputs.snmp]] agents = [\"udp://172.25.79.194:161\"] timeout = \"5s\" version = 3 agent_host_tag = \"ident\" retries = 1 sec_name = \"managev3user\" auth_protocol = \"SHA\" auth_password = \"example.Demo.c0m\" [[inputs.snmp.field]] oid = \"RFC1213-MIB::sysUpTime.0\" name = \"uptime\" [[inputs.snmp.field]] oid = \"RFC1213-MIB::sysName.0\" name = \"source\" is_tag = true [[inputs.snmp.table]] oid = \"IF-MIB::ifTable\" name = \"interface\" inherit_tags = [\"source\"] [[inputs.snmp.table.field]] oid = \"IF-MIB::ifDescr\" name = \"ifDescr\" is_tag = true 上面非常关键的部分是：agent_host_tag = \"ident\"，因为夜莺对ident这个标签会特殊对待处理，把携有这个标签的数据当做隶属某个监控对象的数据，机器和网络设备都是典型的期望作为监控对象来管理的，所以snmp的采集中，我们把网络设备的ip放到ident这个标签里带上去。\n另外这个采集规则是v3的校验方法，不同的公司可能配置的校验方式不同，请各位参照telegraf.conf中那些snmp相关的注释仔细核对，如果是v2会简单很多，把上例中的如下部分：\nversion = 3 sec_name = \"managev3user\" auth_protocol = \"SHA\" auth_password = \"example.Demo.c0m\" 换成：\nversion = 2 community = \"public\" 即可，当然了，community要改成你们自己的，这里写的public只是举个例子。\ninputs.snmp.field相关的那些配置，可以采集到各个网口的监控指标，更多的使用方式请参考官网\n 另外，snmp的采集，建议大家使用专门的Telegraf来做，因为和机器、中间件等的采集频率可能不同，比如边缘交换机，我们5min采集一次就够了，如果按照默认的配置可是10s采集一次，实在是太频繁了，可能会给一些老式交换机造成比较大的压力，采集频率在telegraf.conf的最上面[agent]部分，边缘交换机建议配置为：\n[agent] interval = \"300s\" flush_interval = \"300s\" 核心交换机可以配置的频繁一些，比如60s或者120s，请各位网络工程师朋友自行斟酌。\n",
    "description": "",
    "tags": null,
    "title": "监控网络设备",
    "uri": "/usage/snmp/"
  },
  {
    "content": "Google提出了应用监控的4个黄金指标，分别是：流量、延迟、错误、饱和度，其中前面3个指标都可以通过内嵌SDK的方式埋点采集。夜莺核心模块有两个，webapi主要是提供http接口给JavaScript调用，server主要是负责接收监控数据，处理告警规则，这两个模块都引入了Prometheus的Go的SDK，用此方式做App Performance监控，本节以夜莺的代码为例，讲解如何使用Prometheus的SDK。\nwebapi监控 webapi模块主要统计两个内容，一个是请求的数量统计，一个是请求的延迟统计，统计时，要用不同的Label做维度区分，后面就可以通过不同的维度做多种多样的统计分析，对于HTTP请求，规划4个核心Label，分别是：service、code、path、method。service标识服务名称，要求全局唯一，便于和其他服务名称区分开，比如webapi模块，就定义为n9e-webapi，code是http返回的状态码，200就表示成功数量，其他code就是失败的，后面我们可以据此统计成功率，method是HTTP方法，GET、POST、PUT、DELETE等，比如新增用户和获取用户列表可能都是/api/n9e/users，从路径上无法区分，只能再加上method才能区分开。\npath着重说一下，表示请求路径，比如上面提到的/api/n9e/users，但是，在restful实践中，url中经常会有参数，比如获取编号为1的用户的信息，接口是/api/n9e/user/1，获取编号为2的用户信息，接口是/api/n9e/user/2，如果这俩带有用户编号的url都作为Label，会造成时序库索引爆炸，而且从业务方使用角度来看，我们也不关注编号为1的用户获取请求还是编号为2的用户获取请求，而是关注整体的GET /api/n9e/user/:id这个接口的监控数据。所以我们在设置Label的时候，要把path设置为/api/n9e/user/:id，而不是那具体的带有用户编号的url路径。夜莺用的gin框架，gin框架有个FullPath方法就是获取这个信息的，比较方便。\n首先，我们在webapi下面创建一个stat包，放置相关统计变量：\npackage stat import ( \"time\" \"github.com/prometheus/client_golang/prometheus\" ) const Service = \"n9e-webapi\" var ( labels = []string{\"service\", \"code\", \"path\", \"method\"} uptime = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"uptime\", Help: \"HTTP service uptime.\", }, []string{\"service\"}, ) RequestCounter = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \"http_request_count_total\", Help: \"Total number of HTTP requests made.\", }, labels, ) RequestDuration = prometheus.NewHistogramVec( prometheus.HistogramOpts{ Buckets: []float64{.01, .1, 1, 10}, Name: \"http_request_duration_seconds\", Help: \"HTTP request latencies in seconds.\", }, labels, ) ) func Init() { // Register the summary and the histogram with Prometheus's default registry. \tprometheus.MustRegister( uptime, RequestCounter, RequestDuration, ) go recordUptime() } // recordUptime increases service uptime per second. func recordUptime() { for range time.Tick(time.Second) { uptime.WithLabelValues(Service).Inc() } } uptime变量是顺手为之，统计进程启动了多久时间，不用太关注，RequestCounter和RequestDuration，分别统计请求流量和请求延迟。Init方法是在webapi模块进程初始化的时候调用，所以进程一起，就会自动注册好。\n然后我们写一个middleware，在请求进来的时候拦截一下，省的每个请求都要去统计，middleware方法的代码如下：\nimport ( ... promstat \"github.com/didi/nightingale/v5/src/webapi/stat\" ) func stat() gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() c.Next() code := fmt.Sprintf(\"%d\", c.Writer.Status()) method := c.Request.Method labels := []string{promstat.Service, code, c.FullPath(), method} promstat.RequestCounter.WithLabelValues(labels...).Inc() promstat.RequestDuration.WithLabelValues(labels...).Observe(float64(time.Since(start).Seconds())) } } 有了这个middleware之后，new出gin的engine的时候，就立马Use一下，代码如下：\n... r := gin.New() r.Use(stat()) ... 最后，监控数据要通过/metrics接口暴露出去，我们要暴露这个请求端点，代码如下：\nimport ( ... \"github.com/prometheus/client_golang/prometheus/promhttp\" ) func configRoute(r *gin.Engine, version string) { ... r.GET(\"/metrics\", gin.WrapH(promhttp.Handler())) } 如上，每个webapi的接口的流量和成功率都可以监控到了。如果你也部署了夜莺，请求webapi的端口(默认是18000)的/metrics接口看看吧。\nInfo如果服务部署多个实例，甚至多个region，多个环境，上面的4个Label就不够用了，因为只有这4个Label不足以唯一标识一个具体的实例，此时需要env、region、instance这种Label，这些Label不需要在代码里埋点，在采集的时候一般可以附加额外的标签，通过附加标签的方式来处理即可\n server监控 server模块的监控，和webapi模块的监控差异较大，因为关注点不同，webapi关注的是HTTP接口的请求量和延迟，而server模块关注的是接收了多少监控指标，内部事件队列的长度，从数据库同步告警规则花费多久，同步了多少条数据等，所以，我们也需要在server的package下创建一个stat包，stat包下放置stat.go，内容如下：\npackage stat import ( \"github.com/prometheus/client_golang/prometheus\" ) const ( namespace = \"n9e\" subsystem = \"server\" ) var ( // 各个周期性任务的执行耗时 \tGaugeCronDuration = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"cron_duration\", Help: \"Cron method use duration, unit: ms.\", }, []string{\"cluster\", \"name\"}) // 从数据库同步数据的时候，同步的条数 \tGaugeSyncNumber = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"cron_sync_number\", Help: \"Cron sync number.\", }, []string{\"cluster\", \"name\"}) // 从各个接收接口接收到的监控数据总量 \tCounterSampleTotal = prometheus.NewCounterVec(prometheus.CounterOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"samples_received_total\", Help: \"Total number samples received.\", }, []string{\"cluster\", \"channel\"}) // 产生的告警总量 \tCounterAlertsTotal = prometheus.NewCounterVec(prometheus.CounterOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"alerts_total\", Help: \"Total number alert events.\", }, []string{\"cluster\"}) // 内存中的告警事件队列的长度 \tGaugeAlertQueueSize = prometheus.NewGaugeVec(prometheus.GaugeOpts{ Namespace: namespace, Subsystem: subsystem, Name: \"alert_queue_size\", Help: \"The size of alert queue.\", }, []string{\"cluster\"}) ) func Init() { // Register the summary and the histogram with Prometheus's default registry. \tprometheus.MustRegister( GaugeCronDuration, GaugeSyncNumber, CounterSampleTotal, CounterAlertsTotal, GaugeAlertQueueSize, ) } 定义一个监控指标，除了name之外，还可以设置namespace、subsystem，最终通过/metrics接口暴露的时候，可以发现：监控指标的最终名字，就是$namespace_$subsystem_$name，三者拼接在一起。webapi模块的监控代码中我们看到了counter类型和histogram类型的处理，这次我们拿GaugeAlertQueueSize举例，这是个GAUGE类型的统计数据，起一个goroutine周期性获取队列长度，然后Set到GaugeAlertQueueSize中：\npackage engine import ( \"context\" \"time\" \"github.com/didi/nightingale/v5/src/server/config\" promstat \"github.com/didi/nightingale/v5/src/server/stat\" ) func Start(ctx context.Context) error { ... go reportQueueSize() return nil } func reportQueueSize() { for { time.Sleep(time.Second) promstat.GaugeAlertQueueSize.WithLabelValues(config.C.ClusterName).Set(float64(EventQueue.Len())) } } 另外，Init方法要在server模块初始化的时候调用，server的router.go中要暴露/metrics端点路径，这些就不再详述了，大家可以扒拉一下夜莺的代码看一下。\n数据抓取 应用自身的监控数据已经通过/metrics接口暴露了，后续采集规则可以在prometheus.yml中配置，prometheus.yml中有个section叫：scrape_configs可以配置抓取目标，这是Prometheus范畴的知识了，大家可以参考Prometheus官网。\n参考资料  https://prometheus.io/docs/instrumenting/clientlibs/ https://github.com/prometheus/client_golang/tree/master/examples  ",
    "description": "",
    "tags": null,
    "title": "埋点监控之PromSDK",
    "uri": "/usage/promapm/"
  },
  {
    "content": "StatsD简介 在内嵌Prometheus SDK做APM一节中，我们介绍了业务进程内嵌Prometheus的SDK做埋点，这种方式，会把监控数据聚合计算的逻辑放在业务进程中，比如一些平均值、分位值的计算，可能会对业务进程造成影响，本节要介绍的StatsD的方式，理念是把指标聚合计算的逻辑挪到业务进程之外，业务进程调用埋点函数的时候，通过UDP推送给StatsD，即使StatsD挂了，也不会对业务进程造成影响。\nStatsD的简介，网上一搜一大把，请大家自行Google，这里就不重复描述了。核心要理解一下StatsD的设计理念、协议、支持的各个语言的SDK（在附录里有）即可，下面直接拿一个小例子讲解如何利用Telegraf支持StatsD协议的数据，数据只要进了Telegraf了，就意味着可以推到夜莺了，夜莺就相当于支持了StatsD的埋点监控采集能力。\nTelegraf启用StatsD 在Telegraf的配置文件中搜索inputs.statsd就能看到对应的section：\n[[inputs.statsd]] protocol = \"udp\" service_address = \":8125\" percentiles = [50.0, 90.0, 99.0, 99.9, 99.95, 100.0] metric_separator = \"_\" 启用如上配置，percentiles略微有点多，可以配置的少一点，比如percentiles = [50.0, 90.0, 99.0, 100.0]，这样整体计算存储压力也会小一些。重启Telegraf，Telegraf就会在8125端口监听udp协议，接收业务埋点数据的上报。即，Telegraf实现了StatsD的协议，可以作为StatsD的Server使用。\n在业务程序中埋点 附录里罗列了一些客户端SDK，这里笔者使用Go语言的一个SDK来测试，实现了一个很小的web程序，代码如下：\npackage main import ( \"fmt\" \"math/rand\" \"net/http\" \"time\" \"github.com/smira/go-statsd\" ) var client *statsd.Client func homeHandler(w http.ResponseWriter, r *http.Request) { start := time.Now() // random sleep \tnum := rand.Int31n(100) time.Sleep(time.Duration(num) * time.Millisecond) fmt.Fprintf(w, \"duration: %d\", num) client.Incr(\"requests.counter,page=home\", 1) client.PrecisionTiming(\"requests.latency,page=home\", time.Since(start)) } func main() { // init client \tclient = statsd.NewClient(\"localhost:8125\", statsd.TagStyle(statsd.TagFormatInfluxDB), statsd.MaxPacketSize(1400), statsd.MetricPrefix(\"http.\"), statsd.DefaultTags(statsd.StringTag(\"service\", \"n9e-webapi\"), statsd.StringTag(\"region\", \"bj\")), ) defer client.Close() http.HandleFunc(\"/\", homeHandler) http.ListenAndServe(\":8000\", nil) } 这个web服务只有一个根路径，逻辑也很简单，就是随机sleep几十个毫秒当做业务处理时间。整体逻辑是这样的：首先，我们要通过statsd.NewClient初始化一个statsd的客户端，参数中指定了StatsD的Server地址（在本例中就是Telegraf的8125），指定了所有监控指标的前缀是http.，还指定了两个全局Tag，一个是service=n9e-webapi，另一个是region=bj，通过TagStyle指定了要发送的是InfluxDB样式（因为数据是发给Telegraf的，Telegraf是InfluxDB生态的）的标签。然后，在请求的具体处理逻辑里上报了两个监控指标，一个是requests.counter，另一个是requests.latency，并且，为这俩指标指定了一个指标级别的标签page=home，整体看起来还是比较简单的。\n测试方法 上面的Go程序编译一下，启动，会作为一个web server监听在8000端口，然后周期性请求这个web server的地址做测试，这个web server接收到请求之后，就调用statsd的sdk，statsd的sdk的核心逻辑就是把数据发给Telegraf的8125，然后就是Telegraf处理聚合逻辑，聚合之后的数据每10s（默认flush频率）发给夜莺。\n在页面上，应该可以看到http_requests_latency和http_requests_counter打头的相关指标，比如http_requests_latency_mean这个指标，会看到这个指标有如下几个标签：\n ident: VM-0-4-centos 这个标签其实是Telegraf原始的host标签，夜莺的规范里叫ident，所以做了一下rename metric_type: timing 这个显然是把statsd的数据类型也做为标签了，其他数据类型还有gauge、counter、set等 page: home 这是我们代码里附到监控指标后面的标签，Telegraf自动帮解析出来了 service: n9e-webapi NewClient时候附加的全局默认标签 region: bj NewClient时候附加的全局默认标签  附录资料  Measure Anything, Measure Everything Statsd支持的的客户端SDK列表  ",
    "description": "",
    "tags": null,
    "title": "埋点监控之StatsdSDK",
    "uri": "/usage/statsd/"
  },
  {
    "content": "请参考如下视频教程：\n 01-使用Docker Compose一行命令安装夜莺v5 02-快速在生产环境部署启动单机版夜莺v5 03-讲解夜莺v5人员组织相关功能 04-讲解夜莺v5监控看图相关功能 05-讲解夜莺v5告警规则的使用 06-讲解夜莺v5告警屏蔽规则的使用 07-讲解夜莺告警订阅规则的使用 08-讲解夜莺v5活跃告警和历史告警 09-讲解夜莺v5告警自愈脚本的使用 10-讲解夜莺监控对象的管理功能 11-夜莺v5如何接入多个时序存储 12-讲解夜莺v5配置文件  使用 Telegraf 采集监控数据，可以参考如下调研笔记：\n Telegraf监控客户端调研笔记（1）-介绍、安装、初步测试 Telegraf监控客户端调研笔记（2）-CPU、MEM、DISK、IO相关指标采集 Telegraf监控客户端调研笔记（3）-kernel、system、processes相关指标采集 Telegraf监控客户端调研笔记（4）-exec、net、netstat相关指标采集 Telegraf监控客户端调研笔记（5）-本地端口监控\u0026远程TCP探测 Telegraf监控客户端调研笔记（6）-PING监控、进程监控  如果仍然有使用问题，可以联系我们，联系方式如下，公众号：\n",
    "description": "",
    "tags": null,
    "title": "功能介绍",
    "uri": "/usage/"
  },
  {
    "content": "之前在写调研笔记的时候，测试了PING监控和TCP探测监控，调研笔记在 这里 这个章节主要给大家讲解域名URL探测。直接上测试配置：\n[[inputs.http_response]] urls = [\"https://www.baidu.com\", \"http://ulricqin.io/ping\"] response_timeout = \"5s\" method = \"GET\" fielddrop = [\"result_type\"] tagexclude = [\"result\", \"status_code\"] https://www.baidu.com 显然是通的，http://ulricqin.io/ping 这个是个假的URL，不通，我们测试一下输出的内容：\n[root@10-255-0-34 telegraf-1.20.3]# ./usr/bin/telegraf --config etc/telegraf/telegraf.conf --input-filter http_response --test 2021-12-13T04:16:43Z I! Starting Telegraf 1.20.3 \u003e http_response,host=10-255-0-34,method=GET,server=https://www.baidu.com content_length=227i,http_response_code=200i,response_time=0.028757521,result_code=0i 1639369003000000000 \u003e http_response,host=10-255-0-34,method=GET,server=http://ulricqin.io/ping result_code=5i 1639369003000000000 这里有个字段是result_code，用这个字段配置告警即可，正常可以访问的URL，result_code是0，不正常就是非0，告警规则里可以配置如下promql：\nhttp_response_result_code != 0 或者直接在夜莺的告警规则页面导入这条告警规则JSON：\n[ { \"name\": \"有URL探测失败，请注意\", \"note\": \"\", \"severity\": 1, \"disabled\": 0, \"prom_for_duration\": 60, \"prom_ql\": \"http_response_result_code != 0\", \"prom_eval_interval\": 15, \"enable_stime\": \"00:00\", \"enable_etime\": \"23:59\", \"enable_days_of_week\": [ \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"0\" ], \"notify_recovered\": 1, \"notify_channels\": [ \"email\", \"dingtalk\", \"wecom\" ], \"notify_repeat_step\": 60, \"callbacks\": [], \"runbook_url\": \"\", \"append_tags\": [] } ] 如果想对域名返回的statuscode或者response body的内容做判断，Telegraf也是支持的，使用response_status_code和response_string_match这些字段配置，配置文件里有样例，大家可以自行参考下。\n",
    "description": "",
    "tags": null,
    "title": "监控URL",
    "uri": "/usage/http_response/"
  },
  {
    "content": "前言 说到日志监控，大家第一反应的可能是ELK的方案，或者Loki的方案，这两个方案都是把日志采集了发到中心，在中心存储、查看、分析，不过这个方案相对比较重量级一些，如果我们的需求只是从日志中提取一些metrics数据，比如统计一些日志中出现的Error次数之类的，则有一个更简单的方案。\n这个方案在夜莺v4版本中是有的，不过后来推荐大家客户端使用Telegraf，Telegraf没有这个能力，所以v5版本的夜莺没法监控日志，怎么办呢？这里给大家介绍一个Google出品的小工具，mtail，mtail和夜莺v4的方案类似，就是流式读取日志，通过正则表达式匹配的方式从日志中提取metrics指标，这种方式可以利用目标机器的算力，不过如果量太大，可能会影响目标机器上的业务程序，另外一个好处是无侵入性，不需要业务埋点，如果业务程序是第三方供应商提供的，我们改不了其代码，mtail此时就非常合适了。当然了，如果业务程序是我们公司的人自己写的，那还是建议用埋点的方式采集指标，mtail只是作为一个补充吧。\nmtail简介 mtail的使用方案，参考如下两个文档（下载的话参考Releases页面）：\n Deploying Programming Guide  我们拿mtail的启动命令来举例其用法：\nmtail --progs /etc/mtail --logs /var/log/syslog --logs /var/log/ntp/peerstats 通过 --progs 参数指定一个目录，这个目录里放置一堆的*.mtail文件，每个mtail文件就是描述的正则提取规则，通过 --logs 参数来指定要监控的日志目录，可以写通配符，--logs 可以写多次，上例中只是指定了 --progs 和 --logs ，没有其他参数，mtail启动之后会自动监听一个端口3903，在3903的/metrics接口暴露符合Prometheus协议的监控数据，Prometheus（或者Telegraf）就可以从 /metrics 接口提取监控数据。\n这样看起来，原理就很清晰了，mtail启动之后，根据 --logs 找到相关日志文件文件，seek到文件末尾，开始流式读取，每读到一行，就根据 --progs 指定的那些规则文件做匹配，看是否符合某些正则，从中提取时序数据，然后通过3903的/metrics暴露采集到的监控指标。当然，除了Prometheus这种/metrics方式暴露，mtail还支持把监控数据直接推给graphite或者statsd，具体可以参考：这里\nmtail样例 这里我用mtail监控一下n9e-server的日志，从中提取一下各个告警规则触发的notify的数量，这个日志举例：\n2021-12-27 10:00:30.537582 INFO engine/logger.go:19 event(cbb8d4be5efd07983c296aaa4dec5737 triggered) notify: rule_id=9 [__name__=net_response_result_code author=qin ident=10-255-0-34 port=4567 protocol=tcp server=localhost]2@1640570430 很明显，日志中有这么个关键字：notify: rule_id=9，可以用正则来匹配，统计出现的行数，ruleid也可以从中提取到，这样，我们可以把ruleid作为标签上报，于是乎，我们就可以写出这样的mtail规则了：\n[root@10-255-0-34 nightingale]# cat /etc/mtail/n9e-server.mtail counter mtail_alert_rule_notify_total by ruleid /notify: rule_id=(?P\u003cruleid\u003e\\d+)/ { mtail_alert_rule_notify_total[$ruleid]++ } 然后启动也比较简单，我这里就用nohup简单来做：\nnohup mtail -logtostderr --progs /etc/mtail --logs server.log \u0026\u003e stdout.log \u0026 mtail没有指定绝对路径，是因为我把mtail的二进制直接放在了 /usr/bin 下面了，mtail默认会监听在3903，所以我们可以用如下命令验证：\ncurl -s localhost:3903/metrics 可以看到输出如下内容：\n# HELP mtail_alert_rule_notify_total defined at n9e-server.mtail:1:9-37 # TYPE mtail_alert_rule_notify_total counter mtail_alert_rule_notify_total{prog=\"n9e-server.mtail\",ruleid=\"9\"} 6 上面的输出只是挑选了部分内容，没有全部放出来哈，这就表示正常采集到了，如果n9e的server.log中当前没有打印notify相关的日志，那请求/metrics接口是没法得到上面的输出的，可以手工配置一条必然会触发的规则，待日志里有相关输出的时候再次请求 /metrics 接口，应该就有了\n最后，我们使用Telegraf来采集一下 localhost:3903/metrics 这个地址的输出，在telegraf.conf中添加如下配置：\n[[inputs.prometheus]] urls = [\"http://localhost:3903/metrics\"] 完事重启Telegraf或者给Telegraf进程发一个SIGHUP信号：\nkill -HUP `pidof telegraf` 等一会，就可以在页面上查到相关指标了，我们拿着mtail_alert_rule_notify_total这个指标去即时查询里查，会发现查不到数据，而是出现了一个mtail_alert_rule_notify_total_counter这样的指标，看起来像是Telegraf对于Prometheus协议的监控数据，自动加了后缀，无所谓了，大家注意一下就好。如果在prometheus.yaml中配置scrape_config来抓取mtail，应该不会自动加上_counter的后缀。\n另外，mtail的配置文件如果发生变化，是需要重启mtail才能生效的，或者也是类似Telegraf那样发一个SIGHUP信号给mtail，mtail收到信号就会重新加载配置。\nmtail更多样例 mtail的github repo中有一个examples，里边有挺多例子，大家可以参考。我在这里再给大家举1个简单例子，比如我们要统计/var/log/messages文件中的 Out of memory 关键字，mtail规则应该怎么写呢？其实比上面举例的mtail_alert_rule_notify_total还要更简单：\ncounter mtail_oom_total /Out of memory/ { mtail_oom_total++ } 关于时间戳 最后说一下时间戳的问题，日志中每一行一般都是有个时间戳的，夜莺v4版本在页面上配置采集规则的时候，就是要选择时间戳的，但是mtail，上面的例子中没有处理时间戳，为啥？其实mtail也可以支持从日志中提取时间戳，如果没有配置的话，就用系统当前时间，个人认为，用系统当前时间就可以了，从日志中提取时间稍微还有点麻烦，当然，系统当前时间和日志中的时间可能稍微有差别，但是不会差很多的，可以接受，examples中的mtail样例，也基本都没有给出时间戳的提取，估计这就是最佳实践。\n",
    "description": "",
    "tags": null,
    "title": "监控日志",
    "uri": "/usage/mtail/"
  },
  {
    "content": " 监控方式：https://github.com/prometheus/mysqld_exporter 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/mysql_by_exporter.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/mysql_by_exporter.json  ",
    "description": "",
    "tags": null,
    "title": "监控MySQL",
    "uri": "/usage/mysql/"
  },
  {
    "content": " 监控方式：https://github.com/oliver006/redis_exporter 告警规则：https://github.com/didi/nightingale/blob/main/etc/alerts/redis_by_exporter.json 监控大盘：https://github.com/didi/nightingale/blob/main/etc/dashboards/redis_by_exporter.json  ",
    "description": "",
    "tags": null,
    "title": "监控Redis",
    "uri": "/usage/redis/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "API",
    "uri": "/api/"
  },
  {
    "content": " Acknowledgement: Grafana Agent is a lightweight telemetry collector based on Prometheus that only performs its scraping and remote_write functions. Agent can also collect metrics, logs, and traces for storage in Grafana Cloud and Grafana Enterprise, as well as OSS deployments of Loki (logs), and Tempo (traces), Prometheus (metrics), and Cortex (metrics). Grafana Agent also contains several integrations (embedded metrics exporters) like node-exporter, a MySQL exporter, and many more.\n 如果您使用和管理着Kubernetes集群以及您的应用运行在Kubernetes之上，请参考 在K8s中使用grafana-agent。\n在Windows环境安装和运行grafana-agent  从Grafana github releases下载Windows安装文件； 运行安装文件后，会对grafana-agent进行配置，并注册为Windows服务； 更详细的配置文档，可以参考Windows Guide；  在Docker中运行grafana-agent 如果您的宿主机上运行有docker服务，那么使用docker运行grafana-agent 是最快捷的方式。在命令行终端运行以下命令，即可在容器中启动grafana-agent：\n1. 生成 grafana-agent 的配置文件 cat \u003c\u003cEOF \u003e /tmp/grafana-agent-config.yaml server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 15s scrape_timeout: 10s configs: - name: flashtest host_filter: false scrape_configs: - job_name: local_scrape static_configs: - targets: ['127.0.0.1:12345'] labels: cluster: 'mymac' remote_write: - url: https://n9e-server:19000/prometheus/v1/write basic_auth: username: \u003cstring\u003e password: \u003cstring\u003e EOF 2. 启动 grafana-agent 容器 docker run \\  -v /tmp/agent:/etc/agent/data \\  -v /tmp/grafana-agent-config.yaml:/etc/agent/agent.yaml \\  -p 12345:12345 \\  -d \\  grafana/agent:v0.23.0 \\  --config.file=/etc/agent/agent.yaml \\  --prometheus.wal-directory=/etc/agent/data 或者您也可以从 Dockerfile 在本地 build 镜像之后再运行：\ncurl -sO https://raw.githubusercontent.com/grafana/agent/main/cmd/agent/Dockerfile docker build -t grafana/agent:v0.23.0 -f ./Dockerfile 上述步骤中，几个需要注意的点：\n remote_write 和 basic_auth ，请根据自己的实际情况填写； -p 把容器中的端口12345映射到主机，-d 把容器进程放到后台运行； -v /tmp/agent:/etc/agent/data 是把宿主机的目录 /tmp/agent 映射到容器中 /etc/agent/data，用于 grafana-agent 持久化保存其 WAL(Write Ahead Log) ； -v /tmp/grafana-agent-config.yaml:/etc/agent/agent.yaml 是把 grafana-agent 的配置文件，放置到容器指定的位置，即 /etc/agent/agent.yaml  3. 验证 grafana-agent 是否正常工作 您可以通过直接 curl http://localhost:12345/metrics 来验证数据的产生是否符合预期，正常情况下会显示如下：\nagent_build_info{branch=\"HEAD\",goversion=\"go1.17.6\",revision=\"36b8ca75\",version=\"v0.23.0\"} 1 agent_inflight_requests{method=\"GET\",route=\"metrics\"} 1 agent_metrics_active_configs 1 agent_metrics_active_instances 1 agent_tcp_connections{protocol=\"grpc\"} 0 agent_tcp_connections{protocol=\"http\"} 2 go_gc_duration_seconds_sum 0.0040902 go_gc_duration_seconds_count 6 go_goroutines 50 log_messages_total{level=\"debug\"} 44 log_messages_total{level=\"error\"} 0 log_messages_total{level=\"info\"} 13 log_messages_total{level=\"warn\"} 0 loki_logql_querystats_duplicates_total 0 loki_logql_querystats_ingester_sent_lines_total 0 net_conntrack_dialer_conn_attempted_total{dialer_name=\"local_scrape\"} 1 net_conntrack_dialer_conn_attempted_total{dialer_name=\"remote_storage_write_client\"} 1 net_conntrack_dialer_conn_closed_total{dialer_name=\"local_scrape\"} 0 net_conntrack_dialer_conn_closed_total{dialer_name=\"remote_storage_write_client\"} 0 net_conntrack_dialer_conn_established_total{dialer_name=\"local_scrape\"} 1 net_conntrack_dialer_conn_established_total{dialer_name=\"remote_storage_write_client\"} 1 process_cpu_seconds_total 11.53 process_max_fds 1.048576e+06 process_open_fds 17 process_resident_memory_bytes 9.4773248e+07 process_start_time_seconds 1.64499076013e+09 process_virtual_memory_bytes 1.356931072e+09 process_virtual_memory_max_bytes 1.8446744073709552e+19 prometheus_interner_num_strings 275 prometheus_interner_string_interner_zero_reference_releases_total 0 prometheus_sd_consulagent_rpc_duration_seconds_sum{call=\"services\",endpoint=\"agent\"} 0 prometheus_sd_consulagent_rpc_duration_seconds_count{call=\"services\",endpoint=\"agent\"} 0 prometheus_sd_consulagent_rpc_failures_total 0 prometheus_sd_dns_lookup_failures_total 0 prometheus_sd_dns_lookups_total 0 prometheus_sd_file_read_errors_total 0 prometheus_sd_file_scan_duration_seconds{quantile=\"0.5\"} NaN ... 您也可以通过访问 grafana-agent 所暴露的 API，获取到 targets 列表来确认是否符合预期：\ncurl http://localhost:12345/agent/api/v1/targets |jq  { \"status\": \"success\", \"data\": [ { \"instance\": \"7f383657f506f53a739e2df61be58891\", \"target_group\": \"local_scrape\", \"endpoint\": \"http://127.0.0.1:12345/metrics\", \"state\": \"up\", \"labels\": { \"cluster\": \"mymac\", \"instance\": \"127.0.0.1:12345\", \"job\": \"local_scrape\" }, \"discovered_labels\": { \"__address__\": \"127.0.0.1:12345\", \"__metrics_path__\": \"/metrics\", \"__scheme__\": \"http\", \"__scrape_interval__\": \"15s\", \"__scrape_timeout__\": \"10s\", \"cluster\": \"mymac\", \"job\": \"local_scrape\" }, \"last_scrape\": \"2022-02-16T07:18:55.6221085Z\", \"scrape_duration_ms\": 6, \"scrape_error\": \"\" } ] } 在本机安装运行grafana-agent 如果您的主机上没有docker或者您希望直接把grafana-agent运行在宿主机上，可以依照以下步骤：\n1. 下载预先编译好的二进制包 下载地址为: https://github.com/grafana/agent/releases/download/${version}/agent-${platform}-${arch}.zip\n 其中，version当前为v0.23.0 其中，可下载的platform和arch列表如下：  linux/amd64 linux/arm64 linux/armv7 linux/armv6 darwin/amd64 darwin/arm64 windows/amd64 linux/mipsle freebsd/amd64    比如，我们现在的操作系统为Linux，架构为Amd64， 那么grafana-agent的二进制包下载命令如下：\n# download the binary curl -SOL \"https://github.com/grafana/agent/releases/download/v0.23.0/agent-linux-amd64.zip\" # extract the binary gunzip ./agent-linux-amd64.zip # make sure it is executable chmod a+x \"agent-linux-amd64\" 2. 生成 grafana-agent 的配置文件 cat \u003c\u003cEOF \u003e ./agent-cfg.yaml server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 15s scrape_timeout: 10s remote_write: - url: https://n9e-server:19000/prometheus/v1/write basic_auth: username: \u003cstring\u003e password: \u003cstring\u003e integrations: agent: enabled: true node_exporter: enabled: true include_exporter_metrics: true EOF 3. 启动 grafana-agent nohup ./agent-linux-amd64 \\  -config.file ./agent-cfg.yaml \\  -metrics.wal-directory ./data \\  \u0026\u003e grafana-agent.log \u0026 4. 验证 grafana-agent 是否正常工作  您可以通过直接 curl http://localhost:12345/metrics 来验证数据的产生是否符合预期； 您也可以通过访问 grafana-agent 所暴露的 API ，获取到 targets 列表来确认是否符合预期，操作命令为 curl http://localhost:12345/agent/api/v1/targets；  至此，我们已经成功的将 grafana-agent 运行起来，并且开始收集 grafana-agent 自身的 metrics 指标。下一步，我们讲述如何通过 grafana-agent 的内嵌的各种 exporter 来采集主机、进程、MySQL等监控指标。\n",
    "description": "",
    "tags": null,
    "title": "使用Grafana-agent采集监控数据",
    "uri": "/grafana-agent/"
  },
  {
    "content": "grafana-agent 内置了 cadvisor, 可以支持采集容器的各项指标。不过 cadvisor 针对宿主机需要设置相关的权限，具体可以参考 cAdvisor docs.\n配置并启用cadvisor_exporter 生成grafana-agent-cfg.yaml 配置文件，其中开启cadvisor integration，配置文件具体举例如下：\ncat \u003c\u003cEOF \u003e /tmp/grafana-agent-cfg.yaml server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 15s remote_write: - url: 'https://n9e-server:19000/prometheus/v1/write' basic_auth: username: ${FC_USERNAME} password: ${FC_PASSWORD} integrations: cadvisor: enabled: true EOF 在docker中启动 grafana-agent，同时映射相关目录 docker run \\  -v /tmp/agent:/etc/agent/data \\  -v /tmp/grafana-agent-cfg.yaml:/etc/agent/agent.yaml \\  -p 12345:12345 \\  -d \\  --privileged \\  grafana/agent:v0.23.0 \\  --config.file=/etc/agent/agent.yaml \\  --metrics.wal-directory=/etc/agent/data 执行 curl http://localhost:12345/agent/api/v1/targets |jq,输出结果中预期应该包含 integrations/cadvisor 字段，如下：\n{ \"status\": \"success\", \"data\": [ { \"instance\": \"7f383657f506f53a739e2df61be58891\", \"target_group\": \"integrations/cadvisor\", \"endpoint\": \"http://127.0.0.1:12345/integrations/cadvisor/metrics\", \"state\": \"up\", \"labels\": { \"agent_hostname\": \"509c1284c59c\", \"instance\": \"509c1284c59c:12345\", \"job\": \"integrations/cadvisor\" }, \"discovered_labels\": { \"__address__\": \"127.0.0.1:12345\", \"__metrics_path__\": \"/integrations/cadvisor/metrics\", \"__scheme__\": \"http\", \"__scrape_interval__\": \"15s\", \"__scrape_timeout__\": \"10s\", \"agent_hostname\": \"509c1284c59c\", \"job\": \"integrations/cadvisor\" }, \"last_scrape\": \"2022-02-17T14:54:50.652267586Z\", \"scrape_duration_ms\": 30, \"scrape_error\": \"\" } ] } 执行 curl http://localhost:12345/integrations/cadvisor/metrics,预期输出结果下：\ncadvisor_version_info{cadvisorRevision=\"\",cadvisorVersion=\"\",dockerVersion=\"\",kernelVersion=\"5.10.76-linuxkit\",osVersion=\"Debian GNU/Linux 10 (buster)\"} 1 container_blkio_device_usage_total{device=\"/dev/vda\",id=\"/\",major=\"254\",minor=\"0\",operation=\"Read\"} 4.6509056e+07 1645109878135 container_blkio_device_usage_total{device=\"/dev/vda\",id=\"/\",major=\"254\",minor=\"0\",operation=\"Write\"} 3.13243648e+09 1645109878135 container_cpu_load_average_10s{id=\"/\"} 0 1645109878135 container_cpu_system_seconds_total{id=\"/\"} 57.789 1645109878135 container_cpu_usage_seconds_total{cpu=\"total\",id=\"/\"} 91.57 1645109878135 container_cpu_user_seconds_total{id=\"/\"} 33.781 1645109878135 container_fs_inodes_free{device=\"/dev\",id=\"/\"} 254415 1645109878135 container_fs_inodes_free{device=\"/dev/shm\",id=\"/\"} 254551 1645109878135 container_fs_inodes_free{device=\"/dev/vda1\",id=\"/\"} 3.890602e+06 1645109878135 container_fs_inodes_free{device=\"/rootfs/dev/shm\",id=\"/\"} 254551 1645109878135 ... 采集的指标列表 # CPU # 容器运行经过的cfs周期总数 container_cpu_cfs_periods_total: Number of elapsed enforcement period intervals # 容器运行时发生节流的cfs周期总数 container_cpu_cfs_throttled_periods_total: Number of throttled period intervals # 容器发生cpu节流的总时间 container_cpu_cfs_throttled_seconds_total: Total time duration the container has been throttled container_cpu_load_average_10s: Value of container cpu load average over the last 10 seconds container_cpu_system_seconds_total: Cumulative system cpu time consumed container_cpu_usage_seconds_total: Cumulative cpu time consumed container_cpu_user_seconds_total: Cumulative user cpu time consumed # 容器描述中的CPU周期配置 container_spec_cpu_period: CPU period of the container # 容器描述中的CPU quota配置 container_spec_cpu_quota: CPU quota of the container # 容器描述中的CPU权重配置 container_spec_cpu_shares: CPU share of the container # MEM container_memory_cache: Total page cache memory container_memory_failcnt: Number of memory usage hits limits container_memory_failures_total: Cumulative count of memory allocation failures container_memory_mapped_file: Size of memory mapped files container_memory_max_usage_bytes: Maximum memory usage recorded container_memory_rss: Size of RSS container_memory_swap: Container swap usage container_memory_usage_bytes: Current memory usage, including all memory regardless of when it was accessed container_oom_events_total: Count of out of memory events observed for the container container_spec_memory_limit_bytes: Memory limit for the container container_spec_memory_reservation_limit_bytes: Memory reservation limit for the container container_spec_memory_swap_limit_bytes: Memory swap limit for the container # Disk # 设备IO使用总量 container_blkio_device_usage_total: Blkio device bytes usage container_fs_inodes_free: Number of available Inodes container_fs_inodes_total: Total number of Inodes container_fs_io_current: Number of I/Os currently in progress # 容器IO总耗时 container_fs_io_time_seconds_total: Cumulative count of seconds spent doing I/Os container_fs_io_time_weighted_seconds_total: Cumulative weighted I/O time container_fs_limit_bytes: Number of bytes that can be consumed by the container on this filesystem container_fs_reads_bytes_total: Cumulative count of bytes read container_fs_read_seconds_total: Cumulative count of seconds spent reading container_fs_reads_merged_total: Cumulative count of reads merged container_fs_reads_total: Cumulative count of reads completed container_fs_sector_reads_total: Cumulative count of sector reads completed container_fs_sector_writes_total: Cumulative count of sector writes completed container_fs_usage_bytes: Number of bytes that are consumed by the container on this filesystem container_fs_writes_bytes_total: Cumulative count of bytes written container_fs_write_seconds_total: Cumulative count of seconds spent writing container_fs_writes_merged_total: Cumulative count of writes merged container_fs_writes_total: Cumulative count of writes completed # Network container_network_receive_bytes_total: Cumulative count of bytes received container_network_receive_errors_total: Cumulative count of errors encountered while receiving container_network_receive_packets_dropped_total: Cumulative count of packets dropped while receiving container_network_receive_packets_total: Cumulative count of packets received container_network_transmit_bytes_total: Cumulative count of bytes transmitted container_network_transmit_errors_total: Cumulative count of errors encountered while transmitting container_network_transmit_packets_dropped_total: Cumulative count of packets dropped while transmitting container_network_transmit_packets_total: Cumulative count of packets transmitted # System container_tasks_state: Number of tasks in given state (sleeping, running, stopped, uninterruptible, or ioawaiting) # Others container_last_seen: Last time a container was seen by the exporter container_start_time_seconds: Start time of the container since unix epoch 完整地配置项说明 # Enables the cadvisor integration, allowing the Agent to automatically # collect metrics for the specified github objects. [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. [instance: \u003cstring\u003e | default = \u003cintegrations_config.instance\u003e] # Automatically collect metrics from this integration. If disabled, # the cadvisor integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/cadvisor/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # # cAdvisor-specific configuration options # # Convert container labels and environment variables into labels on prometheus metrics for each container. If false, then only metrics exported are container name, first alias, and image name. [store_container_labels: \u003cboolean\u003e | default = true] # List of container labels to be converted to labels on prometheus metrics for each container. store_container_labels must be set to false for this to take effect. allowlisted_container_labels: [ - \u003cstring\u003e ] # List of environment variable keys matched with specified prefix that needs to be collected for containers, only support containerd and docker runtime for now. env_metadata_allowlist: [ - \u003cstring\u003e ] # List of cgroup path prefix that needs to be collected even when docker_only is specified. raw_cgroup_prefix_allowlist: [ - \u003cstring\u003e ] # Path to a JSON file containing configuration of perf events to measure. Empty value disabled perf events measuring. [perf_events_config: \u003cboolean\u003e] # resctrl mon groups updating interval. Zero value disables updating mon groups. [resctrl_interval: \u003cint\u003e | default = 0] # List of `metrics` to be disabled. If set, overrides the default disabled metrics. disabled_metrics: [ - \u003cstring\u003e ] # List of `metrics` to be enabled. If set, overrides disabled_metrics enabled_metrics: [ - \u003cstring\u003e ] # Length of time to keep data stored in memory [storage_duration: \u003cduration\u003e | default = \"2m\"] # Containerd endpoint [containerd: \u003cstring\u003e | default = \"/run/containerd/containerd.sock\"] # Containerd namespace [containerd_namespace: \u003cstring\u003e | default = \"k8s.io\"] # Docker endpoint [docker: \u003cstring\u003e | default = \"unix:///var/run/docker.sock\"] # Use TLS to connect to docker [docker_tls: \u003cboolean\u003e | default = false] # Path to client certificate for TLS connection to docker [docker_tls_cert: \u003cstring\u003e | default = \"cert.pem\"] # Path to private key for TLS connection to docker [docker_tls_key: \u003cstring\u003e | default = \"key.pem\"] # Path to a trusted CA for TLS connection to docker [docker_tls_ca: \u003cstring\u003e | default = \"ca.pem\"] # Only report docker containers in addition to root stats [docker_only: \u003cboolean\u003e | default = false] ",
    "description": "",
    "tags": null,
    "title": "cAdvisor Exporter",
    "uri": "/grafana-agent/integrations/cadvisor-config/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": "The consul_exporter_config block configures the consul_exporter integration, which is an embedded version of consul_exporter. This allows for the collection of consul metrics and exposing them as Prometheus metrics.\nFull reference of options:\n# Enables the consul_exporter integration, allowing the Agent to automatically # collect system metrics from the configured consul server address [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the hostname portion # of the server URL. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the consul_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/consul_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # # Exporter-specific configuration options # # Prefix from which to expose key/value pairs. [kv_prefix: \u003cstring\u003e | default = \"\"] # Regex that determines which keys to expose. [kv_filter: \u003cstring\u003e | default = \".*\"] # Generate a health summary for each service instance. Needs n+1 queries to # collect all information. [generate_health_summary: \u003cbool\u003e | default = true] # HTTP API address of a Consul server or agent. Prefix with https:// to # connect using HTTPS. [server: \u003cstring\u003e | default = \"http://localhost:8500\"] # Disable TLS host verification. [insecure_skip_verify: \u003cbool\u003e | default = false] # File path to a PEM-encoded certificate authority used to validate the # authenticity of a server certificate. [ca_file: \u003cstring\u003e | default = \"\"] # File path to a PEM-encoded certificate used with the private key to verify # the exporter's authenticity. [cert_file: \u003cstring\u003e | default = \"\"] # File path to a PEM-encoded private key used with the certificate to verify # the exporter's authenticity. [key_file: \u003cstring\u003e | default = \"\"] # When provided, this overrides the hostname for the TLS certificate. It can # be used to ensure that the certificate name matches the hostname we declare. [server_name: \u003cstring\u003e | default = \"\"] # Timeout on HTTP requests to the Consul API. [timeout: \u003cduration\u003e | default = \"500ms\"] # Limit the maximum number of concurrent requests to consul. 0 means no limit. [concurrent_request_limit: \u003cint\u003e | default = 0] # Allows any Consul server (non-leader) to service a read. [allow_stale: \u003cbool\u003e | default = true] # Forces the read to be fully consistent. [require_consistent: \u003cbool\u003e | default = false] 采集的指标列表 consul_memberlist_tcp : irate(consul_memberlist_tcp{host=\"$consul\"}[1m]) consul_memberlist_udp : irate(consul_memberlist_udp{host=\"$consul\"}[1m]) consul_raft_apply[30s]) : delta(consul_raft_apply[30s]) consul_raft_commitTime : consul_raft_commitTime consul_raft_leader_dispatchLog : consul_raft_leader_dispatchLog consul_raft_leader_lastcontact : consul_raft_leader_lastcontact consul_raft_leader_lastcontact_count : consul_raft_leader_lastcontact_count consul_raft_replication_appendEntries_rpc : consul_raft_replication_appendEntries_rpc consul_raft_replication_heartbeat : consul_raft_replication_heartbeat consul_rpc_query : delta(consul_rpc_query{host=\"$consul\"}[30s]) consul_serf_coordinate_adjustment_ms : consul_serf_coordinate_adjustment_ms{host=\"$consul\"} labels) : COUNT (changes(consul_memberlist_gossep_sum[1m]) \u003e 0) BY (labels) node_cpu : sum(irate(node_cpu{mode=\"idle\", host=\"$consul\"}[1m])) * 100 / count_scalar(node_cpu{mode=\"user\", host=\"$consul\"}) node_load1 : node_load1{host=\"$consul\"} node_load15 : node_load15{host=\"$consul\"} node_load5 : node_load5{host=\"$consul\"} ",
    "description": "",
    "tags": null,
    "title": "Consul Exporter",
    "uri": "/grafana-agent/integrations/consul-exporter-config/"
  },
  {
    "content": "The dnsmasq_exporter_config block configures the dnsmasq_exporter integration, which is an embedded version of dnsmasq_exporter. This allows for the collection of metrics from dnsmasq servers.\nNote that currently, an Agent can only collect metrics from a single dnsmasq server. If you want to collect metrics from multiple servers, you can run multiple Agents and add labels using relabel_configs to differentiate between the servers:\ndnsmasq_exporter: enabled: true dnsmasq_address: dnsmasq-a:53 relabel_configs: - source_labels: [__address__] target_label: instance replacement: dnsmasq-a Full reference of options:\n# Enables the dnsmasq_exporter integration, allowing the Agent to automatically # collect system metrics from the configured dnsmasq server address [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the dnsmasq_address # value. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the dnsmasq_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/dnsmasq_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # # Exporter-specific configuration options # # Address of the dnsmasq server in host:port form. [dnsmasq_address: \u003cstring\u003e | default = \"localhost:53\"] # Path to the dnsmasq leases file. If this file doesn't exist, scraping # dnsmasq # will fail with an warning log message. [leases_path: \u003cstring\u003e | default = \"/var/lib/misc/dnsmasq.leases\"] ",
    "description": "",
    "tags": null,
    "title": "dnsmasq Exporter",
    "uri": "/grafana-agent/integrations/dnsmasq-exporter-config/"
  },
  {
    "content": "grafana-agent内置了elasticsearch_exporter，可以采集Elasticsearch的运行指标。\n目前grafana-agent不支持配置多个elasticsearch的地址，只能配置一个ElasticSearch地址对其进行metrics的采集。\n我们强烈推荐您使用独立的账号运行grafana-agent，并做好访问elasticsearch实例的最小化授权，避免过度授权带来的安全隐患，更多可以参考official documentation。\n配置并启用elasticsearch_exporter elasticsearch_exporter: enabled: true address: \"http://localhost:9200\" 采集的关键指标列表 # Estimated size in bytes of breaker # 断路器预估内存大小 # Gauge elasticsearch_breakers_estimated_size_bytes # Limit size in bytes for breaker # 断路器设置内存限制 # Gauge elasticsearch_breakers_limit_size_bytes # tripped for breaker # 断路器累计阻断此时 # Counter  elasticsearch_breakers_tripped # The number of primary shards in your cluster. This is an aggregate total across all indices # 集群主分片数量 # Gauge elasticsearch_cluster_health_active_primary_shards # Aggregate total of all shards across all indices, which includes replica shards. # 集群分片总数 # Gauge elasticsearch_cluster_health_active_shards # Shards delayed to reduce reallocation overhead # 暂缓重分配的分片数 # Gauge elasticsearch_cluster_health_delayed_unassigned_shards # Count of shards that are being freshly created # 创建中的分片数 # Gauge elasticsearch_cluster_health_initializing_shards # Number of data nodes in the cluster # 数据节点数 # Gauge elasticsearch_cluster_health_number_of_data_nodes # Number of nodes in the cluster # 节点总数 # Gauge elasticsearch_cluster_health_number_of_nodes # Cluster level changes which have not yet been executed # 等待执行的集群变更总数 # Gauge elasticsearch_cluster_health_number_of_pending_tasks # The number of shards that are currently moving from one node to another node # 迁移中的分片数 # Gauge elasticsearch_cluster_health_relocating_shards # Whether all primary and replica shards are allocated # 集群健康度 # Gauge elasticsearch_cluster_health_status # The number of shards that exist in the cluster state, but cannot be found in the cluster itself # 集群未分配的分片数 # Gauge elasticsearch_cluster_health_unassigned_shards # Available space on block device in bytes # 可用磁盘容量（byte） # Gauge elasticsearch_filesystem_data_available_bytes # Size of block device in bytes # 磁盘容量（byte） # Gauge elasticsearch_filesystem_data_size_bytes # Count of documents on this node # 节点文档总数 # Gauge elasticsearch_indices_docs # Count of deleted documents on this node # 节点删除文档数 # Gauge elasticsearch_indices_docs_deleted # Count of documents with only primary shards on all nodes # 所有节点主分片文档总数 # Gauge elasticsearch_indices_docs_primary # Evictions from field data # field data cache 内存剔除次数 # Counter elasticsearch_indices_fielddata_evictions # Field data cache memory usage in bytes # field data cache 内存占用（byte） # Gauge elasticsearch_indices_fielddata_memory_size_bytes # Evictions from filter cache # filter cache 内存剔除次数 # Counter elasticsearch_indices_filter_cache_evictions # Filter cache memory usage in bytes # filter cache 内存占用（byte） # Gauge elasticsearch_indices_flush_time_seconds # Total flushes # flush操作次数累计 # Counter elasticsearch_indices_flush_total # Total time get exists in seconds # get成功操作次数累计 # Counter elasticsearch_indices_get_exists_time_seconds # Total get exists operations # get操作次数累计 # Counter elasticsearch_indices_get_exists_total # Total time of get missing in seconds # get失败操作耗时累计（秒） # Counter elasticsearch_indices_get_missing_time_seconds # Total get missing # get失败操作次数累计 # Counter elasticsearch_indices_get_missing_total # Total get time in seconds # get操作耗时累计（秒） # Counter elasticsearch_indices_get_time_seconds # Total get # get操作次数累计 # Counter elasticsearch_indices_get_tota # Total time indexing delete in seconds # 索引删除累计耗时（秒） # Counter elasticsearch_indices_indexing_delete_time_seconds_total # Total indexing deletes # 索引删除操作次数累计 # Counter elasticsearch_indices_indexing_delete_total # Cumulative index time in seconds # index操作累计耗时（秒） # Counter elasticsearch_indices_indexing_index_time_seconds_total # Total index calls # index操作数量累计 # Counter elasticsearch_indices_indexing_index_total # Cumulative docs merged # merge文档数量累计 # Counter elasticsearch_indices_merges_docs_total # Total merges # merge操作数量累计 # Counter elasticsearch_indices_merges_total # Total merge size in bytes # merge操作数据大小累计（byte） # Counter elasticsearch_indices_merges_total_size_bytes_total # Total time spent merging in seconds # merge操作累计耗时（秒） # Counter elasticsearch_indices_merges_total_time_seconds_total # Evictions from query cache # query cache 内存剔除次数 # Counter elasticsearch_indices_query_cache_evictions # Query cache memory usage in bytes # query cache 内存占用（byte） # Gauge elasticsearch_indices_query_cache_memory_size_bytes # Total time spent refreshing in seconds # refresh操作耗时累计（秒） # Counter elasticsearch_indices_refresh_time_seconds_total # Total refreshes # refresh操作次数累计 # Counter elasticsearch_indices_refresh_total # Total search fetch time in seconds # fetch操作耗时累计（秒） # Counter elasticsearch_indices_search_fetch_time_seconds # Total number of fetches # fetch操作次数累计 # Counter elasticsearch_indices_search_fetch_total # Total search query time in seconds # query操作耗时累计（秒） # Counter elasticsearch_indices_search_query_time_seconds # Total number of queries # query操作次数累计 # Counter elasticsearch_indices_search_query_total # Segments with only primary shards on all nodes # 所有节点主分片segment总数 # Gauge elasticsearch_indices_segment_count_primary # Segments with all shards on all nodes # 所有节点所有分片segment总数 # Gauge elasticsearch_indices_segment_count_total # Doc values with only primary shards on all nodes in bytes # 主分片doc value内存占用（byte） # Gauge elasticsearch_indices_segment_doc_values_memory_bytes_primary # Doc values with all shards on all nodes in bytes # 所有分片doc value内存占用（byte） # Gauge elasticsearch_indices_segment_doc_values_memory_bytes_total # Size of fields with only primary shards on all nodes in bytes # 分片field内存占用（byte） # Gauge elasticsearch_indices_segment_fields_memory_bytes_primary # Size of fields with all shards on all nodes in bytes # 所有分片field内存占用（byte） # Gauge elasticsearch_indices_segment_fields_memory_bytes_total # Size of fixed bit with only primary shards on all nodes in bytes # 主分片fixed bit set内存占用（byte） # Gauge elasticsearch_indices_segment_fixed_bit_set_memory_bytes_primary # Size of fixed bit with all shards on all nodes in bytes # 所有分片fixed bit set内存占用（byte） # Gauge elasticsearch_indices_segment_fixed_bit_set_memory_bytes_total # Index writer with only primary shards on all nodes in bytes # 主分片索引写入数据量（byte） # Gauge elasticsearch_indices_segment_index_writer_memory_bytes_primary # Index writer with all shards on all nodes in bytes # 所有分片索引写入数据量（byte） # Gauge elasticsearch_indices_segment_index_writer_memory_bytes_total # Size of segments with only primary shards on all nodes in bytes # 主分片segment数 # Gauge elasticsearch_indices_segment_memory_bytes_primary # Size of segments with all shards on all nodes in bytes # 所有分片segment总数 # Gauge elasticsearch_indices_segment_memory_bytes_total # Size of norms with only primary shards on all nodes in bytes # 主分片normalization factor内存占用（byte） # Gauge elasticsearch_indices_segment_norms_memory_bytes_primary # Size of norms with all shards on all nodes in bytes # 所有分片normalization factor内存占用（byte） # Gauge elasticsearch_indices_segment_norms_memory_bytes_total # Size of points with only primary shards on all nodes in bytes # 主分片point内存占用（byte） # Gauge elasticsearch_indices_segment_points_memory_bytes_primary # Size of points with all shards on all nodes in bytes # 所有分片point内存占用（byte） # Gauge elasticsearch_indices_segment_points_memory_bytes_total # Size of terms with only primary shards on all nodes in bytes # 主分片term内存占用（byte） # Gauge elasticsearch_indices_segment_terms_memory_primary # Number of terms with all shards on all nodes in bytes # 所有分片term内存占用（byte） # Gauge elasticsearch_indices_segment_terms_memory_total # Size of version map with only primary shards on all nodes in bytes # 所有分片version map内存占用（byte） # Gauge elasticsearch_indices_segment_version_map_memory_bytes_primary # Size of version map with all shards on all nodes in bytes # 所有分片version map内存占用（byte） # Gauge elasticsearch_indices_segment_version_map_memory_bytes_total # Count of index segments # segment个数 # Gauge elasticsearch_indices_segments_count # Current memory size of segments in bytes # segment内存占用（byte） # Gauge elasticsearch_indices_segments_memory_bytes # Current size of stored index data in bytes with only primary shards on all nodes # 主分片索引容量（byte） # Gauge elasticsearch_indices_store_size_bytes_primary # Current size of stored index data in bytes with all shards on all nodes # 所有分片索引容量（byte） # Gauge elasticsearch_indices_store_size_bytes_total # Throttle time for index store in seconds # 索引存储限制耗时（秒） # Counter elasticsearch_indices_store_throttle_time_seconds_total # Total translog operations # tranlog操作数累计 # Counter elasticsearch_indices_translog_operations # Total translog size in bytes # tranlog大小累计（byte） # Counter elasticsearch_indices_translog_size_in_bytes # Count of JVM GC runs # GC运行次数累计 # Counter elasticsearch_jvm_gc_collection_seconds_count # GC run time in seconds # GC运行耗时累计（秒） # C欧BT而 elasticsearch_jvm_gc_collection_seconds_sum # JVM memory currently committed by area # JVM申请内存大小（byte） # Gauge elasticsearch_jvm_memory_committed_bytes # JVM memory max # JVM内存限制大小（byte） # Gauge elasticsearch_jvm_memory_max_bytes # JVM memory peak used by pool # JVM内存峰值大小（byte） # Counter elasticsearch_jvm_memory_pool_peak_used_bytes # JVM memory currently used by area # JVM内存占用大小（byte） # Gauge elasticsearch_jvm_memory_used_bytes # Shortterm load average # 系统负载（1分钟） # Gauge elasticsearch_os_load1 # Midterm load average # 系统负载（5分钟） # Gauge elasticsearch_os_load15 # Longterm load average # 系统负载（15分钟） # Gauge elasticsearch_os_load5 # Percent CPU used by process # 进程CPU占用率 # Gauge elasticsearch_process_cpu_percent # Open file descriptors # 进程打开文件数 # Gauge elasticsearch_process_open_files_count # Thread Pool threads active # 活跃线程总数 # Gauge elasticsearch_thread_pool_active_count # Thread Pool operations completed # 线程池complete次数 # Counter elasticsearch_thread_pool_completed_count # Thread Pool operations rejected # 线程池reject次数 # Counter elasticsearch_thread_pool_rejected_count # Total number of bytes received # 网络收流量（byte） # Counter elasticsearch_transport_rx_size_bytes_total # Total number of bytes received # 网络发流量（byte） # Counter elasticsearch_transport_tx_size_bytes_total 完整地配置项说明 # Enables the elasticsearch_exporter integration, allowing the Agent to automatically # collect system metrics from the configured ElasticSearch server address [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the hostname portion # of address. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the elasticsearch_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/elasticsearch_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # # Exporter-specific configuration options # # HTTP API address of an Elasticsearch node. [ address: \u003cstring\u003e | default = \"http://localhost:9200\" ] # Timeout for trying to get stats from Elasticsearch. [ timeout: \u003cduration\u003e | default = \"5s\" ] # Export stats for all nodes in the cluster. If used, this flag will override the flag `node`. [ all: \u003cboolean\u003e ] # Node's name of which metrics should be exposed. [ node: \u003cboolean\u003e ] # Export stats for indices in the cluster. [ indices: \u003cboolean\u003e ] # Export stats for settings of all indices of the cluster. [ indices_settings: \u003cboolean\u003e ] # Export stats for cluster settings. [ cluster_settings: \u003cboolean\u003e ] # Export stats for shards in the cluster (implies indices). [ shards: \u003cboolean\u003e ] # Export stats for the cluster snapshots. [ snapshots: \u003cboolean\u003e ] # Cluster info update interval for the cluster label. [ clusterinfo_interval: \u003cduration\u003e | default = \"5m\" ] # Path to PEM file that contains trusted Certificate Authorities for the Elasticsearch connection. [ ca: \u003cstring\u003e ] # Path to PEM file that contains the private key for client auth when connecting to Elasticsearch. [ client_private_key: \u003cstring\u003e ] # Path to PEM file that contains the corresponding cert for the private key to connect to Elasticsearch. [ client_cert: \u003cstring\u003e ] # Skip SSL verification when connecting to Elasticsearch. [ ssl_skip_verify: \u003cboolean\u003e ] ",
    "description": "",
    "tags": null,
    "title": "Elasticsearch Exporter",
    "uri": "/grafana-agent/integrations/elasticsearch-exporter-config/"
  },
  {
    "content": "The github_exporter_config block configures the github_exporter integration, which is an embedded version of github_exporter. This allows for the collection of metrics from the github api.\nWe strongly recommend that you configure a separate authentication token for the Agent, and give it only the strictly mandatory security privileges necessary for monitoring your repositories, as per the official documentation. We also recommend that you use api_token_file parameter, to avoid setting the authentication token directly on the Agent config file.\nFull reference of options:\n# Enables the github_exporter integration, allowing the Agent to automatically # collect metrics for the specified github objects. [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the hostname portion # of api_url. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the github_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/github_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # # Exporter-specific configuration options # # The full URI of the github API. [api_url: \u003cstring\u003e | default = \"https://api.github.com\"] # A list of github repositories for which to collect metrics. repositories: [ - \u003cstring\u003e ] # A list of github organizations for which to collect metrics. organizations: [ - \u003cstring\u003e ] # A list of github users for which to collect metrics. users: [ - \u003cstring\u003e ] # A github authentication token that allows the API to be queried more often. # Optional, but recommended. [api_token: \u003cstring\u003e] # A path to a file containing a github authentication token that allows the # API to be queried more often. If supplied, this supercedes `api_token` # Optional, but recommended. [api_token_file: \u003cstring\u003e] ",
    "description": "",
    "tags": null,
    "title": "Github Exporter",
    "uri": "/grafana-agent/integrations/github-exporter-config/"
  },
  {
    "content": "grafana-agent内置了kafka_exporter，来采集kafka的metrics指标。\n我们强烈推荐您使用独立的账号运行grafana-agent，并做好访问kafka实例的最小化授权，避免过度授权带来的安全隐患，更多可以参考documentation。\n配置并启用kafka_exporter kafka_exporter: enabled: true # Address array (host:port) of Kafka server kafka_uris: ['xxx','yyy'] 采集的关键指标列表 kafka_brokers: count of kafka_brokers (gauge) kafka_topic_partitions: Number of partitions for this Topic (gauge) kafka_topic_partition_current_offset: Current Offset of a Broker at Topic/Partition (gauge) kafka_consumergroup_current_offset: Current Offset of a ConsumerGroup at Topic/Partition (gauge) kafka_consumer_lag_millis: Current approximation of consumer lag for a ConsumerGroup at Topic/Partition (gauge) kafka_topic_partition_under_replicated_partition: 1 if Topic/Partition is under Replicated 完整地配置项说明 # Enables the kafka_exporter integration, allowing the Agent to automatically # collect system metrics from the configured dnsmasq server address [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the hostname # portion of the first kafka_uri value. If there is more than one string # in kafka_uri, the integration will fail to load and an instance value # must be manually provided. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the dnsmasq_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/dnsmasq_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # Address array (host:port) of Kafka server [kafka_uris: \u003c[]string\u003e] # Connect using SASL/PLAIN [use_sasl: \u003cbool\u003e] # Only set this to false if using a non-Kafka SASL proxy [use_sasl_handshake: \u003cbool\u003e | default = true] # SASL user name [sasl_username: \u003cstring\u003e] # SASL user password [sasl_password: \u003cstring\u003e] # The SASL SCRAM SHA algorithm sha256 or sha512 as mechanism [sasl_mechanism: \u003cstring\u003e] # Connect using TLS [use_tls: \u003cbool\u003e] # The optional certificate authority file for TLS client authentication [ca_file: \u003cstring\u003e] # The optional certificate file for TLS client authentication [cert_file: \u003cstring\u003e] # The optional key file for TLS client authentication [key_file: \u003cstring\u003e] # If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure [insecure_skip_verify: \u003cbool\u003e] # Kafka broker version [kafka_version: \u003cstring\u003e | default = \"2.0.0\"] # if you need to use a group from zookeeper [use_zookeeper_lag: \u003cbool\u003e] # Address array (hosts) of zookeeper server. [zookeeper_uris: \u003c[]string\u003e] # Kafka cluster name [kafka_cluster_name: \u003cstring\u003e] # Metadata refresh interval [metadata_refresh_interval: \u003cduration\u003e | default = \"1m\"] # If true, all scrapes will trigger kafka operations otherwise, they will share results. WARN: This should be disabled on large clusters [allow_concurrency: \u003cbool\u003e | default = true] # Maximum number of offsets to store in the interpolation table for a partition [max_offsets: \u003cint\u003e | default = 1000] # How frequently should the interpolation table be pruned, in seconds [prune_interval_seconds: \u003cint\u003e | default = 30] # Regex filter for topics to be monitored [topics_filter_regex: \u003cstring\u003e | default = \".*\"] # Regex filter for consumer groups to be monitored [groups_filter_regex: \u003cstring\u003e | default = \".*\"] ",
    "description": "",
    "tags": null,
    "title": "Kafka Exporter",
    "uri": "/grafana-agent/integrations/kafka-exporter-config/"
  },
  {
    "content": "grafana-agent内置了memcached_exporter，来采集memcached的运行指标。\n当前grafana-agent只支持配置一个memcached的地址来采集其metrics数据。如果您需要采集多个memcached的metrics指标，那么需要启动多个grafana-agent实例，并通过relabel_configs来区分来自不同memcached server的metrics。\n配置并启用memcached_exporter memcached_exporter: enabled: true memcached_address: memcached-a:53 relabel_configs: - source_labels: [__address__] target_label: instance replacement: memcached-a 采集的关键指标列表 memcached_commands_total : sum (memcached_commands_total{instance=~\"$node\", command=\"set\"}) / sum (memcached_commands_total{instance=~\"$node\", command=\"get\"}) memcached_commands_total : sum (memcached_commands_total{instance=~\"$node\", status=\"miss\"}) / sum (memcached_commands_total{instance=~\"$node\"}) memcached_commands_total : sum (memcached_commands_total{instance=~\"$node\"}) by (command) memcached_current_bytes : sum(memcached_current_bytes{instance=~\"$node\"}) / sum(memcached_limit_bytes{instance=~\"$node\"}) memcached_current_connections : sum (memcached_current_connections{instance=~\"$node\"}) by (instance) memcached_current_items : sum (memcached_current_items{instance=~\"$node\"}) memcached_items_evicted_total : sum(memcached_items_evicted_total{instance=~\"$node\"}) memcached_items_reclaimed_total : sum(memcached_items_reclaimed_total{instance=~\"$node\"}) memcached_read_bytes_total : sum(irate(memcached_read_bytes_total{instance=~\"$node\"}[5m])) memcached_written_bytes_total : irate(memcached_written_bytes_total{instance=~\"$node\"}[10m]) 完整地配置项说明 # Enables the memcached_exporter integration, allowing the Agent to automatically # collect system metrics from the configured memcached server address [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from # memcached_address. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the memcached_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/memcached_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # # Exporter-specific configuration options # # Address of the memcached server in host:port form. [memcached_address: \u003cstring\u003e | default = \"localhost:53\"] # Timeout for connecting to memcached. [timeout: \u003cduration\u003e | default = \"1s\"] ",
    "description": "",
    "tags": null,
    "title": "Memcached Exporter",
    "uri": "/grafana-agent/integrations/memcached-exporter-config/"
  },
  {
    "content": "grafana-agent内置了mongodb_exporter，可以采集mongodb的metrics。\n该mongodb_exporter，不支持同时配置多个mongodb node，目前只支持配置一个mongodb node，对其进行数据采集。此外您需要通过relabel_configs对label做自定义处理，一个是service_name，用来标识mongodb node（例如ReplicaSet1-Node1）；另一个是mongodb_cluster，标识该mongodb cluster（比如prod-cluster）\n一个relabel_configs的例子：\nrelabel_configs: - source_labels: [__address__] target_label: service_name replacement: 'replicaset1-node1' - source_labels: [__address__] target_label: mongodb_cluster replacement: 'prod-cluster' 强烈推荐您为grafana-agent设置一个单独的账号来访问您的mongodb，以避免过度授权带来的安全隐患，具体可以参考official documentation。\n配置并启用mongodb_exporter # grafana-agent 本身的配置 server: log_level: info http_listen_port: 12345 # grafana-agent 抓取 metrics 的相关配置（类似于prometheus的scrape_configs） metrics: global: scrape_interval: 15s scrape_timeout: 10s remote_write: - url: https://n9e-server:19000/prometheus/v1/write basic_auth: username: \u003cstring\u003e password: \u003cstring\u003e integrations: mongodb_exporter: enabled: true 采集的关键指标列表 # Whether MongoDB is up. # 实例是否存活 # Gauge mongodb_up # The number of seconds that the current MongoDB process has been active # 实例启动累计时间（秒） # Counter mongodb_instance_uptime_seconds # The amount of memory, in mebibyte (MiB), currently used by the database process # 内存占用（MiB） # Gauge # mongodb_memory # The total combined latency in microseconds # 累计操作耗时（毫秒） mongodb_mongod_op_latencies_latency_total # The total number of operations performed since startup # 累计操作次数 # Counter mongodb_mongod_op_latencies_ops_total # The total number of operations received since the mongod instance last started  # 累计接收的操作请求次数（即使操作不成功也会增加） # Counter mongodb_op_counters_total # The number of incoming connections from clients to the database server. This number includes the current shell session # 连接数 # Gauge # mongodb_connections # The number of open cursors # 打开游标数量 # Gauge mongodb_mongod_metrics_cursor_open # The total number of document access and modification patterns # 累计文档操作次数 # Counter mongodb_mongod_metrics_document_total # The total number of operations queued waiting for the lock # 当前排队等待获取锁的操作个数 # Gauge mongodb_mongod_global_lock_current_queue # The total number of (index or document) items scanned during queries and query-plan evaluation # 查询和查询计划评估过程扫描的（索引或文档）条目总数 # Counter  mongodb_mongod_metrics_query_executor_total # The number of assertions raised since the MongoDB process started # 累计断言错误次数 # Counter mongodb_asserts_total # The total number of getLastError operations with a specified write concern (i.e. w) that wait for one or more members of a replica set to acknowledge the write operation (i.e. a w value greater than 1.) # 累计getLastError操作数量 # Counter mongodb_mongod_metrics_get_last_error_wtime_num_total # The number of times that write concern operations have timed out as a result of the wtimeout threshold to getLastError. This number increments for both default and non-default write concern specifications. # 累计getLastError超时操作数量 # Counter mongodb_mongod_metrics_get_last_error_wtimeouts_total # Size in byte of the data currently in cache # 当前缓存数据大小（byte） # Gauge mongodb_mongod_wiredtiger_cache_bytes # Size in byte of the data read into or write from cache  # 写入或读取的缓存数据大小（byte） # Counter mongodb_mongod_wiredtiger_cache_bytes_total # Number of pages currently held in the cache # 当前缓存页数量 # Gauge mongodb_mongod_wiredtiger_cache_pages # The total number of pages (modified or unmodified) evicted # 累计缓存移除页数量 # Counter mongodb_mongod_wiredtiger_cache_evicted_total # The total number of page faults # 累计缺页中断次数 # Counter mongodb_extra_info_page_faults_total # The total number of bytes that the server has sent over network connections initiated by clients or other mongod or mongos instances. # 累计发送网络流量（byte） # Counter mongodb_ss_network_bytesOut # The total number of bytes that the server has received over network connections initiated by clients or other mongod or mongos instances # 累计接收网络流量（byte） # Counter mongodb_ss_network_bytesIn # The timestamp the node was elected as replica leader # 副本集选主时间 # Gauge mongodb_mongod_replset_member_election_date # The replication lag that this member has with the primary # 副本集成员主从延迟（秒） # Gauge mongodb_mongod_replset_member_replication_lag 完整地配置项说明 # Enables the mongodb_exporter integration [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the hostname # portion of the mongodb_uri field. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the mongodb_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/mongodb_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # metrics.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # metrics.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # # Exporter-specific configuration options # # MongoDB node connection URL, which must be in the [`Standard Connection String Format`](https://docs.mongodb.com/manual/reference/connection-string/#std-label-connections-standard-connection-string-format) [mongodb_uri: \u003cstring\u003e] ",
    "description": "",
    "tags": null,
    "title": "Mongodb Exporter",
    "uri": "/grafana-agent/integrations/mongodb-exporter-config/"
  },
  {
    "content": "grafana-agent 内置集成了 mysqld_exporter， 来收集MySQL Server的metrics指标。\n目前一个grafana-agent实例，只能配置和采集一个MySQL server的metrics，因此如果您想要配置采集多个MySQL server的指标，那么需要启动多个grafana-agent实例，并使用 relabel_configs 机制来给不同的MySQL server的metrics数据做区分。\n配置并启用mysqld_exporter 下面是开启了mysqld_exporter的配置文件示例:\nmysqld_exporter: enabled: true data_source_name: root@(server-a:3306)/ relabel_configs: - source_labels: [__address__] target_label: instance replacement: server-a 为了安全起见，推荐您为grafana-agent mysqld_exporter 配置一个单独的数据库账号，并授予合适的权限，需要的权限配置详情可以参考 MySQL Expoter 官方文档.\n采集的关键指标列表 mysql_global_status_uptime: The number of seconds that the server has been up.(Gauge) mysql_global_status_uptime_since_flush_status: The number of seconds since the most recent FLUSH STATUS statement.(Gauge) mysql_global_status_queries: The number of statements executed by the server. This variable includes statements executed within stored programs, unlike the Questions variable. It does not count COM_PING or COM_STATISTICS commands.(Counter) mysql_global_status_threads_connected: The number of currently open connections.(Counter) mysql_global_status_connections: The number of connection attempts (successful or not) to the MySQL server.(Gauge) mysql_global_status_max_used_connections: The maximum number of connections that have been in use simultaneously since the server started.(Gauge) mysql_global_status_threads_running: The number of threads that are not sleeping.(Gauge) mysql_global_status_questions: The number of statements executed by the server. This includes only statements sent to the server by clients and not statements executed within stored programs, unlike the Queries variable. This variable does not count COM_PING, COM_STATISTICS, COM_STMT_PREPARE, COM_STMT_CLOSE, or COM_STMT_RESET commands.(Counter) mysql_global_status_threads_cached: The number of threads in the thread cache.(Counter) mysql_global_status_threads_created: The number of threads created to handle connections. If Threads_created is big, you may want to increase the thread_cache_size value. The cache miss rate can be calculated as Threads_created/Connections.(Counter) mysql_global_status_created_tmp_tables: The number of internal temporary tables created by the server while executing statements.(Counter) mysql_global_status_created_tmp_disk_tables: The number of internal on-disk temporary tables created by the server while executing statements. You can compare the number of internal on-disk temporary tables created to the total number of internal temporary tables created by comparing Created_tmp_disk_tables and Created_tmp_tables values.(Counter) mysql_global_status_created_tmp_files: How many temporary files mysqld has created.(Counter) mysql_global_status_select_full_join: The number of joins that perform table scans because they do not use indexes. If this value is not 0, you should carefully check the indexes of your tables.(Counter) mysql_global_status_select_full_range_join: The number of joins that used a range search on a reference table.(Counter) mysql_global_status_select_range: The number of joins that used ranges on the first table. This is normally not a critical issue even if the value is quite large.(Counter) mysql_global_status_select_range_check: The number of joins without keys that check for key usage after each row. If this is not 0, you should carefully check the indexes of your tables.(Counter) mysql_global_status_select_scan: The number of joins that did a full scan of the first table.(Counter) mysql_global_status_sort_rows: The number of sorted rows.(Counter) mysql_global_status_sort_range: The number of sorts that were done using ranges.(Counter) mysql_global_status_sort_merge_passes: The number of merge passes that the sort algorithm has had to do. If this value is large, you should consider increasing the value of the sort_buffer_size system variable.(Counter) mysql_global_status_sort_scan: The number of sorts that were done by scanning the table.(Counter) mysql_global_status_slow_queries: The number of queries that have taken more than long_query_time seconds. This counter increments regardless of whether the slow query log is enabled.(Counter) mysql_global_status_aborted_connects: The number of failed attempts to connect to the MySQL server.(Counter) mysql_global_status_aborted_clients: The number of connections that were aborted because the client died without closing the connection properly.(Counter) mysql_global_status_table_locks_immediate: The number of times that a request for a table lock could be granted immediately. Locks Immediate rising and falling is normal activity.(Counter) mysql_global_status_table_locks_waited: The number of times that a request for a table lock could not be granted immediately and a wait was needed. If this is high and you have performance problems, you should first optimize your queries, and then either split your table or tables or use replication.(Counter) mysql_global_status_bytes_received: The number of bytes received from all clients.(Counter) mysql_global_status_bytes_sent: The number of bytes sent to all clients.(Counter) mysql_global_status_innodb_page_size: InnoDB page size (default 16KB). Many values are counted in pages; the page size enables them to be easily converted to bytes.(Gauge) mysql_global_status_buffer_pool_pages: The number of pages in the InnoDB buffer pool.(Gauge) mysql_global_status_commands_total: The number of times each xxx statement has been executed.(Counter) mysql_global_status_handlers_total: Handler statistics are internal statistics on how MySQL is selecting, updating, inserting, and modifying rows, tables, and indexes. This is in fact the layer between the Storage Engine and MySQL.(Counter) mysql_global_status_opened_files: The number of files that have been opened with my_open() (a mysys library function). Parts of the server that open files without using this function do not increment the count.(Counter) mysql_global_status_open_tables: The number of tables that are open.(Gauge) mysql_global_status_opened_tables: The number of tables that have been opened. If Opened_tables is big, your table_open_cache value is probably too small.(Counter) mysql_global_status_table_open_cache_hits: The number of hits for open tables cache lookups.(Counter) mysql_global_status_table_open_cache_misses: The number of misses for open tables cache lookups.(Counter) mysql_global_status_table_open_cache_overflows: The number of overflows for the open tables cache.(Counter) mysql_global_status_innodb_num_open_files: The number of files InnoDB currently holds open.(Gauge) mysql_global_variables_thread_cache_size: How many threads the server should cache for reuse.(Gauge) mysql_global_variables_max_connections: The maximum permitted number of simultaneous client connections.(Gauge) mysql_global_variables_innodb_buffer_pool_size: The size in bytes of the buffer pool, the memory area where InnoDB caches table and index data. The default value is 134217728 bytes (128MB).(Gauge) mysql_global_variables_innodb_log_buffer_size: The size in bytes of the buffer that InnoDB uses to write to the log files on disk.(Gauge) mysql_global_variables_key_buffer_size: Index blocks for MyISAM tables are buffered and are shared by all threads.(Gauge) mysql_global_variables_query_cache_size: The amount of memory allocated for caching query results.(Gauge) mysql_global_variables_table_open_cache: The number of open tables for all threads.(Gauge) mysql_global_variables_open_files_limit: The number of file descriptors available to mysqld from the operating system.(Gauge) mysqld-exporter-config详细配置项说明 # Enables the mysqld_exporter integration, allowing the Agent to collect # metrics from a MySQL server. [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is a truncated version of the # connection DSN, containing only the server and db name. (Credentials # are not included.) [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the mysqld_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/mysqld_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Data Source Name specifies the MySQL server to connect to. This is REQUIRED # but may also be specified by the MYSQLD_EXPORTER_DATA_SOURCE_NAME # environment variable. If neither are set, the integration will fail to # start. # # The format of this is specified here: https://github.com/go-sql-driver/mysql#dsn-data-source-name # # A working example value for a server with no required password # authentication is: \"root@(localhost:3306)/\" data_source_name: \u003cstring\u003e # A list of collector names to enable on top of the default set. enable_collectors: [ - \u003cstring\u003e ] # A list of collector names to disable from the default set. disable_collectors: [ - \u003cstring\u003e ] # A list of collectors to run. Fully overrides the default set. set_collectors: [ - \u003cstring\u003e ] # Set a lock_wait_timeout on the connection to avoid long metadata locking. [lock_wait_timeout: \u003cint\u003e | default = 2] # Add a low_slow_filter to avoid slow query logging of scrapes. NOT supported # by Oracle MySQL. [log_slow_filter: \u003cbool\u003e | default = false] ## Collector-specific options # Minimum time a thread must be in each state to be counted. [info_schema_processlist_min_time: \u003cint\u003e | default = 0] # Enable collecting the number of processes by user. [info_schema_processlist_processes_by_user: \u003cbool\u003e | default = true] # Enable collecting the number of processes by host. [info_schema_processlist_processes_by_host: \u003cbool\u003e | default = true] # The list of databases to collect table stats for. * for all [info_schema_tables_databases: \u003cstring\u003e | default = \"*\"] # Limit the number of events statements digests by response time. [perf_schema_eventsstatements_limit: \u003cint\u003e | default = 250] # Limit how old the 'last_seen' events statements can be, in seconds. [perf_schema_eventsstatements_time_limit: \u003cint\u003e | default = 86400] # Maximum length of the normalized statement text. [perf_schema_eventsstatements_digtext_text_limit: \u003cint\u003e | default = 120] # Regex file_name filter for performance_schema.file_summary_by_instance [perf_schema_file_instances_filter: \u003cstring\u003e | default = \".*\"] # Remove path prefix in performance_schema.file_summary_by_instance [perf_schema_file_instances_remove_prefix: \u003cstring\u003e | default = \"/var/lib/mysql\"] # Database from where to collect heartbeat data. [heartbeat_database: \u003cstring\u003e | default = \"heartbeat\"] # Table from where to collect heartbeat data. [heartbeat_table: \u003cstring\u003e | default = \"heartbeat\"] # Use UTC for timestamps of the current server (`pt-heartbeat` is called with `--utc`) [heartbeat_utc: \u003cbool\u003e | default = false] # Enable collecting user privileges from mysql.user [mysql_user_privileges: \u003cbool\u003e | default = false] ",
    "description": "",
    "tags": null,
    "title": "MySQLd Exporter",
    "uri": "/grafana-agent/integrations/mysqld-exporter-config/"
  },
  {
    "content": "grafana-agent 内置了 node_exporter, 可以通过在配置文件中 integrations 部分定义 node_exporter_config 来开启该功能。\n配置并启用node_exporter 下面是开启了node_exporter的配置文件示例，生成的配置文件保存为 ./grafana-agent-cfg.yaml:\ncat \u003c\u003cEOF \u003e ./grafana-agent-cfg.yaml # grafana-agent 本身的配置 server: log_level: info http_listen_port: 12345 # grafana-agent 抓取 metrics 的相关配置（类似于prometheus的scrape_configs） metrics: global: scrape_interval: 15s scrape_timeout: 10s remote_write: - url: https://n9e-server:19000/prometheus/v1/write basic_auth: username: \u003cstring\u003e password: \u003cstring\u003e integrations: node_exporter: enabled: true EOF 注意： remote_write 可以配置在 global 部分，也可以针对每个 integration 单独配置不同的remote_write 地址。\n重启grafana-agent后，通过以下两个命令，验证 node_exporter 工作是否符合预期。\ncurl http://localhost:12345/integrations/node_exporter/metrics ，预期输出如下内容：\nnode_boot_time_seconds 1.643256088e+09 node_context_switches_total 1.5136425575e+10 node_cooling_device_cur_state{name=\"0\",type=\"Processor\"} 0 node_cooling_device_cur_state{name=\"1\",type=\"Processor\"} 0 node_cooling_device_cur_state{name=\"2\",type=\"Processor\"} 0 node_cooling_device_cur_state{name=\"3\",type=\"Processor\"} 0 node_cooling_device_max_state{name=\"0\",type=\"Processor\"} 0 node_cooling_device_max_state{name=\"1\",type=\"Processor\"} 0 node_cooling_device_max_state{name=\"2\",type=\"Processor\"} 0 node_cooling_device_max_state{name=\"3\",type=\"Processor\"} 0 node_cpu_seconds_total{cpu=\"0\",mode=\"idle\"} 1.66906519e+06 node_cpu_seconds_total{cpu=\"0\",mode=\"iowait\"} 5031.48 node_cpu_seconds_total{cpu=\"0\",mode=\"irq\"} 0 node_cpu_seconds_total{cpu=\"0\",mode=\"nice\"} 82.84 node_cpu_seconds_total{cpu=\"0\",mode=\"softirq\"} 2332.39 curl http://localhost:12345/agent/api/v1/targets | jq，预期输出如下内容：\n{ \"status\": \"success\", \"data\": [ { \"instance\": \"b81030837ec7f1d162489cb4009325c9\", \"target_group\": \"integrations/node_exporter\", \"endpoint\": \"http://127.0.0.1:12345/integrations/node_exporter/metrics\", \"state\": \"up\", \"labels\": { \"agent_hostname\": \"tt-fc-dev01.nj\", \"instance\": \"tt-fc-dev01.nj:12345\", \"job\": \"integrations/node_exporter\" }, \"discovered_labels\": { \"__address__\": \"127.0.0.1:12345\", \"__metrics_path__\": \"/integrations/node_exporter/metrics\", \"__scheme__\": \"http\", \"__scrape_interval__\": \"15s\", \"__scrape_timeout__\": \"10s\", \"agent_hostname\": \"tt-fc-dev01.nj\", \"job\": \"integrations/node_exporter\" }, \"last_scrape\": \"2022-02-16T18:53:08.79288957+08:00\", \"scrape_duration_ms\": 20, \"scrape_error\": \"\" }, { \"instance\": \"b81030837ec7f1d162489cb4009325c9\", \"target_group\": \"local_scrape\", \"endpoint\": \"http://127.0.0.1:12345/metrics\", \"state\": \"up\", \"labels\": { \"cluster\": \"txnjdev01\", \"instance\": \"127.0.0.1:12345\", \"job\": \"local_scrape\" }, \"discovered_labels\": { \"__address__\": \"127.0.0.1:12345\", \"__metrics_path__\": \"/metrics\", \"__scheme__\": \"http\", \"__scrape_interval__\": \"15s\", \"__scrape_timeout__\": \"10s\", \"cluster\": \"txnjdev01\", \"job\": \"local_scrape\" }, \"last_scrape\": \"2022-02-16T18:53:22.336820442+08:00\", \"scrape_duration_ms\": 4, \"scrape_error\": \"\" } ] } 可以看到，上面的返回结果的 targets 列表中，已经新增了一个instance，其 job 为 integrations/node_exporter，这说明 node_exporter 已经在正常工作了。\n注意：如果 grafana-agent 是运行在容器中时，那么要做以下修改调整：\n 确保在运行容器时，将宿主机的相关目录映射到容器中，如下所示，即 -v \"/:/host/root\"、 -v \"/sys:/host/sys\"、-v \"/proc:/host/proc\".  docker run \\ --net=\"host\" \\ --pid=\"host\" \\ --cap-add=SYS_TIME \\ -d \\ -v \"/:/host/root:ro\" \\ -v \"/sys:/host/sys:ro\" \\ -v \"/proc:/host/proc:ro\" \\ -v /tmp/grafana-agent:/etc/agent/data \\ -v /tmp/grafana-agent-config.yaml:/etc/agent/agent.yaml \\ grafana/agent:v0.23.0 \\ --config.file=/etc/agent/agent.yaml \\ --metrics.wal-directory=/etc/agent/data 其中，配置文件 /tmp/grafana-agent-config.yaml 中 node_exporter 部分要指定 rootfs/sysfs/procfs 在容器中的路径，您可以运行以下命令生成该测试配置文件（当然，您需要把 remote_write 替换为适合您的地址）。  cat \u003c\u003cEOF \u003e /tmp/grafana-agent-config.yaml server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 15s scrape_timeout: 10s remote_write: - url: https://n9e-server:19000/prometheus/v1/write basic_auth: username: \u003cstring\u003e password: \u003cstring\u003e integrations: node_exporter: enabled: true rootfs_path: /host/root sysfs_path: /host/sys procfs_path: /host/proc EOF 注意：如果 grafana-agent 是运行在 K8s 环境中，那么调整步骤如下：\n 推荐将 grafana-agent 的配置文件存储在configmap中, manifest文件如下：  cat \u003c\u003cEOF | apiVersion: v1 kind: ConfigMap metadata: name: grafana-agent namespace: ${NAMESPACE} data: agent.yaml: |server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 15s remote_write: - url: 'https://n9e-server:19000/prometheus/v1/write' basic_auth: username: ${FC_USERNAME} password: ${FC_PASSWORD} integrations: agent: enabled: true node_exporter: enabled: true  EOF envsubst | kubectl apply -f - kubectl describe configmap grafana-agent 生成grafana-agent的pod manifest文件如下，并创建相应Pod实例：  cat \u003c\u003c EOF | apiVersion: v1 kind: Pod metadata: name: grafana-agent namespace: ${NAMESPACE} spec: containers: - image: grafana/agent:v0.23.0 name: grafana-agent args: - --config.file=/fcetc/agent.yaml - --metrics.wal-directory=/etc/agent/data securityContext: capabilities: add: [\"SYS_TIME\"] privileged: true runAsUser: 0 volumeMounts: - name: rootfs mountPath: /host/root readOnly: true - name: sysfs mountPath: /host/sys readOnly: true - name: procfs mountPath: /host/proc readOnly: true - name: fccfg mountPath: /fcetc hostPID: true hostNetwork: true dnsPolicy: ClusterFirstWithHostNet volumes: - name: rootfs hostPath: path: / - name: sysfs hostPath: path: /sys - name: procfs hostPath: path: /proc - name: fccfg configMap: name: grafana-agent EOF envsubst |kubectl apply -f - kubectl logs grafana-agent #查看 grafana-agent 的日志 node_exporter采集的关键指标解析 # SYSTEM # CPU context switch 次数 node_context_switches_total: context_switches # Interrupts 次数 node_intr_total: Interrupts # 运行的进程数 node_procs_running: Processes in runnable state # 熵池大小 node_entropy_available_bits: Entropy available to random number generators node_time_seconds: System time in seconds since epoch (1970) node_boot_time_seconds: Node boot time, in unixtime # CPU node_cpu_seconds_total: Seconds the CPUs spent in each mode node_load1: cpu load 1m node_load5: cpu load 5m node_load15: cpu load 15m # MEM # 内核态 # 用户追踪已从交换区获取但尚未修改的页面的内存 node_memory_SwapCached_bytes: Memory that keeps track of pages that have been fetched from swap but not yet been modified # 内核用于缓存数据结构供自己使用的内存 node_memory_Slab_bytes: Memory used by the kernel to cache data structures for its own use # slab中可回收的部分 node_memory_SReclaimable_bytes: SReclaimable - Part of Slab, that might be reclaimed, such as caches # slab中不可回收的部分 node_memory_SUnreclaim_bytes: Part of Slab, that cannot be reclaimed on memory pressure # Vmalloc内存区的大小 node_memory_VmallocTotal_bytes: Total size of vmalloc memory area # vmalloc已分配的内存，虚拟地址空间上的连续的内存 node_memory_VmallocUsed_bytes: Amount of vmalloc area which is used # vmalloc区可用的连续最大快的大小，通过此指标可以知道vmalloc可分配连续内存的最大值 node_memory_VmallocChunk_bytes: Largest contigious block of vmalloc area which is free # 内存的硬件故障删除掉的内存页的总大小 node_memory_HardwareCorrupted_bytes: Amount of RAM that the kernel identified as corrupted / not working # 用于在虚拟和物理内存地址之间映射的内存 node_memory_PageTables_bytes: Memory used to map between virtual and physical memory addresses (gauge) # 内核栈内存，常驻内存，不可回收 node_memory_KernelStack_bytes: Kernel memory stack. This is not reclaimable # 用来访问高端内存，复制高端内存的临时buffer，称为“bounce buffering”，会降低I/O 性能 node_memory_Bounce_bytes: Memory used for block device bounce buffers #用户态 # 单个巨页大小 node_memory_Hugepagesize_bytes: Huge Page size # 系统分配的常驻巨页数 node_memory_HugePages_Total: Total size of the pool of huge pages # 系统空闲的巨页数 node_memory_HugePages_Free: Huge pages in the pool that are not yet allocated # 进程已申请但未使用的巨页数 node_memory_HugePages_Rsvd: Huge pages for which a commitment to allocate from the pool has been made, but no allocation # 超过系统设定的常驻HugePages数量的个数 node_memory_HugePages_Surp: Huge pages in the pool above the value in /proc/sys/vm/nr_hugepages # 透明巨页 Transparent HugePages (THP) node_memory_AnonHugePages_bytes: Memory in anonymous huge pages # inactivelist中的File-backed内存 node_memory_Inactive_file_bytes: File-backed memory on inactive LRU list # inactivelist中的Anonymous内存 node_memory_Inactive_anon_bytes: Anonymous and swap cache on inactive LRU list, including tmpfs (shmem) # activelist中的File-backed内存 node_memory_Active_file_bytes: File-backed memory on active LRU list # activelist中的Anonymous内存 node_memory_Active_anon_bytes: Anonymous and swap cache on active least-recently-used (LRU) list, including tmpfs # 禁止换出的页，对应 Unevictable 链表 node_memory_Unevictable_bytes: Amount of unevictable memory that can't be swapped out for a variety of reasons # 共享内存 node_memory_Shmem_bytes: Used shared memory (shared between several processes, thus including RAM disks) # 匿名页内存大小 node_memory_AnonPages_bytes: Memory in user pages not backed by files # 被关联的内存页大小 node_memory_Mapped_bytes: Used memory in mapped pages files which have been mmaped, such as libraries # file-backed内存页缓存大小 node_memory_Cached_bytes: Parked file data (file content) cache # 系统中有多少匿名页曾经被swap-out、现在又被swap-in并且swap-in之后页面中的内容一直没发生变化 node_memory_SwapCached_bytes: Memory that keeps track of pages that have been fetched from swap but not yet been modified # 被mlock()系统调用锁定的内存大小 node_memory_Mlocked_bytes: Size of pages locked to memory using the mlock() system call # 块设备(block device)所占用的缓存页 node_memory_Buffers_bytes: Block device (e.g. harddisk) cache node_memory_SwapTotal_bytes: Memory information field SwapTotal_bytes node_memory_SwapFree_bytes: Memory information field SwapFree_bytes # DISK node_filesystem_files_free: Filesystem space available to non-root users in byte node_filesystem_free_bytes: Filesystem free space in bytes node_filesystem_size_bytes: Filesystem size in bytes node_filesystem_files_free: Filesystem total free file nodes node_filesystem_files: Filesystem total free file nodes node_filefd_maximum: Max open files node_filefd_allocated: Open files node_filesystem_readonly: Filesystem read-only status node_filesystem_device_error: Whether an error occurred while getting statistics for the given device node_disk_reads_completed_total: The total number of reads completed successfully node_disk_writes_completed_total: The total number of writes completed successfully node_disk_reads_merged_total: The number of reads merged node_disk_writes_merged_total: The number of writes merged node_disk_read_bytes_total: The total number of bytes read successfully node_disk_written_bytes_total: The total number of bytes written successfully node_disk_io_time_seconds_total: Total seconds spent doing I/Os node_disk_read_time_seconds_total: The total number of seconds spent by all reads node_disk_write_time_seconds_total: The total number of seconds spent by all writes node_disk_io_time_weighted_seconds_total: The weighted of seconds spent doing I/Os # NET node_network_receive_bytes_total: Network device statistic receive_bytes (counter) node_network_transmit_bytes_total: Network device statistic transmit_bytes (counter) node_network_receive_packets_total: Network device statistic receive_bytes node_network_transmit_packets_total: Network device statistic transmit_bytes node_network_receive_errs_total: Network device statistic receive_errs node_network_transmit_errs_total: Network device statistic transmit_errs node_network_receive_drop_total: Network device statistic receive_drop node_network_transmit_drop_total: Network device statistic transmit_drop node_nf_conntrack_entries: Number of currently allocated flow entries for connection tracking node_sockstat_TCP_alloc: Number of TCP sockets in state alloc node_sockstat_TCP_inuse: Number of TCP sockets in state inuse node_sockstat_TCP_orphan: Number of TCP sockets in state orphan node_sockstat_TCP_tw: Number of TCP sockets in state tw node_netstat_Tcp_CurrEstab: Statistic TcpCurrEstab node_sockstat_sockets_used: Number of IPv4 sockets in use node_expoter integration 完整的配置项说明 # Enables the node_exporter integration, allowing the Agent to automatically # collect system metrics from the host UNIX system. [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the agent hostname # and HTTP listen port, delimited by a colon. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the node_exporter integration will be run but not scraped and thus not remote-written. Metrics for the # integration will be exposed at /integrations/node_exporter/metrics and can # be scraped by an external process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timtout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cboolean\u003e | default = false] # Optionally defines the the list of enabled-by-default collectors. # Anything not provided in the list below will be disabled by default, # but requires at least one element to be treated as defined. # # This is useful if you have a very explicit set of collectors you wish # to run. set_collectors: - [\u003cstring\u003e] # Additional collectors to enable on top of the default set of enabled # collectors or on top of the list provided by set_collectors. # # This is useful if you have a few collectors you wish to run that are # not enabled by default, but do not want to explicitly provide an entire # list through set_collectors. enable_collectors: - [\u003cstring\u003e] # Additional collectors to disable on top of the default set of disabled # collectors. Takes precedence over enable_collectors. # # This is useful if you have a few collectors you do not want to run that # are enabled by default, but do not want to explicitly provide an entire # list through set_collectors. disable_collectors: - [\u003cstring\u003e] # procfs mountpoint. [procfs_path: \u003cstring\u003e | default = \"/proc\"] # sysfs mountpoint. [sysfs_path: \u003cstring\u003e | default = \"/sys\"] # rootfs mountpoint. If running in docker, the root filesystem of the host # machine should be mounted and this value should be changed to the mount # directory. [rootfs_path: \u003cstring\u003e | default = \"/\"] # Expose expensive bcache priority stats. [enable_bcache_priority_stats: \u003cboolean\u003e] # Regexp of `bugs` field in cpu info to filter. [cpu_bugs_include: \u003cstring\u003e] # Enable the node_cpu_guest_seconds_total metric. [enable_cpu_guest_seconds_metric: \u003cboolean\u003e | default = true] # Enable the cpu_info metric for the cpu collector. [enable_cpu_info_metric: \u003cboolean\u003e | default = true] # Regexp of `flags` field in cpu info to filter. [cpu_flags_include: \u003cstring\u003e] # Regexmp of devices to ignore for diskstats. [diskstats_ignored_devices: \u003cstring\u003e | default = \"^(ram|loop|fd|(h|s|v|xv)d[a-z]|nvme\\\\d+n\\\\d+p)\\\\d+$\"] # Regexp of ethtool devices to exclude (mutually exclusive with ethtool_device_include) [ethtool_device_exclude: \u003cstring\u003e] # Regexp of ethtool devices to include (mutually exclusive with ethtool_device_exclude) [ethtool_device_include: \u003cstring\u003e] # Regexp of ethtool stats to include. [ethtool_metrics_include: \u003cstring\u003e | default = \".*\"] # Regexp of mount points to ignore for filesystem collector. [filesystem_mount_points_exclude: \u003cstring\u003e | default = \"^/(dev|proc|sys|var/lib/docker/.+)($|/)\"] # Regexp of filesystem types to ignore for filesystem collector. [filesystem_fs_types_exclude: \u003cstring\u003e | default = \"^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"] # How long to wait for a mount to respond before marking it as stale. [filesystem_mount_timeout: \u003cduration\u003e | default = \"5s\"] # Array of IPVS backend stats labels. # # The default is [local_address, local_port, remote_address, remote_port, proto, local_mark]. ipvs_backend_labels: [- \u003cstring\u003e] # NTP server to use for ntp collector [ntp_server: \u003cstring\u003e | default = \"127.0.0.1\"] # NTP protocol version [ntp_protocol_version: \u003cint\u003e | default = 4] # Certify that the server address is not a public ntp server. [ntp_server_is_local: \u003cboolean\u003e | default = false] # IP TTL to use wile sending NTP query. [ntp_ip_ttl: \u003cint\u003e | default = 1] # Max accumulated distance to the root. [ntp_max_distance: \u003cduration\u003e | default = \"3466080us\"] # Offset between local clock and local ntpd time to tolerate. [ntp_local_offset_tolerance: \u003cduration\u003e | default = \"1ms\"] # Regexp of net devices to ignore for netclass collector. [netclass_ignored_devices: \u003cstring\u003e | default = \"^$\"] # Ignore net devices with invalid speed values. This will default to true in # node_exporter 2.0. [netclass_ignore_invalid_speed_device: \u003cboolean\u003e | default = false] # Enable collecting address-info for every device. [netdev_address_info: \u003cboolean\u003e] # Regexp of net devices to exclude (mutually exclusive with include) [netdev_device_exclude: \u003cstring\u003e | default = \"\"] # Regexp of net devices to include (mutually exclusive with exclude) [netdev_device_include: \u003cstring\u003e | default = \"\"] # Regexp of fields to return for netstat collector. [netstat_fields: \u003cstring\u003e | default = \"^(.*_(InErrors|InErrs)|Ip_Forwarding|Ip(6|Ext)_(InOctets|OutOctets)|Icmp6?_(InMsgs|OutMsgs)|TcpExt_(Listen.*|Syncookies.*|TCPSynRetrans|TCPTimeouts)|Tcp_(ActiveOpens|InSegs|OutSegs|OutRsts|PassiveOpens|RetransSegs|CurrEstab)|Udp6?_(InDatagrams|OutDatagrams|NoPorts|RcvbufErrors|SndbufErrors))$\"] # List of CPUs from which perf metrics should be collected. [perf_cpus: \u003cstring\u003e | default = \"\"] # Array of perf tracepoints that should be collected. perf_tracepoint: [- \u003cstring\u003e] # Regexp of power supplies to ignore for the powersupplyclass collector. [powersupply_ignored_supplies: \u003cstring\u003e | default = \"^$\"] # Path to runit service directory. [runit_service_dir: \u003cstring\u003e | default = \"/etc/service\"] # XML RPC endpoint for the supervisord collector. # # Setting SUPERVISORD_URL in the environment will override the default value. # An explicit value in the YAML config takes precedence over the environment # variable. [supervisord_url: \u003cstring\u003e | default = \"http://localhost:9001/RPC2\"] # Regexp of systemd units to include. Units must both match include and not # match exclude to be collected. [systemd_unit_include: \u003cstring\u003e | default = \".+\"] # Regexp of systemd units to exclude. Units must both match include and not # match exclude to be collected. [systemd_unit_exclude: \u003cstring\u003e | default = \".+\\\\.(automount|device|mount|scope|slice)\"] # Enables service unit tasks metrics unit_tasks_current and unit_tasks_max [systemd_enable_task_metrics: \u003cboolean\u003e | default = false] # Enables service unit metric service_restart_total [systemd_enable_restarts_metrics: \u003cboolean\u003e | default = false] # Enables service unit metric unit_start_time_seconds [systemd_enable_start_time_metrics: \u003cboolean\u003e | default = false] # Regexp of tapestats devices to ignore. [tapestats_ignored_devices: \u003cstring\u003e | default = \"^$\"] # Directory to read *.prom files from for the textfile collector. [textfile_directory: \u003cstring\u003e | default = \"\"] # Regexp of fields to return for the vmstat collector. [vmstat_fields: \u003cstring\u003e | default = \"^(oom_kill|pgpg|pswp|pg.*fault).*\"] node_exporter 自定义 collectors 您可以在 integrations node_export 配置中，通过设置和修改 set_collectors enable_collectors disable_collectors，以控制哪些 collector 生效。\nconst ( CollectorARP = \"arp\" CollectorBCache = \"bcache\" CollectorBTRFS = \"btrfs\" CollectorBonding = \"bonding\" CollectorBootTime = \"boottime\" CollectorBuddyInfo = \"buddyinfo\" CollectorCPU = \"cpu\" CollectorCPUFreq = \"cpufreq\" CollectorConntrack = \"conntrack\" CollectorDMI = \"dmi\" CollectorDRBD = \"drbd\" CollectorDRM = \"drm\" CollectorDevstat = \"devstat\" CollectorDiskstats = \"diskstats\" CollectorEDAC = \"edac\" CollectorEntropy = \"entropy\" CollectorEthtool = \"ethtool\" CollectorExec = \"exec\" CollectorFibrechannel = \"fibrechannel\" CollectorFileFD = \"filefd\" CollectorFilesystem = \"filesystem\" CollectorHWMon = \"hwmon\" CollectorIPVS = \"ipvs\" CollectorInfiniband = \"infiniband\" CollectorInterrupts = \"interrupts\" CollectorKSMD = \"ksmd\" CollectorLnstat = \"lnstat\" CollectorLoadAvg = \"loadavg\" CollectorLogind = \"logind\" CollectorMDADM = \"mdadm\" CollectorMeminfo = \"meminfo\" CollectorMeminfoNuma = \"meminfo_numa\" CollectorMountstats = \"mountstats\" CollectorNFS = \"nfs\" CollectorNFSD = \"nfsd\" CollectorNTP = \"ntp\" CollectorNVME = \"nvme\" CollectorNetclass = \"netclass\" CollectorNetdev = \"netdev\" CollectorNetstat = \"netstat\" CollectorNetworkRoute = \"network_route\" CollectorOS = \"os\" CollectorPerf = \"perf\" CollectorPowersuppply = \"powersupplyclass\" CollectorPressure = \"pressure\" CollectorProcesses = \"processes\" CollectorQDisc = \"qdisc\" CollectorRAPL = \"rapl\" CollectorRunit = \"runit\" CollectorSchedstat = \"schedstat\" CollectorSockstat = \"sockstat\" CollectorSoftnet = \"softnet\" CollectorStat = \"stat\" CollectorSupervisord = \"supervisord\" CollectorSystemd = \"systemd\" CollectorTCPStat = \"tcpstat\" CollectorTapestats = \"tapestats\" CollectorTextfile = \"textfile\" CollectorThermal = \"thermal\" CollectorThermalzone = \"thermal_zone\" CollectorTime = \"time\" CollectorTimex = \"timex\" CollectorUDPQueues = \"udp_queues\" CollectorUname = \"uname\" CollectorVMStat = \"vmstat\" CollectorWiFi = \"wifi\" CollectorXFS = \"xfs\" CollectorZFS = \"zfs\" CollectorZoneinfo = \"zoneinfo\" ) ",
    "description": "",
    "tags": null,
    "title": "Node Exporter",
    "uri": "/grafana-agent/integrations/node-exporter-config/"
  },
  {
    "content": "grafana-agent内置了postgres_exporter，来采集Postgres Server的metrics采集。\n我们强烈推荐您分配独立的账号，供grafana-agent来连接到Postgres server，以避免过度授权带来的安全性问题，具体可以餐你考postgres exporter官方文档.\n配置并启用cadvisor_exporter server: log_level: info http_listen_port: 12345 metrics: global: scrape_interval: 15s remote_write: - url: 'https://n9e-server:19000/prometheus/v1/write' basic_auth: username: ${FC_USERNAME} password: ${FC_PASSWORD} integrations: postgres_exporter: enabled: true EOF 采集的关键指标列表 pg_locks_count : pg_locks_count{datname=~\"$datname\", instance=~\"$instance\", mode=~\"$mode\"} != 0 pg_postmaster_start_time_seconds : pg_postmaster_start_time_seconds{release=\"$release\", instance=\"$instance\"} * 1000 pg_settings_effective_cache_size_bytes : pg_settings_effective_cache_size_bytes{instance=\"$instance\"} pg_settings_maintenance_work_mem_bytes : pg_settings_maintenance_work_mem_bytes{instance=\"$instance\"} pg_settings_max_connections : pg_settings_max_connections{release=\"$release\", instance=\"$instance\"} pg_settings_max_parallel_workers : pg_settings_max_parallel_workers{instance=\"$instance\"} pg_settings_max_wal_size_bytes : pg_settings_max_wal_size_bytes{instance=\"$instance\"} pg_settings_max_worker_processes : pg_settings_max_worker_processes{instance=\"$instance\"} pg_settings_random_page_cost : pg_settings_random_page_cost{instance=\"$instance\"} pg_settings_seq_page_cost : pg_settings_seq_page_cost pg_settings_shared_buffers_bytes : pg_settings_shared_buffers_bytes{instance=\"$instance\"} pg_settings_work_mem_bytes : pg_settings_work_mem_bytes{instance=\"$instance\"} pg_stat_activity_count : pg_stat_activity_count{datname=~\"$datname\", instance=~\"$instance\", state=\"active\"} !=0 pg_stat_activity_count : pg_stat_activity_count{datname=~\"$datname\", instance=~\"$instance\", state=~\"idle|idle in transaction|idle in transaction (aborted)\"} pg_stat_bgwriter_buffers_alloc : irate(pg_stat_bgwriter_buffers_alloc{instance=\"$instance\"}[5m]) pg_stat_bgwriter_buffers_backend : irate(pg_stat_bgwriter_buffers_backend{instance=\"$instance\"}[5m]) pg_stat_bgwriter_buffers_backend_fsync : irate(pg_stat_bgwriter_buffers_backend_fsync{instance=\"$instance\"}[5m]) pg_stat_bgwriter_buffers_checkpoint : irate(pg_stat_bgwriter_buffers_checkpoint{instance=\"$instance\"}[5m]) pg_stat_bgwriter_buffers_clean : irate(pg_stat_bgwriter_buffers_clean{instance=\"$instance\"}[5m]) pg_stat_bgwriter_checkpoint_sync_time : irate(pg_stat_bgwriter_checkpoint_sync_time{instance=\"$instance\"}[5m]) pg_stat_bgwriter_checkpoint_write_time : irate(pg_stat_bgwriter_checkpoint_write_time{instance=\"$instance\"}[5m]) pg_stat_database_blks_hit : pg_stat_database_blks_hit{instance=\"$instance\", datname=~\"$datname\"} / (pg_stat_database_blks_read{instance=\"$instance\", datname=~\"$datname\"} + pg_stat_database_blks_hit{instance=\"$instance\", datname=~\"$datname\"}) pg_stat_database_conflicts : irate(pg_stat_database_conflicts{instance=\"$instance\", datname=~\"$datname\"}[5m]) pg_stat_database_deadlocks : irate(pg_stat_database_deadlocks{instance=\"$instance\", datname=~\"$datname\"}[5m]) pg_stat_database_temp_bytes : irate(pg_stat_database_temp_bytes{instance=\"$instance\", datname=~\"$datname\"}[5m]) pg_stat_database_tup_deleted : pg_stat_database_tup_deleted{datname=~\"$datname\", instance=~\"$instance\"} != 0 pg_stat_database_tup_fetched : SUM(pg_stat_database_tup_fetched{datname=~\"$datname\", instance=~\"$instance\"}) pg_stat_database_tup_fetched : pg_stat_database_tup_fetched{datname=~\"$datname\", instance=~\"$instance\"} != 0 pg_stat_database_tup_inserted : SUM(pg_stat_database_tup_inserted{release=\"$release\", datname=~\"$datname\", instance=~\"$instance\"}) pg_stat_database_tup_inserted : pg_stat_database_tup_inserted{datname=~\"$datname\", instance=~\"$instance\"} != 0 pg_stat_database_tup_returned : pg_stat_database_tup_returned{datname=~\"$datname\", instance=~\"$instance\"} != 0 pg_stat_database_tup_updated : SUM(pg_stat_database_tup_updated{datname=~\"$datname\", instance=~\"$instance\"}) pg_stat_database_tup_updated : pg_stat_database_tup_updated{datname=~\"$datname\", instance=~\"$instance\"} != 0 pg_stat_database_xact_commit : irate(pg_stat_database_xact_commit{instance=\"$instance\", datname=~\"$datname\"}[5m]) pg_stat_database_xact_rollback : irate(pg_stat_database_xact_rollback{instance=\"$instance\", datname=~\"$datname\"}[5m]) pg_static : pg_static{release=\"$release\", instance=\"$instance\"} process_cpu_seconds_total : avg(rate(process_cpu_seconds_total{release=\"$release\", instance=\"$instance\"}[5m]) * 1000) process_open_fds : process_open_fds{release=\"$release\", instance=\"$instance\"} process_resident_memory_bytes : avg(rate(process_resident_memory_bytes{release=\"$release\", instance=\"$instance\"}[5m])) process_virtual_memory_bytes : avg(rate(process_virtual_memory_bytes{release=\"$release\", instance=\"$instance\"}[5m])) 完整地配置项说明 # Enables the postgres_exporter integration, allowing the Agent to automatically # collect system metrics from the configured postgres server address [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from a truncated version of # the first DSN in data_source_names. The truncated DSN includes the hostname # and database name (if used) of the server, but does not include any user # information. # # If data_source_names contains more than one entry, the integration will fail to # load and a value for instance must be manually provided. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the postgres_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/postgres_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # # Exporter-specific configuration options # # Data Source Names specifies the Postgres server(s) to connect to. This is # REQUIRED but may also be specified by the POSTGRES_EXPORTER_DATA_SOURCE_NAME # environment variable, where DSNs the environment variable are separated by # commas. If neither are set, the integration will fail to start. # # The format of this is specified here: https://pkg.go.dev/github.com/lib/pq#ParseURL # # A working example value for a server with a password is: # \"postgresql://username:passwword@localhost:5432/database?sslmode=disable\" # # Multiple DSNs may be provided here, allowing for scraping from multiple # servers. data_source_names: - \u003cstring\u003e # Disables collection of metrics from pg_settings. [disable_settings_metrics: \u003cboolean\u003e | default = false] # Autodiscover databases to collect metrics from. If false, only collects # metrics from databases collected from data_source_names. [autodiscover_databases: \u003cboolean\u003e | default = false] # Excludes specific databases from being collected when autodiscover_databases # is true. exclude_databases: [ - \u003cstring\u003e ] # Includes only specific databases (excluding all others) when autodiscover_databases # is true. include_databases: [ - \u003cstring\u003e ] # Path to a YAML file containing custom queries to run. Check out # postgres_exporter's queries.yaml for examples of the format: # https://github.com/prometheus-community/postgres_exporter/blob/master/queries.yaml [query_path: \u003cstring\u003e | default = \"\"] # When true, only exposes metrics supplied from query_path. [disable_default_metrics: \u003cboolean\u003e | default = false] ",
    "description": "",
    "tags": null,
    "title": "Postgres Exporter",
    "uri": "/grafana-agent/integrations/postgres-exporter-config/"
  },
  {
    "content": "grafana-agent内置集成了process-exporter，基于/proc的文件分析结果，来收集Linux系统进程相关的指标（注意，非Linux系统开启该exporter不起作用）。\n如果grafana-agent运行在container中，那么在容器的启动命令中，要做以下调整，即将宿主机的/proc目录映射到容器中相应的位置。\ndocker run \\  -v \"/proc:/proc:ro\" \\  -v /tmp/agent:/etc/agent \\  -v /path/to/config.yaml:/etc/agent-config/agent.yaml \\  grafana/agent:v0.23.0 \\  --config.file=/etc/agent-config/agent.yaml 注意，将/path/to/config.yaml替换成您自己相应的配置文件。\n如果grafana-agent运行在Kubernetes中，那么同样的需要在manifest文件中，做如下调整，即将宿主机的/proc目录映射到容器中相应的位置。\napiVersion: v1 kind: Pod metadata: name: grafana-agent spec: containers: - image: grafana/agent:v0.23.0 name: agent args: - --config.file=/etc/agent-config/agent.yaml volumeMounts: - name: procfs mountPath: /proc readOnly: true volumes: - name: procfs hostPath: path: /proc 配置并启用process_exporter 如下的配置，将会开启process_exporter，并追踪系统中的所有进程。\nprocess_exporter: enabled: true process_names: - name: \"{{.Comm}}\" cmdline: - '.+' 采集的指标列表 # Context switches # 上下文切换数量 # Counter namedprocess_namegroup_context_switches_total # Cpu user/system usage in seconds # CPU 时间（秒） # Counter namedprocess_namegroup_cpu_seconds_total # Major page faults # 主要页缺失次数 # Counter namedprocess_namegroup_major_page_faults_total # Minor page faults # 次要页缺失次数 # Counter namedprocess_namegroup_minor_page_faults_total # number of bytes of memory in use # 内存占用（byte） # Gauge namedprocess_namegroup_memory_bytes # number of processes in this group # 同名进程数量 # Gauge namedprocess_namegroup_num_procs # Number of processes in states Running, Sleeping, Waiting, Zombie, or Other # 同名进程状态分布 # Gauge namedprocess_namegroup_states # Number of threads # 线程数量 # Gauge namedprocess_namegroup_num_threads # start time in seconds since 1970/01/01 of oldest process in group # 启动时间戳 # Gauge namedprocess_namegroup_oldest_start_time_seconds # number of open file descriptors for this group # 打开文件描述符数量 # Gauge namedprocess_namegroup_open_filedesc # the worst (closest to 1) ratio between open fds and max fds among all procs in this group # 打开文件数 / 允许打开文件数 # Gauge namedprocess_namegroup_worst_fd_ratio # number of bytes read by this group # 读数据量（byte） # Counter namedprocess_namegroup_read_bytes_total # number of bytes written by this group # 写数据量（byte） # Counter namedprocess_namegroup_write_bytes_total # Number of threads in this group waiting on each wchan # 内核wchan等待线程数量 # Gauge namedprocess_namegroup_threads_wchan process_exporter的详细配置项说明 # Enables the process_exporter integration, allowing the Agent to automatically # collect system metrics from the host UNIX system. [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the agent hostname # and HTTP listen port, delimited by a colon. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the process_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/process_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # procfs mountpoint. [procfs_path: \u003cstring\u003e | default = \"/proc\"] # If a proc is tracked, track with it any children that aren't a part of their # own group. [track_children: \u003cboolean\u003e | default = true] # Report on per-threadname metrics as well. [track_threads: \u003cboolean\u003e | default = true] # Gather metrics from smaps file, which contains proportional resident memory # size. [gather_smaps: \u003cboolean\u003e | default = true] # Recheck process names on each scrape. [recheck_on_scrape: \u003cboolean\u003e | default = false] # A collection of matching rules to use for deciding which processes to # monitor. Each config can match multiple processes to be tracked as a single # process \"group.\" process_names: [- \u003cprocess_matcher_config\u003e]  process_matcher_config\n # The name to use for identifying the process group name in the metric. By # default, it uses the base path of the executable. # # The following template variables are available: # # - {{.Comm}}: Basename of the original executable from /proc/\u003cpid\u003e/stat # - {{.ExeBase}}: Basename of the executable from argv[0] # - {{.ExeFull}}: Fully qualified path of the executable # - {{.Username}}: Username of the effective user # - {{.Matches}}: Map containing all regex capture groups resulting from # matching a process with the cmdline rule group. # - {{.PID}}: PID of the process. Note that the PID is copied from the # first executable found. # - {{.StartTime}}: The start time of the process. This is useful when combined # with PID as PIDS get reused over time. [name: \u003cstring\u003e | default = \"{{.ExeBase}}\"] # A list of strings that match the base executable name for a process, truncated # at 15 characters. It is derived from reading the second field of # /proc/\u003cpid\u003e/stat minus the parens. # # If any of the strings match, the process will be tracked. comm: [- \u003cstring\u003e] # A list of strings that match argv[0] for a process. If there are no slashes, # only the basename of argv[0] needs to match. Otherwise the name must be an # exact match. For example, \"postgres\" may match any postgres binary but # \"/usr/local/bin/postgres\" can only match a postgres at that path exactly. # # If any of the strings match, the process will be tracked. exe: [- \u003cstring\u003e] # A list of regular expressions applied to the argv of the process. Each # regex here must match the corresponding argv for the process to be tracked. # The first element that is matched is argv[1]. # # Regex Captures are added to the .Matches map for use in the name. cmdline: [- \u003cstring\u003e] ",
    "description": "",
    "tags": null,
    "title": "Process Exporter",
    "uri": "/grafana-agent/integrations/process-exporter-config/"
  },
  {
    "content": "grafana-agent内置了redis_exporter，可以采集Redis server的运行指标。\n目前grafana-agent，只支持配置一个Redis server地址，对其进行数据采集。如果您希望采集多个redis实例的metrics数据，那么需要启动多个grafana-agent实例，并通过relabel_configs来区分来自不同redis实例的数据。\n配置并启用redis_exporter redis_exporter: enabled: true redis_addr: \"redis-2:6379\" relabel_configs: - source_labels: [__address__] target_label: instance replacement: redis-2 我们强烈推荐您使用独立的账号运行grafana-agent，并做好访问redis实例的最小化授权，避免过度授权带来的安全隐患，更多可以参考official documentation。\n采集的关键指标列表 redis_active_defrag_running: When activedefrag is enabled, this indicates whether defragmentation is currently active, and the CPU percentage it intends to utilize. redis_allocator_active_bytes: Total bytes in the allocator active pages, this includes external-fragmentation. redis_allocator_allocated_bytes: Total bytes allocated form the allocator, including internal-fragmentation. Normally the same as used_memory. redis_allocator_frag_bytes: Delta between allocator_active and allocator_allocated. See note about mem_fragmentation_bytes. redis_allocator_frag_ratio: Ratio between allocator_active and allocator_allocated. This is the true (external) fragmentation metric (not mem_fragmentation_ratio). redis_allocator_resident_bytes: Total bytes resident (RSS) in the allocator, this includes pages that can be released to the OS (by MEMORY PURGE, or just waiting). redis_allocator_rss_bytes: Delta between allocator_resident and allocator_active. redis_allocator_rss_ratio: Ratio between allocator_resident and allocator_active. This usually indicates pages that the allocator can and probably will soon release back to the OS. redis_aof_current_rewrite_duration_sec: Duration of the on-going AOF rewrite operation if any. redis_aof_enabled: Flag indicating AOF logging is activated. redis_aof_last_bgrewrite_status: Status of the last AOF rewrite operation. redis_aof_last_cow_size_bytes: The size in bytes of copy-on-write memory during the last AOF rewrite operation. redis_aof_last_rewrite_duration_sec: Duration of the last AOF rewrite operation in seconds. redis_aof_last_write_status: Status of the last write operation to the AOF. redis_aof_rewrite_in_progress: Flag indicating a AOF rewrite operation is on-going. redis_aof_rewrite_scheduled: Flag indicating an AOF rewrite operation will be scheduled once the on-going RDB save is complete. redis_blocked_clients: Number of clients pending on a blocking call (BLPOP, BRPOP, BRPOPLPUSH, BLMOVE, BZPOPMIN, BZPOPMAX). redis_client_recent_max_input_buffer_bytes: Biggest input buffer among current client connections. redis_client_recent_max_output_buffer_bytes: Biggest output buffer among current client connections. redis_cluster_enabled: Indicate Redis cluster is enabled. redis_commands_duration_seconds_total: The total CPU time consumed by these commands.(Counter) redis_commands_processed_total: Total number of commands processed by the server.(Counter) redis_commands_total: The number of calls that reached command execution (not rejected).(Counter) redis_config_maxclients: The value of the maxclients configuration directive. This is the upper limit for the sum of connected_clients, connected_slaves and cluster_connections. redis_config_maxmemory: The value of the maxmemory configuration directive. redis_connected_clients: Number of client connections (excluding connections from replicas). redis_connected_slaves: Number of connected replicas. redis_connections_received_total: Total number of connections accepted by the server.(Counter) redis_cpu_sys_children_seconds_total: System CPU consumed by the background processes.(Counter) redis_cpu_sys_seconds_total: System CPU consumed by the Redis server, which is the sum of system CPU consumed by all threads of the server process (main thread and background threads).(Counter) redis_cpu_user_children_seconds_total: User CPU consumed by the background processes.(Counter) redis_cpu_user_seconds_total: User CPU consumed by the Redis server, which is the sum of user CPU consumed by all threads of the server process (main thread and background threads).(Counter) redis_db_keys: Total number of keys by DB. redis_db_keys_expiring: Total number of expiring keys by DB redis_defrag_hits: Number of value reallocations performed by active the defragmentation process. redis_defrag_misses: Number of aborted value reallocations started by the active defragmentation process. redis_defrag_key_hits: Number of keys that were actively defragmented. redis_defrag_key_misses: Number of keys that were skipped by the active defragmentation process. redis_evicted_keys_total: Number of evicted keys due to maxmemory limit.(Counter) redis_expired_keys_total: Total number of key expiration events.(Counter) redis_expired_stale_percentage: The percentage of keys probably expired. redis_expired_time_cap_reached_total: The count of times that active expiry cycles have stopped early. redis_exporter_last_scrape_connect_time_seconds: The duration(in seconds) to connect when scrape. redis_exporter_last_scrape_duration_seconds: The last scrape duration. redis_exporter_last_scrape_error: The last scrape error status. redis_exporter_scrape_duration_seconds_count: Durations of scrapes by the exporter redis_exporter_scrape_duration_seconds_sum: Durations of scrapes by the exporter redis_exporter_scrapes_total: Current total redis scrapes.(Counter) redis_instance_info: Information about the Redis instance. redis_keyspace_hits_total: Hits total.(Counter) redis_keyspace_misses_total: Misses total.(Counter) redis_last_key_groups_scrape_duration_milliseconds: Duration of the last key group metrics scrape in milliseconds. redis_last_slow_execution_duration_seconds: The amount of time needed for last slow execution, in seconds. redis_latest_fork_seconds: The amount of time needed for last fork, in seconds. redis_lazyfree_pending_objects: The number of objects waiting to be freed (as a result of calling UNLINK, or FLUSHDB and FLUSHALL with the ASYNC option). redis_master_repl_offset: The server's current replication offset. redis_mem_clients_normal: Memory used by normal clients.(Gauge) redis_mem_clients_slaves: Memory used by replica clients - Starting Redis 7.0, replica buffers share memory with the replication backlog, so this field can show 0 when replicas don't trigger an increase of memory usage. redis_mem_fragmentation_bytes: Delta between used_memory_rss and used_memory. Note that when the total fragmentation bytes is low (few megabytes), a high ratio (e.g. 1.5 and above) is not an indication of an issue. redis_mem_fragmentation_ratio: Ratio between used_memory_rss and used_memory. Note that this doesn't only includes fragmentation, but also other process overheads (see the allocator_* metrics), and also overheads like code, shared libraries, stack, etc. redis_mem_not_counted_for_eviction_bytes: (Gauge) redis_memory_max_bytes: Max memory limit in bytes. redis_memory_used_bytes: Total number of bytes allocated by Redis using its allocator (either standard libc, jemalloc, or an alternative allocator such as tcmalloc) redis_memory_used_dataset_bytes: The size in bytes of the dataset (used_memory_overhead subtracted from used_memory) redis_memory_used_lua_bytes: Number of bytes used by the Lua engine. redis_memory_used_overhead_bytes: The sum in bytes of all overheads that the server allocated for managing its internal data structures. redis_memory_used_peak_bytes: Peak memory consumed by Redis (in bytes) redis_memory_used_rss_bytes: Number of bytes that Redis allocated as seen by the operating system (a.k.a resident set size). This is the number reported by tools such as top(1) and ps(1) redis_memory_used_scripts_bytes: Number of bytes used by cached Lua scripts redis_memory_used_startup_bytes: Initial amount of memory consumed by Redis at startup in bytes redis_migrate_cached_sockets_total: The number of sockets open for MIGRATE purposes redis_net_input_bytes_total: Total input bytes(Counter) redis_net_output_bytes_total: Total output bytes(Counter) redis_process_id: Process ID redis_pubsub_channels: Global number of pub/sub channels with client subscriptions redis_pubsub_patterns: Global number of pub/sub pattern with client subscriptions redis_rdb_bgsave_in_progress: Flag indicating a RDB save is on-going redis_rdb_changes_since_last_save: Number of changes since the last dump redis_rdb_current_bgsave_duration_sec: Duration of the on-going RDB save operation if any redis_rdb_last_bgsave_duration_sec: Duration of the last RDB save operation in seconds redis_rdb_last_bgsave_status: Status of the last RDB save operation redis_rdb_last_cow_size_bytes: The size in bytes of copy-on-write memory during the last RDB save operation redis_rdb_last_save_timestamp_seconds: Epoch-based timestamp of last successful RDB save redis_rejected_connections_total: Number of connections rejected because of maxclients limit(Counter) redis_repl_backlog_first_byte_offset: The master offset of the replication backlog buffer redis_repl_backlog_history_bytes: Size in bytes of the data in the replication backlog buffer redis_repl_backlog_is_active: Flag indicating replication backlog is active redis_replica_partial_resync_accepted: The number of accepted partial resync requests(Gauge) redis_replica_partial_resync_denied: The number of denied partial resync requests(Gauge) redis_replica_resyncs_full: The number of full resyncs with replicas redis_replication_backlog_bytes: Memory used by replication backlog redis_second_repl_offset: The offset up to which replication IDs are accepted. redis_slave_expires_tracked_keys: The number of keys tracked for expiry purposes (applicable only to writable replicas)(Gauge) redis_slowlog_last_id: Last id of slowlog redis_slowlog_length: Total slowlog redis_start_time_seconds: Start time of the Redis instance since unix epoch in seconds. redis_target_scrape_request_errors_total: Errors in requests to the exporter redis_up: Flag indicating redis instance is up redis_uptime_in_seconds: Number of seconds since Redis server start 完整地配置项说明 # Enables the redis_exporter integration, allowing the Agent to automatically # collect system metrics from the configured redis address [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the hostname # portion of redis_addr. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the redis_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/redis_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # exporter-specific configuration options # Address of the redis instance. redis_addr: \u003cstring\u003e # User name to use for authentication (Redis ACL for Redis 6.0 and newer). [redis_user: \u003cstring\u003e] # Password of the redis instance. [redis_password: \u003cstring\u003e] # Path of a file containing a passord. If this is defined, it takes precedece # over redis_password. [redis_password_file: \u003cstring\u003e] # Namespace for the metrics. [namespace: \u003cstring\u003e | default = \"redis\"] # What to use for the CONFIG command. [config_command: \u003cstring\u003e | default = \"CONFIG\"] # Comma separated list of key-patterns to export value and length/size, searched for with SCAN. [check_keys: \u003cstring\u003e] # Comma separated list of LUA regex for grouping keys. When unset, no key # groups will be made. [check_key_groups: \u003cstring\u003e] # Check key or key groups batch size hint for the underlying SCAN. Keeping the same name for backwards compatibility, but this applies to both key and key groups batch size configuration. [check_key_groups_batch_size: \u003cint\u003e | default = 10000] # The maximum number of distinct key groups with the most memory utilization # to present as distinct metrics per database. The leftover key groups will be # aggregated in the 'overflow' bucket. [max_distinct_key_groups: \u003cint\u003e | default = 100] # Comma separated list of single keys to export value and length/size. [check_single_keys: \u003cstring\u003e] # Comma separated list of stream-patterns to export info about streams, groups and consumers, searched for with SCAN. [check_streams: \u003cstring\u003e] # Comma separated list of single streams to export info about streams, groups and consumers. [check_single_streams: \u003cstring\u003e] # Comma separated list of individual keys to export counts for. [count_keys: \u003cstring\u003e] # Path to Lua Redis script for collecting extra metrics. [script_path: \u003cstring\u003e] # Timeout for connection to Redis instance (in Golang duration format). [connection_timeout: \u003ctime.Duration\u003e | default = \"15s\"] # Name of the client key file (including full path) if the server requires TLS client authentication. [tls_client_key_file: \u003cstring\u003e] # Name of the client certificate file (including full path) if the server requires TLS client authentication. [tls_client_cert_file: \u003cstring\u003e] # Name of the CA certificate file (including full path) if the server requires TLS client authentication. [tls_ca_cert_file: \u003cstring\u003e] # Whether to set client name to redis_exporter. [set_client_name: \u003cbool\u003e] # Whether to scrape Tile38 specific metrics. [is_tile38: \u003cbool\u003e] # Whether to scrape Client List specific metrics. [export_client_list: \u003cbool\u003e] # Whether to include the client's port when exporting the client list. Note # that including this will increase the cardinality of all redis metrics. [export_client_port: \u003cbool\u003e] # Whether to also export go runtime metrics. [redis_metrics_only: \u003cbool\u003e] # Whether to ping the redis instance after connecting. [ping_on_connect: \u003cbool\u003e] # Whether to include system metrics like e.g. redis_total_system_memory_bytes. [incl_system_metrics: \u003cbool\u003e] # Whether to to skip TLS verification. [skip_tls_verification: \u003cbool\u003e] ",
    "description": "",
    "tags": null,
    "title": "Redis Exporter",
    "uri": "/grafana-agent/integrations/redis-exporter-config/"
  },
  {
    "content": "The statsd_exporter_config block configures the statsd_exporter integration, which is an embedded version of statsd_exporter. This allows for the collection of statsd metrics and exposing them as Prometheus metrics.\nFull reference of options:\n# Enables the statsd_exporter integration, allowing the Agent to automatically # collect system metrics from the configured statsd server address [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the agent hostname # and HTTP listen port, delimited by a colon. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the statsd_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/statsd_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # # Exporter-specific configuration options # # The UDP address on which to receive statsd metric lines. An empty string # will disable UDP collection. [listen_udp: \u003cstring\u003e | default = \":9125\"] # The TCP address on which to receive statsd metric lines. An empty string # will disable TCP collection. [listen_tcp: \u003cstring\u003e | default = \":9125\"] # The Unixgram socket path to receive statsd metric lines. An empty string # will disable unixgram collection. [listen_unixgram: \u003cstring\u003e | default = \"\"] # The permission mode of the unixgram socket, when enabled. [unix_socket_mode: \u003cstring\u003e | default = \"755\"] # An optional mapping config that can translate dot-separated StatsD metrics # into labeled Prometheus metrics. For full instructions on how to write this # object, see the official documentation from the statsd_exporter: # # https://github.com/prometheus/statsd_exporter#metric-mapping-and-configuration # # Note that a SIGHUP will not reload this config. [mapping_config: \u003cstatsd_exporter.mapping_config\u003e] # Size (in bytes) of the operating system's transmit read buffer associated # with the UDP or unixgram connection. Please make sure the kernel parameters # net.core.rmem_max is set to a value greater than the value specified. [read_buffer: \u003cint\u003e | default = 0] # Maximum size of your metric mapping cache. Relies on least recently used # replacement policy if max size is reached. [cache_size: \u003cint\u003e | default = 1000] # Metric mapping cache type. Valid values are \"lru\" and \"random\". [cache_type: \u003cstring\u003e | default = \"lru\"] # Size of internal queue for processing events. [event_queue_size: \u003cint\u003e | default = 10000] # Number of events to hold in queue before flushing. [event_flush_threshold: \u003cint\u003e | default = 1000] # Number of events to hold in queue before flushing. [event_flush_interval: \u003cduration\u003e | default = \"200ms\"] # Parse DogStatsd style tags. [parse_dogstatsd_tags: \u003cbool\u003e | default = true] # Parse InfluxDB style tags. [parse_influxdb_tags: \u003cbool\u003e | default = true] # Parse Librato style tags. [parse_librato_tags: \u003cbool\u003e | default = true] # Parse SignalFX style tags. [parse_signalfx_tags: \u003cbool\u003e | default = true] ",
    "description": "",
    "tags": null,
    "title": "Statsd Exporter",
    "uri": "/grafana-agent/integrations/statsd-exporter-config/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  },
  {
    "content": "grafana-agent内置了windows_exporter的实现，可以采集到windows平台的指标。\n配置并启用windows_exporter # grafana-agent 本身的配置 server: log_level: info http_listen_port: 12345 # grafana-agent 抓取 metrics 的相关配置（类似于prometheus的scrape_configs） metrics: global: scrape_interval: 15s scrape_timeout: 10s remote_write: - url: https://n9e-server:19000/prometheus/v1/write basic_auth: username: \u003cstring\u003e password: \u003cstring\u003e integrations: windows_exporter: enabled: true 采集的关键指标列表 windows_cpu_clock_interrupts_total: Total number of received and serviced clock tick interrupts(counter) windows_cpu_core_frequency_mhz: Core frequency in megahertz(gauge) windows_cpu_cstate_seconds_total: Time spent in low-power idle state(counter) windows_cpu_dpcs_total: Total number of received and serviced deferred procedure calls (DPCs)(counter) windows_cpu_idle_break_events_total: Total number of time processor was woken from idle(counter) windows_cpu_interrupts_total: Total number of received and serviced hardware interrupts(counter) windows_cpu_parking_status: Parking Status represents whether a processor is parked or not(gauge) windows_cpu_processor_performance: Processor Performance is the average performance of the processor while it is executing instructions, as a percentage of the nominal performance of the processor. On some processors, Processor Performance may exceed 100%(gauge) windows_cpu_time_total: Time that processor spent in different modes (idle, user, system, ...)(counter) windows_cs_hostname: Labeled system hostname information as provided by ComputerSystem.DNSHostName and ComputerSystem.Domain(gauge) windows_cs_logical_processors: ComputerSystem.NumberOfLogicalProcessors(gauge) windows_cs_physical_memory_bytes: ComputerSystem.TotalPhysicalMemory(gauge) windows_exporter_build_info: A metric with a constant '1' value labeled by version, revision, branch, and goversion from which windows_exporter was built.(gauge) windows_exporter_collector_duration_seconds: Duration of a collection.(gauge) windows_exporter_collector_success: Whether the collector was successful.(gauge) windows_exporter_collector_timeout: Whether the collector timed out.(gauge) windows_exporter_perflib_snapshot_duration_seconds: Duration of perflib snapshot capture(gauge) windows_logical_disk_free_bytes: Free space in bytes (LogicalDisk.PercentFreeSpace)(gauge) windows_logical_disk_idle_seconds_total: Seconds that the disk was idle (LogicalDisk.PercentIdleTime)(counter) windows_logical_disk_read_bytes_total: The number of bytes transferred from the disk during read operations (LogicalDisk.DiskReadBytesPerSec)(counter) windows_logical_disk_read_latency_seconds_total: Shows the average time, in seconds, of a read operation from the disk (LogicalDisk.AvgDiskSecPerRead)(counter) windows_logical_disk_read_seconds_total: Seconds that the disk was busy servicing read requests (LogicalDisk.PercentDiskReadTime)(counter) windows_logical_disk_read_write_latency_seconds_total: Shows the time, in seconds, of the average disk transfer (LogicalDisk.AvgDiskSecPerTransfer)(counter) windows_logical_disk_reads_total: The number of read operations on the disk (LogicalDisk.DiskReadsPerSec)(counter) windows_logical_disk_requests_queued: The number of requests queued to the disk (LogicalDisk.CurrentDiskQueueLength)(gauge) windows_logical_disk_size_bytes: Total space in bytes (LogicalDisk.PercentFreeSpace_Base)(gauge) windows_logical_disk_split_ios_total: The number of I/Os to the disk were split into multiple I/Os (LogicalDisk.SplitIOPerSec)(counter) windows_logical_disk_write_bytes_total: The number of bytes transferred to the disk during write operations (LogicalDisk.DiskWriteBytesPerSec)(counter) windows_logical_disk_write_latency_seconds_total: Shows the average time, in seconds, of a write operation to the disk (LogicalDisk.AvgDiskSecPerWrite)(counter) windows_logical_disk_write_seconds_total: Seconds that the disk was busy servicing write requests (LogicalDisk.PercentDiskWriteTime)(counter) windows_logical_disk_writes_total: The number of write operations on the disk (LogicalDisk.DiskWritesPerSec)(counter) windows_net_bytes_received_total: (Network.BytesReceivedPerSec)(counter) windows_net_bytes_sent_total: (Network.BytesSentPerSec)(counter) windows_net_bytes_total: (Network.BytesTotalPerSec)(counter) windows_net_current_bandwidth: (Network.CurrentBandwidth)(gauge) windows_net_packets_outbound_discarded_total: (Network.PacketsOutboundDiscarded)(counter) windows_net_packets_outbound_errors_total: (Network.PacketsOutboundErrors)(counter) windows_net_packets_received_discarded_total: (Network.PacketsReceivedDiscarded)(counter) windows_net_packets_received_errors_total: (Network.PacketsReceivedErrors)(counter) windows_net_packets_received_total: (Network.PacketsReceivedPerSec)(counter) windows_net_packets_received_unknown_total: (Network.PacketsReceivedUnknown)(counter) windows_net_packets_sent_total: (Network.PacketsSentPerSec)(counter) windows_net_packets_total: (Network.PacketsPerSec)(counter) windows_os_info: OperatingSystem.Caption, OperatingSystem.Version(gauge) windows_os_paging_free_bytes: OperatingSystem.FreeSpaceInPagingFiles(gauge) windows_os_paging_limit_bytes: OperatingSystem.SizeStoredInPagingFiles(gauge) windows_os_physical_memory_free_bytes: OperatingSystem.FreePhysicalMemory(gauge) windows_os_process_memory_limix_bytes: OperatingSystem.MaxProcessMemorySize(gauge) windows_os_processes: OperatingSystem.NumberOfProcesses(gauge) windows_os_processes_limit: OperatingSystem.MaxNumberOfProcesses(gauge) windows_os_time: OperatingSystem.LocalDateTime(gauge) windows_os_timezone: OperatingSystem.LocalDateTime(gauge) windows_os_users: OperatingSystem.NumberOfUsers(gauge) windows_os_virtual_memory_bytes: OperatingSystem.TotalVirtualMemorySize(gauge) windows_os_virtual_memory_free_bytes: OperatingSystem.FreeVirtualMemory(gauge) windows_os_visible_memory_bytes: OperatingSystem.TotalVisibleMemorySize(gauge) windows_service_info: A metric with a constant '1' value labeled with service information(gauge) windows_service_start_mode: The start mode of the service (StartMode)(gauge) windows_service_state: The state of the service (State)(gauge) windows_service_status: The status of the service (Status)(gauge) windows_system_context_switches_total: Total number of context switches (WMI source is PerfOS_System.ContextSwitchesPersec)(counter) windows_system_exception_dispatches_total: Total number of exceptions dispatched (WMI source is PerfOS_System.ExceptionDispatchesPersec)(counter) windows_system_processor_queue_length: Length of processor queue (WMI source is PerfOS_System.ProcessorQueueLength)(gauge) windows_system_system_calls_total: Total number of system calls (WMI source is PerfOS_System.SystemCallsPersec)(counter) windows_system_system_up_time: System boot time (WMI source is PerfOS_System.SystemUpTime)(gauge) windows_system_threads: Current number of threads (WMI source is PerfOS_System.Threads)(gauge) 完整地配置项说明 # Enables the windows_exporter integration, allowing the Agent to automatically # collect system metrics from the local windows instance [enabled: \u003cboolean\u003e | default = false] # Sets an explicit value for the instance label when the integration is # self-scraped. Overrides inferred values. # # The default value for this integration is inferred from the agent hostname # and HTTP listen port, delimited by a colon. [instance: \u003cstring\u003e] # Automatically collect metrics from this integration. If disabled, # the consul_exporter integration will be run but not scraped and thus not # remote-written. Metrics for the integration will be exposed at # /integrations/windows_exporter/metrics and can be scraped by an external # process. [scrape_integration: \u003cboolean\u003e | default = \u003cintegrations_config.scrape_integrations\u003e] # How often should the metrics be collected? Defaults to # prometheus.global.scrape_interval. [scrape_interval: \u003cduration\u003e | default = \u003cglobal_config.scrape_interval\u003e] # The timeout before considering the scrape a failure. Defaults to # prometheus.global.scrape_timeout. [scrape_timeout: \u003cduration\u003e | default = \u003cglobal_config.scrape_timeout\u003e] # Allows for relabeling labels on the target. relabel_configs: [- \u003crelabel_config\u003e ... ] # Relabel metrics coming from the integration, allowing to drop series # from the integration that you don't care about. metric_relabel_configs: [ - \u003crelabel_config\u003e ... ] # How frequent to truncate the WAL for this integration. [wal_truncate_frequency: \u003cduration\u003e | default = \"60m\"] # Monitor the exporter itself and include those metrics in the results. [include_exporter_metrics: \u003cbool\u003e | default = false] # # Exporter-specific configuration options # # List of collectors to enable. Any non-experimental collector from the # embeded version of windows_exporter can be enabeld here. [enabled_collectors: \u003cstring\u003e | default = \"cpu,cs,logical_disk,net,os,service,system,textfile\"] # Settings for collectors which accept configuration. Settings specified here # are only used if the corresponding collector is enabled in # enabled_collectors. # Configuration for Exchange Mail Server exchange: # Comma-separated List of collectors to use. Defaults to all, if not specified. # Maps to collectors.exchange.enabled in windows_exporter [enabled_list: \u003cstring\u003e] # Configuration for the IIS web server iis: # Regexp of sites to whitelist. Site name must both match whitelist and not match blacklist to be included. # Maps to collector.iis.site-whitelist in windows_exporter [site_whitelist: \u003cstring\u003e | default = \".+\"] # Regexp of sites to blacklist. Site name must both match whitelist and not match blacklist to be included. # Maps to collector.iis.site-blacklist in windows_exporter [site_blacklist: \u003cstring\u003e | default = \"\"] # Regexp of apps to whitelist. App name must both match whitelist and not match blacklist to be included. # Maps to collector.iis.app-whitelist in windows_exporter [app_whitelist: \u003cstring\u003e | default=\".+\"] # Regexp of apps to blacklist. App name must both match whitelist and not match blacklist to be included. # Maps to collector.iis.app-blacklist in windows_exporter [app_blacklist: \u003cstring\u003e | default=\".+\"] # Configuration for reading metrics from a text files in a directory text_file: # Directory to read text files with metrics from. # Maps to collector.textfile.directory in windows_exporter [text_file_directory: \u003cstring\u003e | default=\"C:\\Program Files\\windows_exporter\\textfile_inputs\"] # Configuration for SMTP metrics smtp: # Regexp of virtual servers to whitelist. Server name must both match whitelist and not match blacklist to be included. # Maps to collector.smtp.server-whitelist in windows_exporter [whitelist: \u003cstring\u003e | default=\".+\"] # Regexp of virtual servers to blacklist. Server name must both match whitelist and not match blacklist to be included. # Maps to collector.smtp.server-blacklist in windows_exporter [blacklist: \u003cstring\u003e | default=\"\"] # Configuration for Windows Services service: # \"WQL 'where' clause to use in WMI metrics query. Limits the response to the services you specify and reduces the size of the response. # Maps to collector.service.services-where in windows_exporter [where_clause: \u003cstring\u003e | default=\"\"] # Configuration for Windows Processes process: # Regexp of processes to include. Process name must both match whitelist and not match blacklist to be included. # Maps to collector.process.whitelist in windows_exporter [whitelist: \u003cstring\u003e | default=\".+\"] # Regexp of processes to exclude. Process name must both match whitelist and not match blacklist to be included. # Maps to collector.process.blacklist in windows_exporter [blacklist: \u003cstring\u003e | default=\"\"] # Configuration for NICs network: # Regexp of NIC's to whitelist. NIC name must both match whitelist and not match blacklist to be included. # Maps to collector.net.nic-whitelist in windows_exporter [whitelist: \u003cstring\u003e | default=\".+\"] # Regexp of NIC's to blacklist. NIC name must both match whitelist and not match blacklist to be included. # Maps to collector.net.nic-blacklist in windows_exporter [blacklist: \u003cstring\u003e | default=\"\"] # Configuration for Microsoft SQL Server mssql: # Comma-separated list of mssql WMI classes to use. # Maps to collectors.mssql.classes-enabled in windows_exporter [enabled_classes: \u003cstring\u003e | default=\"accessmethods,availreplica,bufman,databases,dbreplica,genstats,locks,memmgr,sqlstats,sqlerrors,transactions\"] # Configuration for Microsoft Queue msqm: # WQL 'where' clause to use in WMI metrics query. Limits the response to the msmqs you specify and reduces the size of the response. # Maps to collector.msmq.msmq-where in windows_exporter [where_clause: \u003cstring\u003e | default=\"\"] # Configuration for disk information logical_disk: # Regexp of volumes to whitelist. Volume name must both match whitelist and not match blacklist to be included. # Maps to collector.logical_disk.volume-whitelist in windows_exporter [whitelist: \u003cstring\u003e | default=\".+\"] # Regexp of volumes to blacklist. Volume name must both match whitelist and not match blacklist to be included. # Maps to collector.logical_disk.volume-blacklist in windows_exporter [blacklist: \u003cstring\u003e | default=\".+\"] ",
    "description": "",
    "tags": null,
    "title": "Windows Exporter",
    "uri": "/grafana-agent/integrations/windows-exporter-config/"
  },
  {
    "content": "夜莺简介  夜莺（ Nightingale ）是一款国产开源、云原生监控系统，Nightingale 在 2020.3.20 发布 v1 版本，目前是 v5 版本，从这个版本开始，与 Prometheus、VictoriaMetrics、Grafana、Telegraf、Datadog 等生态做了协同集成，力争打造国内最好用的开源运维监控系统。出自 Open-Falcon 研发团队。夜莺监控项目，于 2022 年 5 月 11 日，捐赠予中国计算机学会开源发展委员会(CCF ODC)，为 CCF ODC 成立后接受捐赠的首个开源项目。\n 项目代码  后端：💡 https://github.com/ccfos/nightingale 前端：💡 https://github.com/n9e/fe-v5  前后端都是开源的，如果觉得不错，欢迎 star 一下，给我们持续坚持的动力！\n产品截图 查看监控数据，即监控大盘页面：\n配置告警规则的列表页面：\n活跃告警列表页面，即当前未恢复的告警页面：\n产品架构 Nightingale 有四个核心功能：\n Query Proxy：承接前端时序数据查询请求，转发给时序库，并将时序库返回的结果返回给前端 Push Gateway：承接各类采集客户端的监控数据推送，然后把数据转存到后端多种时序库 Conf Manager：配置管理，比如告警规则、屏蔽规则、订阅规则、自愈脚本、权限等相关配置的管理 Alerting Engine：告警引擎，根据用户配置的 PromQL，查询时序库，判断是否应该触发告警并发送  系统架构 夜莺 v5 的设计非常简单，核心是 server 和 webapi 两个模块，webapi 无状态，放到中心端，承接前端请求，将用户配置写入数据库；server 是告警引擎和数据转发模块，一般随着时序库走，一个时序库就对应一套 server，每套 server 可以只用一个实例，也可以多个实例组成集群，server 可以接收 Telegraf、Grafana-Agent、Datadog-Agent、Falcon-Plugins 上报的数据，写入后端时序库，周期性从数据库同步告警规则，然后查询时序库做告警判断。每套 server 依赖一个 redis。架构图如下：\n产品对比 与 Open-Falcon 的区别 因为开发 Open-Falcon 和 Nightingale 的是一拨人，所以很多社区伙伴会比较好奇，为何要新做一个监控开源软件。核心点是 Open-Falcon 和 Nightingale 的差异点实在是太大了，Nightingale 并非是 Open-Falcon 设计逻辑的一个延续，就看做两个不同的软件就好。\nOpen-Falcon 是 14 年开发的，当时是想解决 Zabbix 的一些容量问题，可以看做是物理机时代的产物，整个设计偏向运维视角，虽然数据结构上已经开始设计了标签，但是查询语法还是比较简单，无法应对比较复杂的场景。\nNightingale 直接支持 PromQL，支持 Prometheus、M3DB、VictoriaMetrics 多种时序库，支持 Telegraf、Datadog-Agent、Grafana-Agent 做监控数据采集，支持 Grafana 看图，整个设计更加云原生。\n与 Prometheus 的区别 Nightingale 可以简单看做是 Prometheus 的一个企业级版本，把 Prometheus 当做 Nightingale 的一个内部组件（时序库），当然，也不是必须的，时序库除了 Prometheus，还可以使用 VictoriaMetrics、M3DB 等，各种 Exporter 采集器也可以继续使用。\nNightingale 可以接入多个 Prometheus，可以允许用户在页面上配置告警规则、屏蔽规则、订阅规则，在页面上查看告警事件、做告警事件聚合统计，配置告警自愈机制，管理监控对象，配置监控大盘等，就把 Nightingale 看做是 Prometheus 的一个 WEBUI 也是可以的，不过实际上，它远远不止是一个 WEBUI，用一下就会深有感触。\n加入社区 微信公众号:cloudmon（云原生监控）这是夜莺的大本营，可以在公众号菜单里找到加群入口、社区答疑入口，关注起来吧！我们团队做运维监控这个事情差不多有 10 年了，一直在坚持，希望能把这个事做到极致，欢迎加入我们一起！\n",
    "description": "",
    "tags": null,
    "title": "夜莺手册",
    "uri": "/"
  },
  {
    "content": " Acknowledgement: grafana-agent is powered by Grafana Agent. Grafana Agent is a lightweight telemetry collector based on Prometheus that only performs its scraping and remote_write functions. Agent can also collect metrics, logs, and traces for storage in Grafana Cloud and Grafana Enterprise, as well as OSS deployments of Loki (logs), and Tempo (traces), Prometheus (metrics), and Cortex (metrics). Grafana Agent also contains several integrations (embedded metrics exporters) like node-exporter, a MySQL exporter, and many more.\nThe Grafana Agent uses the same code as Prometheus, but tackles these issues by only using the most relevant parts of Prometheus for interaction with hosted metrics:\n Service Discovery Scraping Write Ahead Log (WAL) Remote Write   对于Kubernetes集群及其上应用，我们推荐从以下几个方面，建立起完整的kubernetes指标监控体系：\n前置依赖  如何在K8s中运行和启动grafana-agent，请参考在kubernetes中运行grafana-agent收集。 推荐您以daemonset，在每个节点上启动一个grafana-agent实例。  通过kubelet来了解和监控k8s节点的基本运行状态数据 方案一：直接访问kubelet来获取节点状态指标数据 Kubelet组件运行在Kubernetes集群的各个节点中，其负责维护和管理节点上Pod的运行状态。kubelet组件的正常运行直接关系到该节点是否能够正常的被Kubernetes集群正常使用。\n基于Prometheus在K8s环境下的服务发现能力，在Node模式，grafana-agent会自动发现Kubernetes中所有Node节点的信息并作为监控的目标Target。 而这些Target的访问地址实际上就是Kubelet的访问地址。\n 创建ConfigMap，其中包含grafana-agent的配置文件如下\n export NAMESPACE=default export CLUSTER_NAME=kubernetes export REMOTE_WRITE_URL=http://n9e-server:19000/prometheus/v1/write export REMOTE_WRITE_USERNAME=fc_laiwei export REMOTE_WRITE_PASSWORD=fc_laiweisecret cat \u003c\u003cEOF | kind: ConfigMap metadata: name: grafana-agent apiVersion: v1 data: agent.yaml: |server: http_listen_port: 12345 metrics: wal_directory: /tmp/grafana-agent-wal global: scrape_interval: 15s scrape_timeout: 10s external_labels: cluster: ${CLUSTER_NAME} configs: - name: fc_k8s_scrape remote_write: - url: ${REMOTE_WRITE_URL} basic_auth: username: ${REMOTE_WRITE_USERNAME} password: ${REMOTE_WRITE_PASSWORD} scrape_configs: - job_name: integrations/kubernetes/kubelet scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) EOF envsubst | kubectl apply -n $NAMESPACE -f -  重建grafana-agent实例\n kubectl rollout restart daemonset/grafana-agent 这里使用Node模式自动发现集群中所有Kubelet作为监控的数据采集目标，同时通过labelmap步骤，将Node节点上的标签，作为样本的标签保存到时间序列当中。 重新加载grafana-agent的配置文件，并重建grafana-agent的Pod实例后，在nightingale dashboard中搜索{job=\"integrations/kubernetes/kubelet\"}，即可看到相应的时序数据了。\n方案二：通过kube-apiserver提供的API间接获取kubelet的指标数据 不同于上面第一种方法，其直接通过kubelet的metrics服务采集监控数据，方法二通过Kubernetes的api-server提供的代理API访问各个节点中kubelet的metrics服务。\n 创建ConfigMap，其中包含grafana-agent的配置文件如下\n export NAMESPACE=default export CLUSTER_NAME=kubernetes export REMOTE_WRITE_URL=http://10.206.0.16:8480/insert/0/prometheus/api/v1/write export REMOTE_WRITE_URL=http://n9e-server:19000/prometheus/v1/write export REMOTE_WRITE_USERNAME=fc_laiwei export REMOTE_WRITE_PASSWORD=fc_laiweisecret cat \u003c\u003cEOF | kind: ConfigMap metadata: name: grafana-agent apiVersion: v1 data: agent.yaml: |server: http_listen_port: 12345 metrics: wal_directory: /tmp/grafana-agent-wal global: scrape_interval: 15s scrape_timeout: 10s external_labels: cluster: ${CLUSTER_NAME} configs: - name: fc_k8s_scrape remote_write: - url: ${REMOTE_WRITE_URL} basic_auth: username: ${REMOTE_WRITE_USERNAME} password: ${REMOTE_WRITE_PASSWORD} scrape_configs: - job_name: 'integrations/kubernetes/kubelet' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/\\${1}/proxy/metrics EOF envsubst | kubectl apply -n $NAMESPACE -f - 通过relabeling，将从Kubernetes获取到的默认地址__address__替换为kubernetes.default.svc:443。同时将__metrics_path__替换为api-server的代理地址/api/v1/nodes/${1}/proxy/metrics。\n通过获取各个节点中kubelet的监控指标，您可以评估集群中各节点的性能表现。例如:\n1. 通过指标kubelet_pod_start_duration_seconds可以获得当前节点中Pod启动时间相关的统计数据。\nkubelet_pod_start_duration_seconds{quantile=\"0.99\"} 2. Pod平均启动时间（包含镜像下载时间）：\nkubelet_pod_start_duration_seconds_sum / kubelet_pod_start_duration_seconds_count 除此以外，监控指标kubelet_docker_*还可以体现出kubelet与当前节点的docker服务的调用情况，从而可以反映出docker本身是否会影响kubelet的性能表现等问题。\n通过cAdvisor来了解和监控节点中的容器运行状态 各节点的kubelet组件中除了包含自身的监控指标信息以外，kubelet组件还内置了对cAdvisor的支持。cAdvisor能够获取当前节点上运行的所有容器的资源使用情况，通过访问kubelet的/metrics/cadvisor地址可以获取到cadvisor的监控指标，因此和获取kubelet监控指标类似，这里同样通过node模式自动发现所有的kubelet信息，并通过适当的relabel过程，修改监控采集任务的配置。 与采集kubelet自身监控指标相似，这里也有两种方式采集cadvisor中的监控指标：\n方案一：直接访问kubelet的/metrics/cadvisor地址，需要跳过ca证书认证 export NAMESPACE=default export CLUSTER_NAME=kubernetes export REMOTE_WRITE_URL=http://10.206.0.16:8480/insert/0/prometheus/api/v1/write export REMOTE_WRITE_URL=http://n9e-server:19000/prometheus/v1/write export REMOTE_WRITE_USERNAME=fc_laiwei export REMOTE_WRITE_PASSWORD=fc_laiweisecret cat \u003c\u003cEOF | kind: ConfigMap metadata: name: grafana-agent apiVersion: v1 data: agent.yaml: |server: http_listen_port: 12345 metrics: wal_directory: /tmp/grafana-agent-wal global: scrape_interval: 15s scrape_timeout: 10s external_labels: cluster: ${CLUSTER_NAME} configs: - name: fc_k8s_scrape remote_write: - url: ${REMOTE_WRITE_URL} basic_auth: username: ${REMOTE_WRITE_USERNAME} password: ${REMOTE_WRITE_PASSWORD} scrape_configs: - job_name: 'integrations/kubernetes/cadvisor' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: metrics/cadvisor - action: labelmap regex: __meta_kubernetes_node_label_(.+) EOF envsubst | kubectl apply -n $NAMESPACE -f - 方案二：通过api-server提供的代理地址访问kubelet的/metrics/cadvisor地址 export NAMESPACE=default export CLUSTER_NAME=kubernetes export REMOTE_WRITE_URL=http://10.206.0.16:8480/insert/0/prometheus/api/v1/write export REMOTE_WRITE_URL=http://n9e-server:19000/prometheus/v1/write export REMOTE_WRITE_USERNAME=fc_laiwei export REMOTE_WRITE_PASSWORD=fc_laiweisecret cat \u003c\u003cEOF | kind: ConfigMap metadata: name: grafana-agent apiVersion: v1 data: agent.yaml: |server: http_listen_port: 12345 metrics: wal_directory: /tmp/grafana-agent-wal global: scrape_interval: 15s scrape_timeout: 10s external_labels: cluster: ${CLUSTER_NAME} configs: - name: fc_k8s_scrape remote_write: - url: ${REMOTE_WRITE_URL} basic_auth: username: ${REMOTE_WRITE_USERNAME} password: ${REMOTE_WRITE_PASSWORD} scrape_configs: - job_name: 'integrations/kubernetes/cadvisor' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor - action: labelmap regex: __meta_kubernetes_node_label_(.+) 使用NodeExporter监控节点资源使用情况 为了能够采集集群中各个节点的资源使用情况，我们可以借助grafana-agent内置的NodeExporter。具体的步骤可以参考：grafana-agent node_exporter。\n通过kube-apiserver来了解整个K8s集群的详细运行状态 kube-apiserver扮演了整个Kubernetes集群管理的入口的角色，负责对外暴露Kubernetes API。kube-apiserver组件一般是独立部署在集群外的，为了能够让部署在集群内的应用（kubernetes插件或者用户应用）能够与kube-apiserver交互，Kubernetes会默认在命名空间下创建一个名为kubernetes的服务，如下所示：\n$ kubectl get svc kubernetes -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 166d \u003cnone\u003e 而该kubernetes服务代理的后端实际地址通过endpoints进行维护，如下所示：\n$ kubectl get endpoints kubernetes NAME ENDPOINTS AGE kubernetes 10.0.2.15:8443 166d 通过这种方式集群内的应用或者系统主机就可以通过集群内部的DNS域名kubernetes.default.svc访问到部署外部的kube-apiserver实例。\n因此，如果我们想要监控kube-apiserver相关的指标，只需要通过endpoints资源找到kubernetes对应的所有后端地址即可。\n如下所示，创建监控任务kubernetes-apiservers，这里指定了服务发现模式为endpoints。grafana-agent会查找当前集群中所有的endpoints配置，并通过relabel进行判断是否为apiserver对应的访问地址：\n- job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - target_label: __address__ replacement: kubernetes.default.svc:443 在relabel_configs配置中第一步用于判断当前endpoints是否为kube-apiserver对用的地址。第二步，替换监控采集地址到kubernetes.default.svc:443即可。重新加载配置文件，重建grafana-agent实例，用以下promql {service=\"kubernetes\", job=\"apiserver\"}即可在nightingale dashboard中得到kube-apiserver相关的metrics数据。\n通过BlackboxExporter了解和监控K8s集群中的网络连通状况 为了能够对Ingress和Service进行探测，我们需要在K8s集群部署Blackbox Exporter实例。 如下所示，创建blackbox-exporter.yaml用于描述部署相关的内容:\ncat \u003c\u003c EOF | apiVersion: v1 kind: Service metadata: labels: app: blackbox-exporter name: blackbox-exporter spec: ports: - name: blackbox port: 9115 protocol: TCP selector: app: blackbox-exporter type: ClusterIP --- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: app: blackbox-exporter name: blackbox-exporter spec: replicas: 1 selector: matchLabels: app: blackbox-exporter template: metadata: labels: app: blackbox-exporter spec: containers: - image: prom/blackbox-exporter imagePullPolicy: IfNotPresent name: blackbox-exporter EOF kubectl apply -f - 通过以上命令，将在K8s集群中部署了一个Blackbox Exporter的Pod实例，同时通过服务blackbox-exporter在集群内暴露访问地址blackbox-exporter.default.svc.cluster.local，对于集群内的任意服务都可以通过该内部DNS域名访问Blackbox Exporter实例：\n$ kubectl get pods NAME READY STATUS RESTARTS AGE blackbox-exporter-f77fc78b6-72bl5 1/1 Running 0 4s $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE blackbox-exporter ClusterIP 10.109.144.192 \u003cnone\u003e 9115/TCP 3m 为了能够让grafana-agent能够自动的对Service进行探测，我们需要通过服务发现自动找到所有的Service信息。 如下所示，在grafana-agent的配置文件中添加名为kubernetes-services的监控采集任务：\n- job_name: 'kubernetes-services' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.default.svc.cluster.local:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name 在该任务配置中，通过指定kubernetes_sd_config的role为service指定服务发现模式：\nkubernetes_sd_configs: - role: service 为了区分集群中需要进行探测的Service实例，我们通过标签‘prometheus.io/probe: true’进行判断，从而过滤出需要探测的所有Service实例：\n- source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true 并且将通过服务发现获取到的Service实例地址__address__转换为获取监控数据的请求参数。同时将__address执行Blackbox Exporter实例的访问地址，并且重写了标签instance的内容：\n- source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.default.svc.cluster.local:9115 - source_labels: [__param_target] target_label: instance 最后，为监控样本添加了额外的标签信息：\n- action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name 对于Ingress而言，也是一个相对类似的过程，这里给出对Ingress探测的grafana-agent任务配置作为参考：\n- job_name: 'kubernetes-ingresses' metrics_path: /probe params: module: [http_2xx] kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: ${1}://${2}${3} target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.default.svc.cluster.local:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name 通过kube-state-metrics了解和监控K8s集群自身和应用的运行状态 kube-state-metrics重点回答以下方面的问题：\n 我调度了多少个replicas？现在可用的有几个？ 多少个Pod是running/stopped/terminated状态？ Pod重启了多少次？ 我有多少job在运行中？  kube-state-metrics基于client-go开发，轮询Kubernetes API，并将Kubernetes的结构化信息转换为metrics。他所支持的指标包括：\n CronJob Metrics DaemonSet Metrics Deployment Metrics Job Metrics LimitRange Metrics Node Metrics PersistentVolume Metrics PersistentVolumeClaim Metrics Pod Metrics Pod Disruption Budget Metrics ReplicaSet Metrics ReplicationController Metrics ResourceQuota Metrics Service Metrics StatefulSet Metrics Namespace Metrics Horizontal Pod Autoscaler Metrics Endpoint Metrics Secret Metrics ConfigMap Metrics  以Pod为例：\n kube_pod_info kube_pod_owner kube_pod_status_phase kube_pod_status_ready kube_pod_status_scheduled kube_pod_container_status_waiting kube_pod_container_status_terminated_reason …  部署清单：\n├── cluster-role-binding.yaml ├── cluster-role.yaml ├── deployment.yaml ├── service-account.yaml ├── service.yaml 主要镜像有：\n image: quay.io/coreos/kube-state-metrics:v2.4.2 image: k8s.gcr.io/kube-state-metrics/kube-state-metrics  由于 quay.io/coreos/kube-state-metrics 不再更新，推荐使用 k8s.gcr.io/kube-state-metrics/kube-state-metrics\n quay.io/coreos/kube-state-metrics images will no longer be updated. k8s.gcr.io/kube-state-metrics/kube-state-metrics is the new canonical location.\n 对于pod的资源限制，一般情况下：\n200MiB memory 0.1 cores 超过100节点的集群：\n2MiB memory per node 0.001 cores per node 因为kube-state-metrics-service.yaml中有prometheus.io/scrape: 'true'标识，因此会将metric暴露给grafana-agent，而grafana-agent会在kubernetes-service-endpoints这个job下自动发现kube-state-metrics，并开始拉取metrics，无需其他配置。\n使用kube-state-metrics后的常用场景有：\n 存在执行失败的Job: kube_job_status_failed{job=“kubernetes-service-endpoints”,k8s_app=“kube-state-metrics”}==1 集群节点状态错误: kube_node_status_condition{condition=“Ready”,status!=“true”}==1 集群中存在启动失败的Pod：kube_pod_status_phase{phase=~“Failed|Unknown”}==1 最近30分钟内有Pod容器重启: changes(kube_pod_container_status_restarts[30m])\u003e0   参考资料\n  Prometheus与服务发现 基于文件的服务发现 基于Consul的服务发现 服务发现与Relabel Kubernetes下的服务发现 监控Kubernetes集群 kube-state-metrics kube-state-metrics deoplyment   Acknowledgement:本文档在yunlzheng 监控Kubernetes集群的基础上修改和补充而成，相关文字的版权归属原作者yunlzheng所有，并致以谢意。\n ",
    "description": "",
    "tags": null,
    "title": "监控K8s集群和应用",
    "uri": "/grafana-agent/how-to-monitoring-k8s/"
  }
]
